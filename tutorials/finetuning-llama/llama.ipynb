{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-07T15:48:47.818234Z",
     "start_time": "2024-08-07T15:48:39.093186Z"
    }
   },
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"../pretrained/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=0,\n",
    "    # device_map=\"auto\",\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5ee4192b24684559924c236102d0a6ea"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "eec544dc8876089a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T15:48:58.761662Z",
     "start_time": "2024-08-07T15:48:53.761447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a good 3d object grounder!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])"
   ],
   "id": "5af546ea766e24a6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': 'I\\'m a 3D object grounder. A grounder is a type of AI model that specializes in understanding and generating text descriptions of visual scenes, specifically focusing on objects and their properties within those scenes.\\n\\nAs a 3D object grounder, my primary function is to take 3D scene data or images and generate grounded text descriptions of the objects present in the scene. This means I can identify, describe, and contextualize objects in a way that\\'s accurate and relevant to the scene.\\n\\nFor example, if you show me a 3D model of a kitchen, I might generate a text description like: \"The kitchen contains a refrigerator, a stove, a sink, and a table.\" Or, if you show me an image of a living room, I might say: \"The living room contains a couch, a coffee table, a TV, and a bookshelf.\"\\n\\nMy goal is to provide a rich and accurate understanding of the objects in a scene, which can be useful for a wide range of applications, such as visual question answering, scene understanding, and even robotics and autonomous systems.'}\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T15:18:21.551045Z",
     "start_time": "2024-08-07T15:18:21.171410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# First, define a tool\n",
    "def get_current_temperature(location: str) -> float:\n",
    "    \"\"\"\n",
    "    Get the current temperature at a location.\n",
    "    \n",
    "    Args:\n",
    "        location: The location to get the temperature for, in the format \"City, Country\"\n",
    "    Returns:\n",
    "        The current temperature at the specified location in the specified units, as a float.\n",
    "    \"\"\"\n",
    "    return 22.  # A real function should probably actually get the temperature!\n",
    "\n",
    "# Next, create a chat and apply the chat template\n",
    "messages = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a bot that responds to weather queries.\"},\n",
    "  {\"role\": \"user\", \"content\": \"Hey, what's the temperature in Paris right now?\"}\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(messages, tools=[get_current_temperature], add_generation_prompt=True)\n",
    "inputs"
   ],
   "id": "e96eecff5eed3fa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000,\n",
       " 128006,\n",
       " 9125,\n",
       " 128007,\n",
       " 271,\n",
       " 13013,\n",
       " 25,\n",
       " 6125,\n",
       " 27993,\n",
       " 198,\n",
       " 38766,\n",
       " 1303,\n",
       " 33025,\n",
       " 2696,\n",
       " 25,\n",
       " 6790,\n",
       " 220,\n",
       " 2366,\n",
       " 18,\n",
       " 198,\n",
       " 15724,\n",
       " 2696,\n",
       " 25,\n",
       " 220,\n",
       " 1627,\n",
       " 10263,\n",
       " 220,\n",
       " 2366,\n",
       " 19,\n",
       " 271,\n",
       " 2675,\n",
       " 527,\n",
       " 264,\n",
       " 11164,\n",
       " 430,\n",
       " 31680,\n",
       " 311,\n",
       " 9282,\n",
       " 20126,\n",
       " 13,\n",
       " 128009,\n",
       " 128006,\n",
       " 882,\n",
       " 128007,\n",
       " 271,\n",
       " 22818,\n",
       " 279,\n",
       " 2768,\n",
       " 5865,\n",
       " 11,\n",
       " 4587,\n",
       " 6013,\n",
       " 449,\n",
       " 264,\n",
       " 4823,\n",
       " 369,\n",
       " 264,\n",
       " 734,\n",
       " 1650,\n",
       " 449,\n",
       " 1202,\n",
       " 6300,\n",
       " 6105,\n",
       " 430,\n",
       " 1888,\n",
       " 11503,\n",
       " 279,\n",
       " 2728,\n",
       " 10137,\n",
       " 382,\n",
       " 66454,\n",
       " 304,\n",
       " 279,\n",
       " 3645,\n",
       " 5324,\n",
       " 609,\n",
       " 794,\n",
       " 734,\n",
       " 836,\n",
       " 11,\n",
       " 330,\n",
       " 14105,\n",
       " 794,\n",
       " 11240,\n",
       " 315,\n",
       " 5811,\n",
       " 836,\n",
       " 323,\n",
       " 1202,\n",
       " 907,\n",
       " 7966,\n",
       " 5519,\n",
       " 539,\n",
       " 1005,\n",
       " 7482,\n",
       " 382,\n",
       " 517,\n",
       " 262,\n",
       " 330,\n",
       " 1337,\n",
       " 794,\n",
       " 330,\n",
       " 1723,\n",
       " 761,\n",
       " 262,\n",
       " 330,\n",
       " 1723,\n",
       " 794,\n",
       " 341,\n",
       " 286,\n",
       " 330,\n",
       " 609,\n",
       " 794,\n",
       " 330,\n",
       " 456,\n",
       " 11327,\n",
       " 54625,\n",
       " 761,\n",
       " 286,\n",
       " 330,\n",
       " 4789,\n",
       " 794,\n",
       " 330,\n",
       " 1991,\n",
       " 279,\n",
       " 1510,\n",
       " 9499,\n",
       " 520,\n",
       " 264,\n",
       " 3813,\n",
       " 10560,\n",
       " 286,\n",
       " 330,\n",
       " 14105,\n",
       " 794,\n",
       " 341,\n",
       " 310,\n",
       " 330,\n",
       " 1337,\n",
       " 794,\n",
       " 330,\n",
       " 1735,\n",
       " 761,\n",
       " 310,\n",
       " 330,\n",
       " 13495,\n",
       " 794,\n",
       " 341,\n",
       " 394,\n",
       " 330,\n",
       " 2588,\n",
       " 794,\n",
       " 341,\n",
       " 504,\n",
       " 330,\n",
       " 1337,\n",
       " 794,\n",
       " 330,\n",
       " 928,\n",
       " 761,\n",
       " 504,\n",
       " 330,\n",
       " 4789,\n",
       " 794,\n",
       " 330,\n",
       " 791,\n",
       " 3813,\n",
       " 311,\n",
       " 636,\n",
       " 279,\n",
       " 9499,\n",
       " 369,\n",
       " 11,\n",
       " 304,\n",
       " 279,\n",
       " 3645,\n",
       " 7393,\n",
       " 13020,\n",
       " 11,\n",
       " 14438,\n",
       " 2153,\n",
       " 702,\n",
       " 394,\n",
       " 457,\n",
       " 310,\n",
       " 1173,\n",
       " 310,\n",
       " 330,\n",
       " 6413,\n",
       " 794,\n",
       " 2330,\n",
       " 394,\n",
       " 330,\n",
       " 2588,\n",
       " 702,\n",
       " 310,\n",
       " 5243,\n",
       " 286,\n",
       " 1173,\n",
       " 286,\n",
       " 330,\n",
       " 693,\n",
       " 794,\n",
       " 341,\n",
       " 310,\n",
       " 330,\n",
       " 1337,\n",
       " 794,\n",
       " 330,\n",
       " 4174,\n",
       " 761,\n",
       " 310,\n",
       " 330,\n",
       " 4789,\n",
       " 794,\n",
       " 330,\n",
       " 791,\n",
       " 1510,\n",
       " 9499,\n",
       " 520,\n",
       " 279,\n",
       " 5300,\n",
       " 3813,\n",
       " 304,\n",
       " 279,\n",
       " 5300,\n",
       " 8316,\n",
       " 11,\n",
       " 439,\n",
       " 264,\n",
       " 2273,\n",
       " 10246,\n",
       " 286,\n",
       " 457,\n",
       " 262,\n",
       " 457,\n",
       " 633,\n",
       " 19182,\n",
       " 11,\n",
       " 1148,\n",
       " 596,\n",
       " 279,\n",
       " 9499,\n",
       " 304,\n",
       " 12366,\n",
       " 1314,\n",
       " 1457,\n",
       " 30,\n",
       " 128009,\n",
       " 128006,\n",
       " 78191,\n",
       " 128007,\n",
       " 271]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T15:19:51.529881Z",
     "start_time": "2024-08-07T15:19:51.304205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])"
   ],
   "id": "f83d3ad526466fd1",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"int\") to str",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mpipeline\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m256\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(outputs[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgenerated_text\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n",
      "File \u001B[0;32m~/anaconda3/envs/lmmgrounder/lib/python3.8/site-packages/transformers/pipelines/text_generation.py:262\u001B[0m, in \u001B[0;36mTextGenerationPipeline.__call__\u001B[0;34m(self, text_inputs, **kwargs)\u001B[0m\n\u001B[1;32m    260\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(chats, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    261\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 262\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtext_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/lmmgrounder/lib/python3.8/site-packages/transformers/pipelines/base.py:1235\u001B[0m, in \u001B[0;36mPipeline.__call__\u001B[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m can_use_iterator:\n\u001B[1;32m   1232\u001B[0m     final_iterator \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_iterator(\n\u001B[1;32m   1233\u001B[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001B[1;32m   1234\u001B[0m     )\n\u001B[0;32m-> 1235\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfinal_iterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1236\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n\u001B[1;32m   1237\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/lmmgrounder/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py:124\u001B[0m, in \u001B[0;36mPipelineIterator.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloader_batch_item()\n\u001B[1;32m    123\u001B[0m \u001B[38;5;66;03m# We're out of items within a batch\u001B[39;00m\n\u001B[0;32m--> 124\u001B[0m item \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    125\u001B[0m processed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfer(item, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparams)\n\u001B[1;32m    126\u001B[0m \u001B[38;5;66;03m# We now have a batch of \"inferred things\".\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/lmmgrounder/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py:124\u001B[0m, in \u001B[0;36mPipelineIterator.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloader_batch_item()\n\u001B[1;32m    123\u001B[0m \u001B[38;5;66;03m# We're out of items within a batch\u001B[39;00m\n\u001B[0;32m--> 124\u001B[0m item \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    125\u001B[0m processed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfer(item, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparams)\n\u001B[1;32m    126\u001B[0m \u001B[38;5;66;03m# We now have a batch of \"inferred things\".\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/lmmgrounder/lib/python3.8/site-packages/torch/utils/data/dataloader.py:633\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    631\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    632\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 633\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    634\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    635\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    636\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/anaconda3/envs/lmmgrounder/lib/python3.8/site-packages/torch/utils/data/dataloader.py:677\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    675\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    676\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 677\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    678\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    679\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/anaconda3/envs/lmmgrounder/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/anaconda3/envs/lmmgrounder/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/anaconda3/envs/lmmgrounder/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py:19\u001B[0m, in \u001B[0;36mPipelineDataset.__getitem__\u001B[0;34m(self, i)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, i):\n\u001B[1;32m     18\u001B[0m     item \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[i]\n\u001B[0;32m---> 19\u001B[0m     processed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m processed\n",
      "File \u001B[0;32m~/anaconda3/envs/lmmgrounder/lib/python3.8/site-packages/transformers/pipelines/text_generation.py:294\u001B[0m, in \u001B[0;36mTextGenerationPipeline.preprocess\u001B[0;34m(self, prompt_text, prefix, handle_long_generation, add_special_tokens, truncation, padding, max_length, **generate_kwargs)\u001B[0m\n\u001B[1;32m    286\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39mapply_chat_template(\n\u001B[1;32m    287\u001B[0m         prompt_text\u001B[38;5;241m.\u001B[39mmessages,\n\u001B[1;32m    288\u001B[0m         add_generation_prompt\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    291\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mtokenizer_kwargs,\n\u001B[1;32m    292\u001B[0m     )\n\u001B[1;32m    293\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 294\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer(\u001B[43mprefix\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mprompt_text\u001B[49m, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mtokenizer_kwargs)\n\u001B[1;32m    296\u001B[0m inputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprompt_text\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m prompt_text\n\u001B[1;32m    298\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m handle_long_generation \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhole\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "\u001B[0;31mTypeError\u001B[0m: can only concatenate str (not \"int\") to str"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3133488ba5b865e3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
