2024-08-21 22:13:11.016 | INFO     | __main__:configure_model:71 - => creating model ...
2024-08-21 22:13:11.047 | INFO     | __main__:configure_model:71 - => creating model ...
2024-08-21 22:13:11.056 | INFO     | __main__:configure_model:74 - Number of parameters: 1199882
2024-08-21 22:13:11.060 | INFO     | __main__:configure_model:74 - Number of parameters: 1199882
2024-08-21 22:13:13.241 | INFO     | trim.callbacks.misc:on_training_phase_start:70 - Namespace(block_size=None, checkpointing_steps='300', gamma=0.7, load_best_model=False, log_project='accl_test', log_tag='init_1', lr=1.0, lr_scheduler_type='linear', num_train_epochs=8, num_warmup_steps=0, output_dir='./output', per_device_eval_batch_size=1, per_device_train_batch_size=196, preprocessing_num_workers=None, report_to='all', resume_from_checkpoint='output/step_300', save_path='output/', seed=1, weight_decay=0.0, with_tracking=False)
2024-08-21 22:13:13.242 | INFO     | trim.engine.trainer:fit:125 - >>>>>>>>>>>>>>>> Start Training >>>>>>>>>>>>>>>>
2024-08-21 22:13:13.927 | INFO     | trim.callbacks.misc:on_training_phase_start:70 - Namespace(block_size=None, checkpointing_steps='300', gamma=0.7, load_best_model=False, log_project='accl_test', log_tag='init_1', lr=1.0, lr_scheduler_type='linear', num_train_epochs=8, num_warmup_steps=0, output_dir='./output', per_device_eval_batch_size=1, per_device_train_batch_size=196, preprocessing_num_workers=None, report_to='all', resume_from_checkpoint='output/step_300', save_path='output/', seed=1, weight_decay=0.0, with_tracking=False)
2024-08-21 22:13:13.927 | INFO     | trim.engine.trainer:fit:125 - >>>>>>>>>>>>>>>> Start Training >>>>>>>>>>>>>>>>
2024-08-21 22:13:15.292 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][1/77] Data 0.749 (0.749) Batch 2.049 (2.049) Remain 00:21:02 loss: 4.6114 data: -0.0020 Lr: 0.99838
2024-08-21 22:13:15.292 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][1/77] Data 0.064 (0.064) Batch 1.365 (1.365) Remain 00:14:00 loss: 4.6114 data: 0.0166 Lr: 0.99838
2024-08-21 22:13:15.368 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][2/77] Data 0.033 (0.033) Batch 0.077 (0.077) Remain 00:00:47 loss: 4.1277 data: -0.0047 Lr: 0.99675
2024-08-21 22:13:15.368 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][2/77] Data 0.030 (0.030) Batch 0.076 (0.076) Remain 00:00:46 loss: 4.1277 data: 0.0051 Lr: 0.99675
2024-08-21 22:13:15.441 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][3/77] Data 0.029 (0.031) Batch 0.073 (0.075) Remain 00:00:46 loss: 3.8913 data: 0.0001 Lr: 0.99513
2024-08-21 22:13:15.442 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][3/77] Data 0.030 (0.030) Batch 0.073 (0.075) Remain 00:00:45 loss: 3.8913 data: -0.0145 Lr: 0.99513
2024-08-21 22:13:15.515 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][4/77] Data 0.029 (0.030) Batch 0.073 (0.074) Remain 00:00:45 loss: 4.7847 data: 0.0074 Lr: 0.99351
2024-08-21 22:13:15.515 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][4/77] Data 0.029 (0.030) Batch 0.073 (0.074) Remain 00:00:45 loss: 4.7847 data: 0.0131 Lr: 0.99351
2024-08-21 22:13:15.589 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][5/77] Data 0.029 (0.030) Batch 0.074 (0.074) Remain 00:00:45 loss: 3.9611 data: -0.0012 Lr: 0.99188
2024-08-21 22:13:15.589 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][5/77] Data 0.030 (0.030) Batch 0.074 (0.074) Remain 00:00:45 loss: 3.9611 data: -0.0104 Lr: 0.99188
2024-08-21 22:13:15.662 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][6/77] Data 0.030 (0.030) Batch 0.072 (0.074) Remain 00:00:45 loss: 2.8280 data: 0.0012 Lr: 0.99026
2024-08-21 22:13:15.663 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][6/77] Data 0.030 (0.030) Batch 0.074 (0.074) Remain 00:00:45 loss: 2.8280 data: -0.0134 Lr: 0.99026
2024-08-21 22:13:15.731 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][7/77] Data 0.028 (0.030) Batch 0.068 (0.073) Remain 00:00:44 loss: 2.2321 data: 0.0075 Lr: 0.98864
2024-08-21 22:13:15.731 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][7/77] Data 0.028 (0.029) Batch 0.070 (0.073) Remain 00:00:44 loss: 2.2321 data: 0.0025 Lr: 0.98864
2024-08-21 22:13:15.797 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][8/77] Data 0.027 (0.029) Batch 0.066 (0.072) Remain 00:00:44 loss: 5.4620 data: 0.0156 Lr: 0.98701
2024-08-21 22:13:15.797 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][8/77] Data 0.027 (0.029) Batch 0.066 (0.072) Remain 00:00:43 loss: 5.4620 data: 0.0018 Lr: 0.98701
2024-08-21 22:13:15.864 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][9/77] Data 0.027 (0.029) Batch 0.067 (0.072) Remain 00:00:43 loss: 3.2855 data: 0.0009 Lr: 0.98539
2024-08-21 22:13:15.864 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][9/77] Data 0.027 (0.029) Batch 0.067 (0.071) Remain 00:00:43 loss: 3.2855 data: -0.0080 Lr: 0.98539
2024-08-21 22:13:15.930 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][10/77] Data 0.027 (0.029) Batch 0.066 (0.071) Remain 00:00:43 loss: 2.2602 data: -0.0091 Lr: 0.98377
2024-08-21 22:13:15.930 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][10/77] Data 0.027 (0.029) Batch 0.066 (0.071) Remain 00:00:43 loss: 2.2602 data: 0.0137 Lr: 0.98377
2024-08-21 22:13:16.000 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][11/77] Data 0.028 (0.029) Batch 0.070 (0.071) Remain 00:00:42 loss: 1.6412 data: 0.0052 Lr: 0.98214
2024-08-21 22:13:16.000 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][11/77] Data 0.028 (0.029) Batch 0.070 (0.071) Remain 00:00:42 loss: 1.6412 data: -0.0144 Lr: 0.98214
2024-08-21 22:13:16.070 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][12/77] Data 0.028 (0.029) Batch 0.071 (0.071) Remain 00:00:42 loss: 1.6889 data: -0.0045 Lr: 0.98052
2024-08-21 22:13:16.071 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][12/77] Data 0.028 (0.029) Batch 0.071 (0.071) Remain 00:00:42 loss: 1.6889 data: 0.0194 Lr: 0.98052
2024-08-21 22:13:16.140 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][13/77] Data 0.028 (0.029) Batch 0.070 (0.071) Remain 00:00:42 loss: 2.4939 data: -0.0132 Lr: 0.97890
2024-08-21 22:13:16.140 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][13/77] Data 0.028 (0.028) Batch 0.070 (0.071) Remain 00:00:42 loss: 2.4939 data: -0.0004 Lr: 0.97890
2024-08-21 22:13:16.213 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][14/77] Data 0.029 (0.029) Batch 0.073 (0.071) Remain 00:00:42 loss: 2.2244 data: -0.0047 Lr: 0.97727
2024-08-21 22:13:16.214 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][14/77] Data 0.029 (0.028) Batch 0.073 (0.071) Remain 00:00:42 loss: 2.2244 data: -0.0052 Lr: 0.97727
2024-08-21 22:13:16.286 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][15/77] Data 0.029 (0.029) Batch 0.073 (0.071) Remain 00:00:42 loss: 1.6076 data: -0.0027 Lr: 0.97565
2024-08-21 22:13:16.287 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][15/77] Data 0.029 (0.029) Batch 0.073 (0.071) Remain 00:00:42 loss: 1.6076 data: 0.0123 Lr: 0.97565
2024-08-21 22:13:16.360 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][16/77] Data 0.029 (0.029) Batch 0.074 (0.071) Remain 00:00:42 loss: 1.0908 data: 0.0051 Lr: 0.97403
2024-08-21 22:13:16.360 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][16/77] Data 0.029 (0.029) Batch 0.074 (0.071) Remain 00:00:42 loss: 1.0908 data: 0.0022 Lr: 0.97403
2024-08-21 22:13:16.430 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][17/77] Data 0.029 (0.029) Batch 0.069 (0.071) Remain 00:00:42 loss: 1.0574 data: -0.0046 Lr: 0.97240
2024-08-21 22:13:16.430 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][17/77] Data 0.027 (0.029) Batch 0.069 (0.071) Remain 00:00:42 loss: 1.0574 data: 0.0012 Lr: 0.97240
2024-08-21 22:13:16.498 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][18/77] Data 0.029 (0.029) Batch 0.069 (0.071) Remain 00:00:42 loss: 1.3595 data: -0.0046 Lr: 0.97078
2024-08-21 22:13:16.499 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][18/77] Data 0.028 (0.028) Batch 0.069 (0.071) Remain 00:00:42 loss: 1.3595 data: -0.0073 Lr: 0.97078
2024-08-21 22:13:16.565 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][19/77] Data 0.027 (0.029) Batch 0.067 (0.071) Remain 00:00:42 loss: 1.3803 data: 0.0046 Lr: 0.96916
2024-08-21 22:13:16.565 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][19/77] Data 0.027 (0.028) Batch 0.067 (0.071) Remain 00:00:42 loss: 1.3803 data: 0.0016 Lr: 0.96916
2024-08-21 22:13:16.636 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][20/77] Data 0.028 (0.029) Batch 0.071 (0.071) Remain 00:00:42 loss: 1.2213 data: 0.0213 Lr: 0.96753
2024-08-21 22:13:16.636 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][20/77] Data 0.030 (0.028) Batch 0.071 (0.071) Remain 00:00:42 loss: 1.2213 data: 0.0002 Lr: 0.96753
2024-08-21 22:13:16.711 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][21/77] Data 0.028 (0.028) Batch 0.075 (0.071) Remain 00:00:42 loss: 0.9231 data: 0.0067 Lr: 0.96591
2024-08-21 22:13:16.711 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][21/77] Data 0.028 (0.028) Batch 0.075 (0.071) Remain 00:00:42 loss: 0.9231 data: -0.0068 Lr: 0.96591
2024-08-21 22:13:16.779 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][22/77] Data 0.028 (0.028) Batch 0.069 (0.071) Remain 00:00:42 loss: 0.9055 data: -0.0177 Lr: 0.96429
2024-08-21 22:13:16.780 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][22/77] Data 0.028 (0.028) Batch 0.068 (0.071) Remain 00:00:42 loss: 0.9055 data: -0.0010 Lr: 0.96429
2024-08-21 22:13:16.852 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][23/77] Data 0.028 (0.028) Batch 0.073 (0.071) Remain 00:00:42 loss: 1.2298 data: -0.0053 Lr: 0.96266
2024-08-21 22:13:16.853 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][23/77] Data 0.034 (0.029) Batch 0.073 (0.071) Remain 00:00:42 loss: 1.2298 data: 0.0092 Lr: 0.96266
2024-08-21 22:13:16.921 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][24/77] Data 0.028 (0.028) Batch 0.069 (0.071) Remain 00:00:42 loss: 1.0984 data: 0.0015 Lr: 0.96104
2024-08-21 22:13:16.921 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][24/77] Data 0.028 (0.029) Batch 0.069 (0.071) Remain 00:00:41 loss: 1.0984 data: -0.0083 Lr: 0.96104
2024-08-21 22:13:16.990 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][25/77] Data 0.027 (0.028) Batch 0.069 (0.071) Remain 00:00:41 loss: 0.9578 data: 0.0146 Lr: 0.95942
2024-08-21 22:13:16.990 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][25/77] Data 0.028 (0.029) Batch 0.069 (0.071) Remain 00:00:41 loss: 0.9578 data: 0.0073 Lr: 0.95942
2024-08-21 22:13:17.060 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][26/77] Data 0.027 (0.028) Batch 0.070 (0.071) Remain 00:00:41 loss: 0.8823 data: 0.0036 Lr: 0.95779
2024-08-21 22:13:17.060 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][26/77] Data 0.028 (0.029) Batch 0.070 (0.071) Remain 00:00:41 loss: 0.8823 data: -0.0063 Lr: 0.95779
2024-08-21 22:13:17.132 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][27/77] Data 0.029 (0.028) Batch 0.072 (0.071) Remain 00:00:41 loss: 0.6521 data: 0.0060 Lr: 0.95617
2024-08-21 22:13:17.133 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][27/77] Data 0.029 (0.029) Batch 0.072 (0.071) Remain 00:00:41 loss: 0.6521 data: -0.0078 Lr: 0.95617
2024-08-21 22:13:17.200 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][28/77] Data 0.028 (0.028) Batch 0.067 (0.071) Remain 00:00:41 loss: 0.6214 data: -0.0034 Lr: 0.95455
2024-08-21 22:13:17.200 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][28/77] Data 0.028 (0.029) Batch 0.067 (0.071) Remain 00:00:41 loss: 0.6214 data: 0.0087 Lr: 0.95455
2024-08-21 22:13:17.269 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][29/77] Data 0.028 (0.028) Batch 0.069 (0.071) Remain 00:00:41 loss: 0.7130 data: 0.0020 Lr: 0.95292
2024-08-21 22:13:17.269 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][29/77] Data 0.029 (0.029) Batch 0.069 (0.071) Remain 00:00:41 loss: 0.7130 data: -0.0066 Lr: 0.95292
2024-08-21 22:13:17.342 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][30/77] Data 0.029 (0.028) Batch 0.073 (0.071) Remain 00:00:41 loss: 0.5384 data: -0.0166 Lr: 0.95130
2024-08-21 22:13:17.342 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][30/77] Data 0.030 (0.029) Batch 0.073 (0.071) Remain 00:00:41 loss: 0.5384 data: 0.0025 Lr: 0.95130
2024-08-21 22:13:17.414 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][31/77] Data 0.029 (0.028) Batch 0.073 (0.071) Remain 00:00:41 loss: 0.6237 data: -0.0011 Lr: 0.94968
2024-08-21 22:13:17.414 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][31/77] Data 0.030 (0.029) Batch 0.072 (0.071) Remain 00:00:41 loss: 0.6237 data: -0.0099 Lr: 0.94968
2024-08-21 22:13:17.486 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][32/77] Data 0.028 (0.028) Batch 0.072 (0.071) Remain 00:00:41 loss: 0.7375 data: -0.0060 Lr: 0.94805
2024-08-21 22:13:17.486 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][32/77] Data 0.028 (0.029) Batch 0.072 (0.071) Remain 00:00:41 loss: 0.7375 data: 0.0004 Lr: 0.94805
2024-08-21 22:13:17.553 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][33/77] Data 0.027 (0.028) Batch 0.067 (0.071) Remain 00:00:41 loss: 0.7248 data: 0.0095 Lr: 0.94643
2024-08-21 22:13:17.553 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][33/77] Data 0.028 (0.029) Batch 0.067 (0.071) Remain 00:00:41 loss: 0.7248 data: 0.0082 Lr: 0.94643
2024-08-21 22:13:17.634 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][34/77] Data 0.028 (0.028) Batch 0.081 (0.071) Remain 00:00:41 loss: 0.6715 data: 0.0002 Lr: 0.94481
2024-08-21 22:13:17.634 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][34/77] Data 0.039 (0.029) Batch 0.081 (0.071) Remain 00:00:41 loss: 0.6715 data: -0.0063 Lr: 0.94481
2024-08-21 22:13:17.701 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][35/77] Data 0.027 (0.028) Batch 0.067 (0.071) Remain 00:00:41 loss: 0.5328 data: -0.0063 Lr: 0.94318
2024-08-21 22:13:17.701 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][35/77] Data 0.027 (0.029) Batch 0.067 (0.071) Remain 00:00:41 loss: 0.5328 data: 0.0032 Lr: 0.94318
2024-08-21 22:13:17.768 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][36/77] Data 0.027 (0.028) Batch 0.067 (0.071) Remain 00:00:41 loss: 0.5698 data: 0.0109 Lr: 0.94156
2024-08-21 22:13:17.768 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][36/77] Data 0.028 (0.029) Batch 0.067 (0.071) Remain 00:00:41 loss: 0.5698 data: 0.0116 Lr: 0.94156
2024-08-21 22:13:17.836 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][37/77] Data 0.028 (0.028) Batch 0.068 (0.071) Remain 00:00:40 loss: 0.6656 data: -0.0038 Lr: 0.93994
2024-08-21 22:13:17.836 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][37/77] Data 0.028 (0.029) Batch 0.069 (0.071) Remain 00:00:40 loss: 0.6656 data: -0.0030 Lr: 0.93994
2024-08-21 22:13:17.901 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][38/77] Data 0.027 (0.028) Batch 0.065 (0.071) Remain 00:00:40 loss: 0.6383 data: 0.0153 Lr: 0.93831
2024-08-21 22:13:17.902 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][38/77] Data 0.028 (0.029) Batch 0.065 (0.071) Remain 00:00:40 loss: 0.6383 data: -0.0051 Lr: 0.93831
2024-08-21 22:13:17.967 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][39/77] Data 0.027 (0.028) Batch 0.065 (0.070) Remain 00:00:40 loss: 0.5394 data: 0.0052 Lr: 0.93669
2024-08-21 22:13:17.967 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][39/77] Data 0.028 (0.029) Batch 0.065 (0.070) Remain 00:00:40 loss: 0.5394 data: -0.0065 Lr: 0.93669
2024-08-21 22:13:18.035 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][40/77] Data 0.027 (0.028) Batch 0.068 (0.070) Remain 00:00:40 loss: 0.5079 data: -0.0181 Lr: 0.93506
2024-08-21 22:13:18.035 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][40/77] Data 0.028 (0.029) Batch 0.068 (0.070) Remain 00:00:40 loss: 0.5079 data: 0.0227 Lr: 0.93506
2024-08-21 22:13:18.106 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][41/77] Data 0.029 (0.028) Batch 0.071 (0.070) Remain 00:00:40 loss: 0.5105 data: 0.0090 Lr: 0.93344
2024-08-21 22:13:18.106 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][41/77] Data 0.029 (0.029) Batch 0.071 (0.070) Remain 00:00:40 loss: 0.5105 data: -0.0145 Lr: 0.93344
2024-08-21 22:13:18.180 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][42/77] Data 0.029 (0.028) Batch 0.075 (0.070) Remain 00:00:40 loss: 0.5546 data: -0.0162 Lr: 0.93182
2024-08-21 22:13:18.181 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][42/77] Data 0.029 (0.029) Batch 0.075 (0.070) Remain 00:00:40 loss: 0.5546 data: -0.0042 Lr: 0.93182
2024-08-21 22:13:18.258 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][43/77] Data 0.035 (0.028) Batch 0.078 (0.071) Remain 00:00:40 loss: 0.5683 data: -0.0053 Lr: 0.93019
2024-08-21 22:13:18.258 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][43/77] Data 0.030 (0.029) Batch 0.078 (0.071) Remain 00:00:40 loss: 0.5683 data: 0.0085 Lr: 0.93019
2024-08-21 22:13:18.324 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][44/77] Data 0.027 (0.028) Batch 0.066 (0.071) Remain 00:00:40 loss: 0.6073 data: 0.0113 Lr: 0.92857
2024-08-21 22:13:18.325 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][44/77] Data 0.028 (0.029) Batch 0.066 (0.071) Remain 00:00:40 loss: 0.6073 data: -0.0014 Lr: 0.92857
2024-08-21 22:13:18.391 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][45/77] Data 0.028 (0.028) Batch 0.067 (0.070) Remain 00:00:40 loss: 0.6232 data: -0.0135 Lr: 0.92695
2024-08-21 22:13:18.391 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][45/77] Data 0.028 (0.029) Batch 0.067 (0.070) Remain 00:00:40 loss: 0.6232 data: 0.0041 Lr: 0.92695
2024-08-21 22:13:18.456 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][46/77] Data 0.027 (0.028) Batch 0.065 (0.070) Remain 00:00:40 loss: 0.4704 data: 0.0038 Lr: 0.92532
2024-08-21 22:13:18.456 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][46/77] Data 0.028 (0.029) Batch 0.065 (0.070) Remain 00:00:40 loss: 0.4704 data: 0.0097 Lr: 0.92532
2024-08-21 22:13:18.522 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][47/77] Data 0.028 (0.028) Batch 0.067 (0.070) Remain 00:00:40 loss: 0.5077 data: -0.0026 Lr: 0.92370
2024-08-21 22:13:18.523 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][47/77] Data 0.028 (0.029) Batch 0.067 (0.070) Remain 00:00:40 loss: 0.5077 data: 0.0149 Lr: 0.92370
2024-08-21 22:13:18.589 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][48/77] Data 0.027 (0.028) Batch 0.067 (0.070) Remain 00:00:39 loss: 0.5481 data: 0.0026 Lr: 0.92208
2024-08-21 22:13:18.589 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][48/77] Data 0.028 (0.029) Batch 0.067 (0.070) Remain 00:00:39 loss: 0.5481 data: 0.0116 Lr: 0.92208
2024-08-21 22:13:18.656 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][49/77] Data 0.028 (0.028) Batch 0.067 (0.070) Remain 00:00:39 loss: 0.4510 data: 0.0012 Lr: 0.92045
2024-08-21 22:13:18.656 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][49/77] Data 0.028 (0.029) Batch 0.067 (0.070) Remain 00:00:39 loss: 0.4510 data: 0.0008 Lr: 0.92045
2024-08-21 22:13:18.723 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][50/77] Data 0.028 (0.028) Batch 0.067 (0.070) Remain 00:00:39 loss: 0.4964 data: 0.0113 Lr: 0.91883
2024-08-21 22:13:18.723 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_50
2024-08-21 22:13:18.723 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][50/77] Data 0.028 (0.029) Batch 0.067 (0.070) Remain 00:00:39 loss: 0.4964 data: 0.0039 Lr: 0.91883
2024-08-21 22:13:18.724 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_50
2024-08-21 22:13:18.754 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -223.38320922851562
2024-08-21 22:13:18.754 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -223.38320922851562
2024-08-21 22:13:18.754 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -115.26531982421875
2024-08-21 22:13:18.754 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -108.11787414550781
2024-08-21 22:13:18.836 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][51/77] Data 0.060 (0.029) Batch 0.113 (0.071) Remain 00:00:40 loss: 0.4194 data: 0.0059 Lr: 0.91721
2024-08-21 22:13:18.836 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][51/77] Data 0.060 (0.029) Batch 0.113 (0.071) Remain 00:00:40 loss: 0.4194 data: 0.0113 Lr: 0.91721
2024-08-21 22:13:18.919 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][52/77] Data 0.035 (0.029) Batch 0.084 (0.071) Remain 00:00:40 loss: 0.4249 data: 0.0255 Lr: 0.91558
2024-08-21 22:13:18.920 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][52/77] Data 0.033 (0.029) Batch 0.084 (0.071) Remain 00:00:40 loss: 0.4249 data: -0.0018 Lr: 0.91558
2024-08-21 22:13:18.994 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][53/77] Data 0.030 (0.029) Batch 0.075 (0.071) Remain 00:00:40 loss: 0.4813 data: -0.0151 Lr: 0.91396
2024-08-21 22:13:18.994 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][53/77] Data 0.029 (0.029) Batch 0.075 (0.071) Remain 00:00:40 loss: 0.4813 data: 0.0226 Lr: 0.91396
2024-08-21 22:13:19.092 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][54/77] Data 0.030 (0.029) Batch 0.098 (0.072) Remain 00:00:40 loss: 0.3326 data: -0.0040 Lr: 0.91234
2024-08-21 22:13:19.093 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][54/77] Data 0.029 (0.029) Batch 0.098 (0.072) Remain 00:00:40 loss: 0.3326 data: -0.0010 Lr: 0.91234
2024-08-21 22:13:19.181 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][55/77] Data 0.031 (0.029) Batch 0.089 (0.072) Remain 00:00:40 loss: 0.3743 data: 0.0194 Lr: 0.91071
2024-08-21 22:13:19.182 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][55/77] Data 0.038 (0.029) Batch 0.089 (0.072) Remain 00:00:40 loss: 0.3743 data: 0.0157 Lr: 0.91071
2024-08-21 22:13:19.264 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][56/77] Data 0.036 (0.029) Batch 0.082 (0.072) Remain 00:00:40 loss: 0.3336 data: -0.0027 Lr: 0.90909
2024-08-21 22:13:19.264 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][56/77] Data 0.030 (0.030) Batch 0.083 (0.072) Remain 00:00:40 loss: 0.3336 data: -0.0060 Lr: 0.90909
2024-08-21 22:13:19.339 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][57/77] Data 0.030 (0.029) Batch 0.075 (0.072) Remain 00:00:40 loss: 0.3566 data: -0.0072 Lr: 0.90747
2024-08-21 22:13:19.339 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][57/77] Data 0.030 (0.030) Batch 0.075 (0.072) Remain 00:00:40 loss: 0.3566 data: -0.0121 Lr: 0.90747
2024-08-21 22:13:19.414 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][58/77] Data 0.030 (0.029) Batch 0.076 (0.072) Remain 00:00:40 loss: 0.3554 data: 0.0202 Lr: 0.90584
2024-08-21 22:13:19.414 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][58/77] Data 0.030 (0.030) Batch 0.076 (0.072) Remain 00:00:40 loss: 0.3554 data: -0.0011 Lr: 0.90584
2024-08-21 22:13:19.489 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][59/77] Data 0.030 (0.029) Batch 0.075 (0.072) Remain 00:00:40 loss: 0.3607 data: -0.0228 Lr: 0.90422
2024-08-21 22:13:19.489 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][59/77] Data 0.030 (0.030) Batch 0.075 (0.072) Remain 00:00:40 loss: 0.3607 data: -0.0205 Lr: 0.90422
2024-08-21 22:13:19.563 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][60/77] Data 0.029 (0.029) Batch 0.074 (0.072) Remain 00:00:40 loss: 0.3259 data: 0.0153 Lr: 0.90260
2024-08-21 22:13:19.563 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][60/77] Data 0.030 (0.030) Batch 0.074 (0.072) Remain 00:00:40 loss: 0.3259 data: 0.0115 Lr: 0.90260
2024-08-21 22:13:19.633 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][61/77] Data 0.029 (0.029) Batch 0.070 (0.072) Remain 00:00:40 loss: 0.5034 data: 0.0110 Lr: 0.90097
2024-08-21 22:13:19.633 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][61/77] Data 0.029 (0.030) Batch 0.070 (0.072) Remain 00:00:40 loss: 0.5034 data: -0.0004 Lr: 0.90097
2024-08-21 22:13:19.708 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][62/77] Data 0.034 (0.029) Batch 0.075 (0.072) Remain 00:00:40 loss: 0.3727 data: 0.0010 Lr: 0.89935
2024-08-21 22:13:19.708 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][62/77] Data 0.028 (0.030) Batch 0.075 (0.072) Remain 00:00:40 loss: 0.3727 data: -0.0151 Lr: 0.89935
2024-08-21 22:13:19.775 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][63/77] Data 0.028 (0.029) Batch 0.067 (0.072) Remain 00:00:40 loss: 0.4684 data: 0.0141 Lr: 0.89773
2024-08-21 22:13:19.776 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][63/77] Data 0.028 (0.029) Batch 0.067 (0.072) Remain 00:00:40 loss: 0.4684 data: 0.0257 Lr: 0.89773
2024-08-21 22:13:19.842 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][64/77] Data 0.028 (0.029) Batch 0.067 (0.072) Remain 00:00:39 loss: 0.4913 data: 0.0234 Lr: 0.89610
2024-08-21 22:13:19.843 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][64/77] Data 0.028 (0.029) Batch 0.067 (0.072) Remain 00:00:39 loss: 0.4913 data: -0.0081 Lr: 0.89610
2024-08-21 22:13:19.913 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][65/77] Data 0.028 (0.029) Batch 0.070 (0.072) Remain 00:00:39 loss: 0.6195 data: 0.0046 Lr: 0.89448
2024-08-21 22:13:19.913 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][65/77] Data 0.028 (0.029) Batch 0.071 (0.072) Remain 00:00:39 loss: 0.6195 data: -0.0014 Lr: 0.89448
2024-08-21 22:13:19.982 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][66/77] Data 0.029 (0.029) Batch 0.069 (0.072) Remain 00:00:39 loss: 0.3913 data: -0.0121 Lr: 0.89286
2024-08-21 22:13:19.982 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][66/77] Data 0.029 (0.029) Batch 0.069 (0.072) Remain 00:00:39 loss: 0.3913 data: -0.0012 Lr: 0.89286
2024-08-21 22:13:20.049 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][67/77] Data 0.028 (0.029) Batch 0.066 (0.072) Remain 00:00:39 loss: 0.3400 data: -0.0150 Lr: 0.89123
2024-08-21 22:13:20.049 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][67/77] Data 0.028 (0.029) Batch 0.066 (0.072) Remain 00:00:39 loss: 0.3400 data: 0.0110 Lr: 0.89123
2024-08-21 22:13:20.117 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][68/77] Data 0.028 (0.029) Batch 0.068 (0.072) Remain 00:00:39 loss: 0.3087 data: -0.0077 Lr: 0.88961
2024-08-21 22:13:20.117 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][68/77] Data 0.027 (0.029) Batch 0.068 (0.072) Remain 00:00:39 loss: 0.3087 data: -0.0059 Lr: 0.88961
2024-08-21 22:13:20.185 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][69/77] Data 0.028 (0.029) Batch 0.068 (0.072) Remain 00:00:39 loss: 0.3069 data: 0.0118 Lr: 0.88799
2024-08-21 22:13:20.185 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][69/77] Data 0.028 (0.029) Batch 0.068 (0.072) Remain 00:00:39 loss: 0.3069 data: -0.0006 Lr: 0.88799
2024-08-21 22:13:20.262 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][70/77] Data 0.029 (0.029) Batch 0.077 (0.072) Remain 00:00:39 loss: 0.3505 data: -0.0061 Lr: 0.88636
2024-08-21 22:13:20.262 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][70/77] Data 0.034 (0.029) Batch 0.077 (0.072) Remain 00:00:39 loss: 0.3505 data: -0.0068 Lr: 0.88636
2024-08-21 22:13:20.338 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][71/77] Data 0.029 (0.029) Batch 0.076 (0.072) Remain 00:00:39 loss: 0.3158 data: 0.0023 Lr: 0.88474
2024-08-21 22:13:20.338 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][71/77] Data 0.029 (0.029) Batch 0.076 (0.072) Remain 00:00:39 loss: 0.3158 data: 0.0071 Lr: 0.88474
2024-08-21 22:13:20.413 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][72/77] Data 0.029 (0.029) Batch 0.074 (0.072) Remain 00:00:39 loss: 0.3641 data: -0.0090 Lr: 0.88312
2024-08-21 22:13:20.413 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][72/77] Data 0.029 (0.029) Batch 0.074 (0.072) Remain 00:00:39 loss: 0.3641 data: -0.0034 Lr: 0.88312
2024-08-21 22:13:20.484 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][73/77] Data 0.029 (0.029) Batch 0.071 (0.072) Remain 00:00:39 loss: 0.3541 data: 0.0081 Lr: 0.88149
2024-08-21 22:13:20.484 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][73/77] Data 0.029 (0.029) Batch 0.071 (0.072) Remain 00:00:39 loss: 0.3541 data: -0.0024 Lr: 0.88149
2024-08-21 22:13:20.564 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][74/77] Data 0.029 (0.029) Batch 0.080 (0.072) Remain 00:00:39 loss: 0.2666 data: -0.0020 Lr: 0.87987
2024-08-21 22:13:20.564 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][74/77] Data 0.028 (0.029) Batch 0.080 (0.072) Remain 00:00:39 loss: 0.2666 data: 0.0154 Lr: 0.87987
2024-08-21 22:13:20.630 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][75/77] Data 0.028 (0.029) Batch 0.066 (0.072) Remain 00:00:39 loss: 0.2669 data: 0.0045 Lr: 0.87825
2024-08-21 22:13:20.630 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][75/77] Data 0.028 (0.029) Batch 0.066 (0.072) Remain 00:00:39 loss: 0.2669 data: -0.0077 Lr: 0.87825
2024-08-21 22:13:20.697 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][76/77] Data 0.028 (0.029) Batch 0.067 (0.072) Remain 00:00:38 loss: 0.4222 data: 0.0079 Lr: 0.87662
2024-08-21 22:13:20.697 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][76/77] Data 0.027 (0.029) Batch 0.067 (0.072) Remain 00:00:38 loss: 0.4222 data: 0.0015 Lr: 0.87662
2024-08-21 22:13:20.739 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][77/77] Data 0.031 (0.029) Batch 0.042 (0.072) Remain 00:00:38 loss: 0.3256 data: -0.0059 Lr: 0.87500
2024-08-21 22:13:20.739 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 22:13:20.739 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][77/77] Data 0.031 (0.029) Batch 0.042 (0.072) Remain 00:00:38 loss: 0.3256 data: -0.0010 Lr: 0.87500
2024-08-21 22:13:20.739 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 22:13:25.072 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0951, Accuracy: 0.9731
2024-08-21 22:13:25.073 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 22:13:25.073 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0951, Accuracy: 0.9731
2024-08-21 22:13:25.073 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 22:13:25.077 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 22:13:25.077 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 22:13:25.157 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][1/77] Data 0.044 (0.044) Batch 0.080 (0.080) Remain 00:00:43 loss: 0.3104 data: 0.0100 Lr: 0.87338
2024-08-21 22:13:25.158 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][1/77] Data 0.044 (0.044) Batch 0.080 (0.080) Remain 00:00:43 loss: 0.3104 data: 0.0028 Lr: 0.87338
2024-08-21 22:13:25.213 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][2/77] Data 0.023 (0.023) Batch 0.056 (0.056) Remain 00:00:30 loss: 0.3229 data: -0.0113 Lr: 0.87175
2024-08-21 22:13:25.214 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][2/77] Data 0.022 (0.022) Batch 0.056 (0.056) Remain 00:00:30 loss: 0.3229 data: 0.0047 Lr: 0.87175
2024-08-21 22:13:25.270 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][3/77] Data 0.024 (0.024) Batch 0.056 (0.056) Remain 00:00:30 loss: 0.3184 data: -0.0137 Lr: 0.87013
2024-08-21 22:13:25.270 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][3/77] Data 0.023 (0.022) Batch 0.056 (0.056) Remain 00:00:30 loss: 0.3184 data: 0.0043 Lr: 0.87013
2024-08-21 22:13:25.325 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][4/77] Data 0.022 (0.023) Batch 0.055 (0.056) Remain 00:00:29 loss: 0.3552 data: 0.0291 Lr: 0.86851
2024-08-21 22:13:25.325 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][4/77] Data 0.022 (0.022) Batch 0.055 (0.056) Remain 00:00:29 loss: 0.3552 data: -0.0114 Lr: 0.86851
2024-08-21 22:13:25.382 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][5/77] Data 0.023 (0.023) Batch 0.057 (0.056) Remain 00:00:30 loss: 0.3922 data: 0.0033 Lr: 0.86688
2024-08-21 22:13:25.382 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][5/77] Data 0.022 (0.022) Batch 0.057 (0.056) Remain 00:00:30 loss: 0.3922 data: 0.0068 Lr: 0.86688
2024-08-21 22:13:25.440 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][6/77] Data 0.024 (0.023) Batch 0.058 (0.056) Remain 00:00:30 loss: 0.3161 data: 0.0031 Lr: 0.86526
2024-08-21 22:13:25.440 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][6/77] Data 0.021 (0.022) Batch 0.058 (0.056) Remain 00:00:30 loss: 0.3161 data: -0.0023 Lr: 0.86526
2024-08-21 22:13:25.496 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][7/77] Data 0.023 (0.023) Batch 0.056 (0.056) Remain 00:00:30 loss: 0.3084 data: 0.0027 Lr: 0.86364
2024-08-21 22:13:25.496 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][7/77] Data 0.021 (0.022) Batch 0.056 (0.056) Remain 00:00:30 loss: 0.3084 data: -0.0093 Lr: 0.86364
2024-08-21 22:13:25.552 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][8/77] Data 0.023 (0.023) Batch 0.057 (0.056) Remain 00:00:30 loss: 0.2192 data: -0.0128 Lr: 0.86201
2024-08-21 22:13:25.553 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][8/77] Data 0.022 (0.022) Batch 0.057 (0.056) Remain 00:00:30 loss: 0.2192 data: -0.0005 Lr: 0.86201
2024-08-21 22:13:25.610 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][9/77] Data 0.024 (0.023) Batch 0.057 (0.057) Remain 00:00:30 loss: 0.2662 data: -0.0067 Lr: 0.86039
2024-08-21 22:13:25.610 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][9/77] Data 0.022 (0.022) Batch 0.057 (0.057) Remain 00:00:30 loss: 0.2662 data: -0.0131 Lr: 0.86039
2024-08-21 22:13:25.666 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][10/77] Data 0.024 (0.023) Batch 0.056 (0.057) Remain 00:00:29 loss: 0.3167 data: -0.0018 Lr: 0.85877
2024-08-21 22:13:25.666 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][10/77] Data 0.022 (0.022) Batch 0.056 (0.057) Remain 00:00:29 loss: 0.3167 data: 0.0008 Lr: 0.85877
2024-08-21 22:13:25.722 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][11/77] Data 0.023 (0.023) Batch 0.056 (0.057) Remain 00:00:29 loss: 0.3123 data: -0.0030 Lr: 0.85714
2024-08-21 22:13:25.723 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][11/77] Data 0.022 (0.022) Batch 0.057 (0.057) Remain 00:00:29 loss: 0.3123 data: -0.0072 Lr: 0.85714
2024-08-21 22:13:25.780 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][12/77] Data 0.024 (0.023) Batch 0.058 (0.057) Remain 00:00:29 loss: 0.2207 data: 0.0053 Lr: 0.85552
2024-08-21 22:13:25.780 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][12/77] Data 0.022 (0.022) Batch 0.057 (0.057) Remain 00:00:29 loss: 0.2207 data: 0.0176 Lr: 0.85552
2024-08-21 22:13:25.836 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][13/77] Data 0.023 (0.023) Batch 0.056 (0.057) Remain 00:00:29 loss: 0.2682 data: 0.0083 Lr: 0.85390
2024-08-21 22:13:25.836 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][13/77] Data 0.022 (0.022) Batch 0.056 (0.057) Remain 00:00:29 loss: 0.2682 data: 0.0099 Lr: 0.85390
2024-08-21 22:13:25.891 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][14/77] Data 0.023 (0.023) Batch 0.056 (0.056) Remain 00:00:29 loss: 0.2620 data: -0.0109 Lr: 0.85227
2024-08-21 22:13:25.892 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][14/77] Data 0.022 (0.022) Batch 0.056 (0.056) Remain 00:00:29 loss: 0.2620 data: -0.0031 Lr: 0.85227
2024-08-21 22:13:25.950 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][15/77] Data 0.022 (0.022) Batch 0.058 (0.057) Remain 00:00:29 loss: 0.2340 data: -0.0017 Lr: 0.85065
2024-08-21 22:13:25.950 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][15/77] Data 0.025 (0.023) Batch 0.058 (0.057) Remain 00:00:29 loss: 0.2340 data: -0.0187 Lr: 0.85065
2024-08-21 22:13:26.007 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][16/77] Data 0.024 (0.024) Batch 0.057 (0.057) Remain 00:00:29 loss: 0.2758 data: -0.0092 Lr: 0.84903
2024-08-21 22:13:26.007 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][16/77] Data 0.022 (0.022) Batch 0.057 (0.057) Remain 00:00:29 loss: 0.2758 data: -0.0074 Lr: 0.84903
2024-08-21 22:13:26.070 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][17/77] Data 0.022 (0.023) Batch 0.064 (0.057) Remain 00:00:29 loss: 0.3242 data: 0.0063 Lr: 0.84740
2024-08-21 22:13:26.070 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][17/77] Data 0.022 (0.022) Batch 0.064 (0.057) Remain 00:00:29 loss: 0.3242 data: 0.0055 Lr: 0.84740
2024-08-21 22:13:26.136 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][18/77] Data 0.027 (0.024) Batch 0.066 (0.058) Remain 00:00:30 loss: 0.3490 data: -0.0094 Lr: 0.84578
2024-08-21 22:13:26.136 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][18/77] Data 0.021 (0.022) Batch 0.066 (0.058) Remain 00:00:30 loss: 0.3490 data: -0.0130 Lr: 0.84578
2024-08-21 22:13:26.201 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][19/77] Data 0.027 (0.024) Batch 0.065 (0.058) Remain 00:00:30 loss: 0.1948 data: -0.0153 Lr: 0.84416
2024-08-21 22:13:26.201 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][19/77] Data 0.021 (0.022) Batch 0.065 (0.058) Remain 00:00:30 loss: 0.1948 data: 0.0081 Lr: 0.84416
2024-08-21 22:13:26.267 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][20/77] Data 0.027 (0.024) Batch 0.065 (0.058) Remain 00:00:30 loss: 0.3743 data: -0.0177 Lr: 0.84253
2024-08-21 22:13:26.267 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][20/77] Data 0.021 (0.022) Batch 0.065 (0.058) Remain 00:00:30 loss: 0.3743 data: -0.0091 Lr: 0.84253
2024-08-21 22:13:26.332 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][21/77] Data 0.027 (0.024) Batch 0.065 (0.059) Remain 00:00:30 loss: 0.2980 data: -0.0022 Lr: 0.84091
2024-08-21 22:13:26.332 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][21/77] Data 0.021 (0.022) Batch 0.065 (0.059) Remain 00:00:30 loss: 0.2980 data: -0.0036 Lr: 0.84091
2024-08-21 22:13:26.397 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][22/77] Data 0.027 (0.024) Batch 0.065 (0.059) Remain 00:00:30 loss: 0.2895 data: -0.0025 Lr: 0.83929
2024-08-21 22:13:26.397 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][22/77] Data 0.021 (0.022) Batch 0.065 (0.059) Remain 00:00:30 loss: 0.2895 data: 0.0054 Lr: 0.83929
2024-08-21 22:13:26.462 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][23/77] Data 0.027 (0.025) Batch 0.065 (0.059) Remain 00:00:30 loss: 0.2814 data: -0.0095 Lr: 0.83766
2024-08-21 22:13:26.462 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][23/77] Data 0.022 (0.022) Batch 0.065 (0.059) Remain 00:00:30 loss: 0.2814 data: -0.0070 Lr: 0.83766
2024-08-21 22:13:26.462 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_100
2024-08-21 22:13:26.462 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_100
2024-08-21 22:13:26.483 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -333.2380676269531
2024-08-21 22:13:26.484 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -163.90542602539062
2024-08-21 22:13:26.484 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -333.2380676269531
2024-08-21 22:13:26.484 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -169.3326416015625
2024-08-21 22:13:26.550 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][24/77] Data 0.049 (0.026) Batch 0.088 (0.061) Remain 00:00:31 loss: 0.2858 data: -0.0063 Lr: 0.83604
2024-08-21 22:13:26.550 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][24/77] Data 0.043 (0.023) Batch 0.088 (0.061) Remain 00:00:31 loss: 0.2858 data: 0.0036 Lr: 0.83604
2024-08-21 22:13:26.620 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][25/77] Data 0.033 (0.026) Batch 0.070 (0.061) Remain 00:00:31 loss: 0.2697 data: 0.0165 Lr: 0.83442
2024-08-21 22:13:26.620 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][25/77] Data 0.021 (0.023) Batch 0.070 (0.061) Remain 00:00:31 loss: 0.2697 data: 0.0106 Lr: 0.83442
2024-08-21 22:13:26.685 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][26/77] Data 0.027 (0.026) Batch 0.064 (0.061) Remain 00:00:31 loss: 0.2188 data: -0.0164 Lr: 0.83279
2024-08-21 22:13:26.685 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][26/77] Data 0.021 (0.023) Batch 0.064 (0.061) Remain 00:00:31 loss: 0.2188 data: -0.0029 Lr: 0.83279
2024-08-21 22:13:26.750 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][27/77] Data 0.027 (0.026) Batch 0.065 (0.061) Remain 00:00:31 loss: 0.3187 data: 0.0028 Lr: 0.83117
2024-08-21 22:13:26.750 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][27/77] Data 0.022 (0.023) Batch 0.065 (0.061) Remain 00:00:31 loss: 0.3187 data: 0.0169 Lr: 0.83117
2024-08-21 22:13:26.812 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][28/77] Data 0.027 (0.026) Batch 0.063 (0.061) Remain 00:00:31 loss: 0.2331 data: -0.0073 Lr: 0.82955
2024-08-21 22:13:26.813 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][28/77] Data 0.022 (0.023) Batch 0.063 (0.061) Remain 00:00:31 loss: 0.2331 data: -0.0012 Lr: 0.82955
2024-08-21 22:13:26.877 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][29/77] Data 0.027 (0.026) Batch 0.065 (0.061) Remain 00:00:31 loss: 0.3149 data: -0.0012 Lr: 0.82792
2024-08-21 22:13:26.877 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][29/77] Data 0.021 (0.022) Batch 0.065 (0.061) Remain 00:00:31 loss: 0.3149 data: -0.0033 Lr: 0.82792
2024-08-21 22:13:26.942 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][30/77] Data 0.027 (0.026) Batch 0.065 (0.062) Remain 00:00:31 loss: 0.2111 data: -0.0090 Lr: 0.82630
2024-08-21 22:13:26.942 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][30/77] Data 0.022 (0.022) Batch 0.065 (0.062) Remain 00:00:31 loss: 0.2111 data: -0.0072 Lr: 0.82630
2024-08-21 22:13:27.007 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][31/77] Data 0.027 (0.026) Batch 0.065 (0.062) Remain 00:00:31 loss: 0.3079 data: 0.0136 Lr: 0.82468
2024-08-21 22:13:27.007 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][31/77] Data 0.022 (0.022) Batch 0.065 (0.062) Remain 00:00:31 loss: 0.3079 data: -0.0186 Lr: 0.82468
2024-08-21 22:13:27.072 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][32/77] Data 0.027 (0.026) Batch 0.065 (0.062) Remain 00:00:31 loss: 0.2183 data: 0.0071 Lr: 0.82305
2024-08-21 22:13:27.072 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][32/77] Data 0.021 (0.022) Batch 0.065 (0.062) Remain 00:00:31 loss: 0.2183 data: -0.0037 Lr: 0.82305
2024-08-21 22:13:27.137 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][33/77] Data 0.027 (0.026) Batch 0.065 (0.062) Remain 00:00:31 loss: 0.2403 data: 0.0149 Lr: 0.82143
2024-08-21 22:13:27.137 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][33/77] Data 0.023 (0.022) Batch 0.065 (0.062) Remain 00:00:31 loss: 0.2403 data: -0.0027 Lr: 0.82143
2024-08-21 22:13:27.202 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][34/77] Data 0.027 (0.026) Batch 0.065 (0.062) Remain 00:00:31 loss: 0.3028 data: 0.0117 Lr: 0.81981
2024-08-21 22:13:27.202 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][34/77] Data 0.022 (0.022) Batch 0.065 (0.062) Remain 00:00:31 loss: 0.3028 data: -0.0164 Lr: 0.81981
2024-08-21 22:13:27.266 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][35/77] Data 0.027 (0.026) Batch 0.065 (0.062) Remain 00:00:31 loss: 0.2077 data: 0.0046 Lr: 0.81818
2024-08-21 22:13:27.267 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][35/77] Data 0.022 (0.022) Batch 0.065 (0.062) Remain 00:00:31 loss: 0.2077 data: 0.0049 Lr: 0.81818
2024-08-21 22:13:27.331 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][36/77] Data 0.027 (0.026) Batch 0.065 (0.062) Remain 00:00:31 loss: 0.2814 data: -0.0107 Lr: 0.81656
2024-08-21 22:13:27.331 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][36/77] Data 0.022 (0.022) Batch 0.065 (0.062) Remain 00:00:31 loss: 0.2814 data: -0.0141 Lr: 0.81656
2024-08-21 22:13:27.397 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][37/77] Data 0.027 (0.026) Batch 0.065 (0.062) Remain 00:00:31 loss: 0.1938 data: -0.0088 Lr: 0.81494
2024-08-21 22:13:27.397 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][37/77] Data 0.022 (0.022) Batch 0.065 (0.062) Remain 00:00:31 loss: 0.1938 data: 0.0044 Lr: 0.81494
2024-08-21 22:13:27.462 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][38/77] Data 0.027 (0.026) Batch 0.065 (0.062) Remain 00:00:31 loss: 0.2396 data: 0.0169 Lr: 0.81331
2024-08-21 22:13:27.462 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][38/77] Data 0.022 (0.022) Batch 0.065 (0.062) Remain 00:00:31 loss: 0.2396 data: 0.0043 Lr: 0.81331
2024-08-21 22:13:27.526 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][39/77] Data 0.027 (0.026) Batch 0.064 (0.062) Remain 00:00:31 loss: 0.2582 data: 0.0070 Lr: 0.81169
2024-08-21 22:13:27.526 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][39/77] Data 0.022 (0.022) Batch 0.064 (0.062) Remain 00:00:31 loss: 0.2582 data: 0.0037 Lr: 0.81169
2024-08-21 22:13:27.591 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][40/77] Data 0.027 (0.026) Batch 0.065 (0.062) Remain 00:00:31 loss: 0.1748 data: 0.0044 Lr: 0.81006
2024-08-21 22:13:27.591 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][40/77] Data 0.021 (0.022) Batch 0.065 (0.062) Remain 00:00:31 loss: 0.1748 data: -0.0223 Lr: 0.81006
2024-08-21 22:13:27.647 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][41/77] Data 0.023 (0.026) Batch 0.056 (0.062) Remain 00:00:31 loss: 0.2578 data: 0.0002 Lr: 0.80844
2024-08-21 22:13:27.647 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][41/77] Data 0.022 (0.022) Batch 0.056 (0.062) Remain 00:00:31 loss: 0.2578 data: 0.0003 Lr: 0.80844
2024-08-21 22:13:27.703 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][42/77] Data 0.023 (0.026) Batch 0.056 (0.062) Remain 00:00:30 loss: 0.2609 data: -0.0108 Lr: 0.80682
2024-08-21 22:13:27.703 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][42/77] Data 0.022 (0.022) Batch 0.056 (0.062) Remain 00:00:30 loss: 0.2609 data: -0.0043 Lr: 0.80682
2024-08-21 22:13:27.759 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][43/77] Data 0.022 (0.026) Batch 0.056 (0.062) Remain 00:00:30 loss: 0.4019 data: -0.0178 Lr: 0.80519
2024-08-21 22:13:27.759 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][43/77] Data 0.022 (0.022) Batch 0.056 (0.062) Remain 00:00:30 loss: 0.4019 data: -0.0002 Lr: 0.80519
2024-08-21 22:13:27.813 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][44/77] Data 0.022 (0.026) Batch 0.054 (0.062) Remain 00:00:30 loss: 0.4527 data: -0.0085 Lr: 0.80357
2024-08-21 22:13:27.813 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][44/77] Data 0.023 (0.022) Batch 0.054 (0.062) Remain 00:00:30 loss: 0.4527 data: 0.0022 Lr: 0.80357
2024-08-21 22:13:27.869 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][45/77] Data 0.022 (0.026) Batch 0.056 (0.062) Remain 00:00:30 loss: 0.2723 data: -0.0076 Lr: 0.80195
2024-08-21 22:13:27.869 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][45/77] Data 0.022 (0.022) Batch 0.056 (0.062) Remain 00:00:30 loss: 0.2723 data: -0.0029 Lr: 0.80195
2024-08-21 22:13:27.934 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][46/77] Data 0.023 (0.026) Batch 0.065 (0.062) Remain 00:00:30 loss: 0.2986 data: -0.0116 Lr: 0.80032
2024-08-21 22:13:27.934 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][46/77] Data 0.023 (0.022) Batch 0.065 (0.062) Remain 00:00:30 loss: 0.2986 data: 0.0039 Lr: 0.80032
2024-08-21 22:13:27.988 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][47/77] Data 0.021 (0.026) Batch 0.054 (0.062) Remain 00:00:30 loss: 0.2841 data: -0.0083 Lr: 0.79870
2024-08-21 22:13:27.989 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][47/77] Data 0.022 (0.022) Batch 0.054 (0.062) Remain 00:00:30 loss: 0.2841 data: 0.0109 Lr: 0.79870
2024-08-21 22:13:28.074 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][48/77] Data 0.023 (0.026) Batch 0.086 (0.062) Remain 00:00:30 loss: 0.1764 data: -0.0018 Lr: 0.79708
2024-08-21 22:13:28.074 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][48/77] Data 0.037 (0.023) Batch 0.086 (0.062) Remain 00:00:30 loss: 0.1764 data: 0.0121 Lr: 0.79708
2024-08-21 22:13:28.142 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][49/77] Data 0.027 (0.026) Batch 0.068 (0.062) Remain 00:00:30 loss: 0.3041 data: -0.0057 Lr: 0.79545
2024-08-21 22:13:28.142 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][49/77] Data 0.028 (0.023) Batch 0.068 (0.062) Remain 00:00:30 loss: 0.3041 data: -0.0056 Lr: 0.79545
2024-08-21 22:13:28.212 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][50/77] Data 0.028 (0.026) Batch 0.071 (0.062) Remain 00:00:30 loss: 0.2695 data: -0.0059 Lr: 0.79383
2024-08-21 22:13:28.212 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][50/77] Data 0.028 (0.023) Batch 0.071 (0.062) Remain 00:00:30 loss: 0.2695 data: -0.0177 Lr: 0.79383
2024-08-21 22:13:28.292 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][51/77] Data 0.027 (0.026) Batch 0.080 (0.063) Remain 00:00:30 loss: 0.1977 data: 0.0019 Lr: 0.79221
2024-08-21 22:13:28.292 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][51/77] Data 0.033 (0.023) Batch 0.080 (0.063) Remain 00:00:30 loss: 0.1977 data: 0.0120 Lr: 0.79221
2024-08-21 22:13:28.366 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][52/77] Data 0.028 (0.026) Batch 0.074 (0.063) Remain 00:00:30 loss: 0.1861 data: -0.0249 Lr: 0.79058
2024-08-21 22:13:28.366 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][52/77] Data 0.035 (0.023) Batch 0.074 (0.063) Remain 00:00:30 loss: 0.1861 data: 0.0017 Lr: 0.79058
2024-08-21 22:13:28.436 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][53/77] Data 0.027 (0.026) Batch 0.070 (0.063) Remain 00:00:30 loss: 0.2033 data: -0.0014 Lr: 0.78896
2024-08-21 22:13:28.436 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][53/77] Data 0.031 (0.023) Batch 0.070 (0.063) Remain 00:00:30 loss: 0.2033 data: 0.0034 Lr: 0.78896
2024-08-21 22:13:28.502 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][54/77] Data 0.027 (0.026) Batch 0.066 (0.063) Remain 00:00:30 loss: 0.2667 data: 0.0218 Lr: 0.78734
2024-08-21 22:13:28.502 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][54/77] Data 0.028 (0.024) Batch 0.066 (0.063) Remain 00:00:30 loss: 0.2667 data: 0.0069 Lr: 0.78734
2024-08-21 22:13:28.568 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][55/77] Data 0.027 (0.026) Batch 0.066 (0.063) Remain 00:00:30 loss: 0.1959 data: 0.0029 Lr: 0.78571
2024-08-21 22:13:28.568 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][55/77] Data 0.028 (0.024) Batch 0.066 (0.063) Remain 00:00:30 loss: 0.1959 data: 0.0152 Lr: 0.78571
2024-08-21 22:13:28.634 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][56/77] Data 0.027 (0.026) Batch 0.066 (0.063) Remain 00:00:30 loss: 0.2772 data: 0.0070 Lr: 0.78409
2024-08-21 22:13:28.634 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][56/77] Data 0.028 (0.024) Batch 0.066 (0.063) Remain 00:00:30 loss: 0.2772 data: -0.0028 Lr: 0.78409
2024-08-21 22:13:28.708 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][57/77] Data 0.022 (0.026) Batch 0.074 (0.063) Remain 00:00:30 loss: 0.2550 data: 0.0044 Lr: 0.78247
2024-08-21 22:13:28.708 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][57/77] Data 0.028 (0.024) Batch 0.074 (0.063) Remain 00:00:30 loss: 0.2550 data: -0.0048 Lr: 0.78247
2024-08-21 22:13:28.774 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][58/77] Data 0.021 (0.026) Batch 0.067 (0.063) Remain 00:00:30 loss: 0.1554 data: 0.0036 Lr: 0.78084
2024-08-21 22:13:28.774 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][58/77] Data 0.028 (0.024) Batch 0.066 (0.063) Remain 00:00:30 loss: 0.1554 data: 0.0017 Lr: 0.78084
2024-08-21 22:13:28.840 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][59/77] Data 0.022 (0.026) Batch 0.066 (0.063) Remain 00:00:30 loss: 0.2714 data: -0.0058 Lr: 0.77922
2024-08-21 22:13:28.840 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][59/77] Data 0.028 (0.024) Batch 0.066 (0.063) Remain 00:00:30 loss: 0.2714 data: 0.0104 Lr: 0.77922
2024-08-21 22:13:28.906 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][60/77] Data 0.021 (0.026) Batch 0.066 (0.064) Remain 00:00:30 loss: 0.1998 data: 0.0062 Lr: 0.77760
2024-08-21 22:13:28.907 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][60/77] Data 0.028 (0.024) Batch 0.066 (0.064) Remain 00:00:30 loss: 0.1998 data: -0.0078 Lr: 0.77760
2024-08-21 22:13:28.973 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][61/77] Data 0.022 (0.026) Batch 0.066 (0.064) Remain 00:00:30 loss: 0.2022 data: -0.0027 Lr: 0.77597
2024-08-21 22:13:28.973 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][61/77] Data 0.028 (0.024) Batch 0.066 (0.064) Remain 00:00:30 loss: 0.2022 data: 0.0060 Lr: 0.77597
2024-08-21 22:13:29.039 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][62/77] Data 0.022 (0.026) Batch 0.066 (0.064) Remain 00:00:30 loss: 0.1744 data: 0.0132 Lr: 0.77435
2024-08-21 22:13:29.039 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][62/77] Data 0.028 (0.024) Batch 0.067 (0.064) Remain 00:00:30 loss: 0.1744 data: -0.0130 Lr: 0.77435
2024-08-21 22:13:29.116 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][63/77] Data 0.031 (0.026) Batch 0.077 (0.064) Remain 00:00:30 loss: 0.2041 data: 0.0036 Lr: 0.77273
2024-08-21 22:13:29.116 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][63/77] Data 0.028 (0.024) Batch 0.077 (0.064) Remain 00:00:30 loss: 0.2041 data: 0.0162 Lr: 0.77273
2024-08-21 22:13:29.183 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][64/77] Data 0.022 (0.026) Batch 0.067 (0.064) Remain 00:00:30 loss: 0.1735 data: 0.0171 Lr: 0.77110
2024-08-21 22:13:29.183 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][64/77] Data 0.028 (0.024) Batch 0.067 (0.064) Remain 00:00:30 loss: 0.1735 data: 0.0150 Lr: 0.77110
2024-08-21 22:13:29.249 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][65/77] Data 0.022 (0.026) Batch 0.066 (0.064) Remain 00:00:30 loss: 0.2626 data: -0.0064 Lr: 0.76948
2024-08-21 22:13:29.249 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][65/77] Data 0.028 (0.024) Batch 0.066 (0.064) Remain 00:00:30 loss: 0.2626 data: 0.0033 Lr: 0.76948
2024-08-21 22:13:29.315 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][66/77] Data 0.022 (0.026) Batch 0.066 (0.064) Remain 00:00:30 loss: 0.2081 data: 0.0051 Lr: 0.76786
2024-08-21 22:13:29.316 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][66/77] Data 0.028 (0.024) Batch 0.066 (0.064) Remain 00:00:30 loss: 0.2081 data: 0.0126 Lr: 0.76786
2024-08-21 22:13:29.382 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][67/77] Data 0.022 (0.025) Batch 0.067 (0.064) Remain 00:00:30 loss: 0.1938 data: -0.0006 Lr: 0.76623
2024-08-21 22:13:29.382 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][67/77] Data 0.028 (0.024) Batch 0.067 (0.064) Remain 00:00:30 loss: 0.1938 data: 0.0009 Lr: 0.76623
2024-08-21 22:13:29.448 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][68/77] Data 0.022 (0.025) Batch 0.066 (0.064) Remain 00:00:30 loss: 0.2109 data: -0.0102 Lr: 0.76461
2024-08-21 22:13:29.448 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][68/77] Data 0.028 (0.024) Batch 0.066 (0.064) Remain 00:00:30 loss: 0.2109 data: -0.0143 Lr: 0.76461
2024-08-21 22:13:29.513 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][69/77] Data 0.022 (0.025) Batch 0.065 (0.064) Remain 00:00:30 loss: 0.2081 data: -0.0094 Lr: 0.76299
2024-08-21 22:13:29.513 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][69/77] Data 0.028 (0.024) Batch 0.065 (0.064) Remain 00:00:30 loss: 0.2081 data: 0.0056 Lr: 0.76299
2024-08-21 22:13:29.576 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][70/77] Data 0.022 (0.025) Batch 0.063 (0.064) Remain 00:00:30 loss: 0.1933 data: 0.0183 Lr: 0.76136
2024-08-21 22:13:29.576 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][70/77] Data 0.027 (0.024) Batch 0.063 (0.064) Remain 00:00:30 loss: 0.1933 data: -0.0179 Lr: 0.76136
2024-08-21 22:13:29.654 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][71/77] Data 0.023 (0.025) Batch 0.077 (0.064) Remain 00:00:30 loss: 0.1376 data: 0.0079 Lr: 0.75974
2024-08-21 22:13:29.654 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][71/77] Data 0.027 (0.025) Batch 0.078 (0.064) Remain 00:00:30 loss: 0.1376 data: 0.0003 Lr: 0.75974
2024-08-21 22:13:29.740 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][72/77] Data 0.022 (0.025) Batch 0.086 (0.065) Remain 00:00:30 loss: 0.1204 data: -0.0032 Lr: 0.75812
2024-08-21 22:13:29.740 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][72/77] Data 0.037 (0.025) Batch 0.086 (0.065) Remain 00:00:30 loss: 0.1204 data: 0.0072 Lr: 0.75812
2024-08-21 22:13:29.804 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][73/77] Data 0.021 (0.025) Batch 0.065 (0.065) Remain 00:00:30 loss: 0.1542 data: -0.0001 Lr: 0.75649
2024-08-21 22:13:29.804 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_150
2024-08-21 22:13:29.804 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][73/77] Data 0.027 (0.025) Batch 0.065 (0.065) Remain 00:00:30 loss: 0.1542 data: -0.0136 Lr: 0.75649
2024-08-21 22:13:29.805 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_150
2024-08-21 22:13:29.824 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -393.9523010253906
2024-08-21 22:13:29.824 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -188.40049743652344
2024-08-21 22:13:29.824 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -393.9523010253906
2024-08-21 22:13:29.824 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -205.55181884765625
2024-08-21 22:13:29.890 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][74/77] Data 0.041 (0.025) Batch 0.086 (0.065) Remain 00:00:30 loss: 0.3094 data: 0.0015 Lr: 0.75487
2024-08-21 22:13:29.890 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][74/77] Data 0.047 (0.025) Batch 0.086 (0.065) Remain 00:00:30 loss: 0.3094 data: -0.0041 Lr: 0.75487
2024-08-21 22:13:29.956 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][75/77] Data 0.023 (0.025) Batch 0.066 (0.065) Remain 00:00:30 loss: 0.2534 data: 0.0104 Lr: 0.75325
2024-08-21 22:13:29.956 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][75/77] Data 0.027 (0.025) Batch 0.066 (0.065) Remain 00:00:30 loss: 0.2534 data: -0.0017 Lr: 0.75325
2024-08-21 22:13:30.020 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][76/77] Data 0.023 (0.025) Batch 0.065 (0.065) Remain 00:00:30 loss: 0.1001 data: -0.0117 Lr: 0.75162
2024-08-21 22:13:30.021 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][76/77] Data 0.027 (0.025) Batch 0.065 (0.065) Remain 00:00:30 loss: 0.1001 data: 0.0095 Lr: 0.75162
2024-08-21 22:13:30.062 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][77/77] Data 0.025 (0.025) Batch 0.042 (0.065) Remain 00:00:29 loss: 0.2523 data: -0.0051 Lr: 0.75000
2024-08-21 22:13:30.063 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 22:13:30.063 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][77/77] Data 0.030 (0.025) Batch 0.042 (0.065) Remain 00:00:29 loss: 0.2523 data: -0.0003 Lr: 0.75000
2024-08-21 22:13:30.063 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 22:13:33.775 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0563, Accuracy: 0.9821
2024-08-21 22:13:33.775 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0563, Accuracy: 0.9821
2024-08-21 22:13:33.776 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 22:13:33.776 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 22:13:33.776 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 22:13:33.776 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 22:13:33.872 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][1/77] Data 0.046 (0.046) Batch 0.096 (0.096) Remain 00:00:44 loss: 0.2107 data: -0.0045 Lr: 0.74838
2024-08-21 22:13:33.872 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][1/77] Data 0.056 (0.056) Batch 0.096 (0.096) Remain 00:00:44 loss: 0.2107 data: -0.0011 Lr: 0.74838
2024-08-21 22:13:33.940 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][2/77] Data 0.023 (0.023) Batch 0.067 (0.067) Remain 00:00:31 loss: 0.2823 data: 0.0096 Lr: 0.74675
2024-08-21 22:13:33.940 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][2/77] Data 0.028 (0.028) Batch 0.067 (0.067) Remain 00:00:31 loss: 0.2823 data: 0.0084 Lr: 0.74675
2024-08-21 22:13:34.007 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][3/77] Data 0.024 (0.023) Batch 0.068 (0.067) Remain 00:00:31 loss: 0.2101 data: -0.0070 Lr: 0.74513
2024-08-21 22:13:34.007 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][3/77] Data 0.028 (0.028) Batch 0.068 (0.067) Remain 00:00:31 loss: 0.2101 data: 0.0068 Lr: 0.74513
2024-08-21 22:13:34.080 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][4/77] Data 0.022 (0.023) Batch 0.073 (0.069) Remain 00:00:31 loss: 0.2044 data: 0.0159 Lr: 0.74351
2024-08-21 22:13:34.080 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][4/77] Data 0.028 (0.028) Batch 0.073 (0.069) Remain 00:00:31 loss: 0.2044 data: 0.0033 Lr: 0.74351
2024-08-21 22:13:34.148 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][5/77] Data 0.028 (0.024) Batch 0.068 (0.069) Remain 00:00:31 loss: 0.1806 data: -0.0087 Lr: 0.74188
2024-08-21 22:13:34.148 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][5/77] Data 0.027 (0.028) Batch 0.068 (0.069) Remain 00:00:31 loss: 0.1806 data: 0.0059 Lr: 0.74188
2024-08-21 22:13:34.216 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][6/77] Data 0.028 (0.025) Batch 0.068 (0.069) Remain 00:00:31 loss: 0.3011 data: 0.0066 Lr: 0.74026
2024-08-21 22:13:34.216 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][6/77] Data 0.027 (0.028) Batch 0.068 (0.069) Remain 00:00:31 loss: 0.3011 data: 0.0003 Lr: 0.74026
2024-08-21 22:13:34.284 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][7/77] Data 0.028 (0.025) Batch 0.068 (0.069) Remain 00:00:31 loss: 0.2604 data: 0.0009 Lr: 0.73864
2024-08-21 22:13:34.284 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][7/77] Data 0.027 (0.028) Batch 0.068 (0.069) Remain 00:00:31 loss: 0.2604 data: -0.0049 Lr: 0.73864
2024-08-21 22:13:34.358 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][8/77] Data 0.028 (0.026) Batch 0.074 (0.069) Remain 00:00:31 loss: 0.2427 data: 0.0145 Lr: 0.73701
2024-08-21 22:13:34.358 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][8/77] Data 0.027 (0.028) Batch 0.074 (0.069) Remain 00:00:31 loss: 0.2427 data: 0.0171 Lr: 0.73701
2024-08-21 22:13:34.429 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][9/77] Data 0.028 (0.026) Batch 0.070 (0.070) Remain 00:00:31 loss: 0.1861 data: -0.0157 Lr: 0.73539
2024-08-21 22:13:34.429 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][9/77] Data 0.028 (0.028) Batch 0.070 (0.070) Remain 00:00:31 loss: 0.1861 data: 0.0071 Lr: 0.73539
2024-08-21 22:13:34.499 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][10/77] Data 0.029 (0.026) Batch 0.070 (0.070) Remain 00:00:31 loss: 0.2109 data: -0.0009 Lr: 0.73377
2024-08-21 22:13:34.499 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][10/77] Data 0.028 (0.028) Batch 0.070 (0.070) Remain 00:00:31 loss: 0.2109 data: -0.0004 Lr: 0.73377
2024-08-21 22:13:34.569 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][11/77] Data 0.029 (0.027) Batch 0.070 (0.070) Remain 00:00:31 loss: 0.2342 data: 0.0062 Lr: 0.73214
2024-08-21 22:13:34.569 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][11/77] Data 0.028 (0.028) Batch 0.070 (0.070) Remain 00:00:31 loss: 0.2342 data: -0.0147 Lr: 0.73214
2024-08-21 22:13:34.637 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][12/77] Data 0.028 (0.027) Batch 0.068 (0.070) Remain 00:00:31 loss: 0.1784 data: 0.0201 Lr: 0.73052
2024-08-21 22:13:34.637 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][12/77] Data 0.028 (0.028) Batch 0.068 (0.070) Remain 00:00:31 loss: 0.1784 data: 0.0268 Lr: 0.73052
2024-08-21 22:13:34.705 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][13/77] Data 0.028 (0.027) Batch 0.067 (0.069) Remain 00:00:31 loss: 0.1758 data: 0.0123 Lr: 0.72890
2024-08-21 22:13:34.705 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][13/77] Data 0.028 (0.028) Batch 0.067 (0.069) Remain 00:00:31 loss: 0.1758 data: -0.0012 Lr: 0.72890
2024-08-21 22:13:34.774 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][14/77] Data 0.028 (0.027) Batch 0.070 (0.069) Remain 00:00:31 loss: 0.2359 data: -0.0102 Lr: 0.72727
2024-08-21 22:13:34.775 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][14/77] Data 0.028 (0.028) Batch 0.070 (0.069) Remain 00:00:31 loss: 0.2359 data: -0.0023 Lr: 0.72727
2024-08-21 22:13:34.845 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][15/77] Data 0.028 (0.027) Batch 0.070 (0.069) Remain 00:00:31 loss: 0.1784 data: -0.0138 Lr: 0.72565
2024-08-21 22:13:34.845 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][15/77] Data 0.028 (0.028) Batch 0.070 (0.069) Remain 00:00:31 loss: 0.1784 data: -0.0008 Lr: 0.72565
2024-08-21 22:13:34.916 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][16/77] Data 0.030 (0.027) Batch 0.072 (0.070) Remain 00:00:31 loss: 0.2232 data: 0.0008 Lr: 0.72403
2024-08-21 22:13:34.917 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][16/77] Data 0.029 (0.028) Batch 0.072 (0.070) Remain 00:00:31 loss: 0.2232 data: -0.0084 Lr: 0.72403
2024-08-21 22:13:34.987 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][17/77] Data 0.028 (0.027) Batch 0.070 (0.070) Remain 00:00:31 loss: 0.1644 data: 0.0065 Lr: 0.72240
2024-08-21 22:13:34.987 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][17/77] Data 0.028 (0.028) Batch 0.070 (0.070) Remain 00:00:31 loss: 0.1644 data: 0.0021 Lr: 0.72240
2024-08-21 22:13:35.056 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][18/77] Data 0.028 (0.027) Batch 0.070 (0.070) Remain 00:00:30 loss: 0.2250 data: 0.0004 Lr: 0.72078
2024-08-21 22:13:35.056 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][18/77] Data 0.028 (0.028) Batch 0.070 (0.070) Remain 00:00:30 loss: 0.2250 data: -0.0179 Lr: 0.72078
2024-08-21 22:13:35.124 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][19/77] Data 0.028 (0.027) Batch 0.068 (0.070) Remain 00:00:30 loss: 0.2599 data: -0.0039 Lr: 0.71916
2024-08-21 22:13:35.124 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][19/77] Data 0.028 (0.028) Batch 0.068 (0.070) Remain 00:00:30 loss: 0.2599 data: 0.0078 Lr: 0.71916
2024-08-21 22:13:35.196 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][20/77] Data 0.029 (0.027) Batch 0.071 (0.070) Remain 00:00:30 loss: 0.1632 data: -0.0017 Lr: 0.71753
2024-08-21 22:13:35.196 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][20/77] Data 0.028 (0.028) Batch 0.072 (0.070) Remain 00:00:30 loss: 0.1632 data: 0.0066 Lr: 0.71753
2024-08-21 22:13:35.271 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][21/77] Data 0.030 (0.027) Batch 0.075 (0.070) Remain 00:00:30 loss: 0.1879 data: -0.0026 Lr: 0.71591
2024-08-21 22:13:35.271 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][21/77] Data 0.029 (0.028) Batch 0.075 (0.070) Remain 00:00:30 loss: 0.1879 data: 0.0077 Lr: 0.71591
2024-08-21 22:13:35.348 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][22/77] Data 0.030 (0.028) Batch 0.077 (0.070) Remain 00:00:30 loss: 0.1470 data: -0.0002 Lr: 0.71429
2024-08-21 22:13:35.348 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][22/77] Data 0.030 (0.028) Batch 0.077 (0.070) Remain 00:00:30 loss: 0.1470 data: -0.0077 Lr: 0.71429
2024-08-21 22:13:35.426 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][23/77] Data 0.031 (0.028) Batch 0.078 (0.071) Remain 00:00:31 loss: 0.1915 data: -0.0122 Lr: 0.71266
2024-08-21 22:13:35.426 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][23/77] Data 0.031 (0.028) Batch 0.078 (0.071) Remain 00:00:31 loss: 0.1915 data: 0.0054 Lr: 0.71266
2024-08-21 22:13:35.497 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][24/77] Data 0.029 (0.028) Batch 0.070 (0.071) Remain 00:00:31 loss: 0.2027 data: 0.0237 Lr: 0.71104
2024-08-21 22:13:35.497 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][24/77] Data 0.028 (0.028) Batch 0.070 (0.071) Remain 00:00:31 loss: 0.2027 data: 0.0091 Lr: 0.71104
2024-08-21 22:13:35.567 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][25/77] Data 0.029 (0.028) Batch 0.070 (0.071) Remain 00:00:30 loss: 0.2321 data: -0.0124 Lr: 0.70942
2024-08-21 22:13:35.567 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][25/77] Data 0.028 (0.028) Batch 0.070 (0.071) Remain 00:00:30 loss: 0.2321 data: 0.0053 Lr: 0.70942
2024-08-21 22:13:35.635 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][26/77] Data 0.029 (0.028) Batch 0.068 (0.071) Remain 00:00:30 loss: 0.1683 data: 0.0091 Lr: 0.70779
2024-08-21 22:13:35.635 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][26/77] Data 0.028 (0.028) Batch 0.068 (0.071) Remain 00:00:30 loss: 0.1683 data: 0.0113 Lr: 0.70779
2024-08-21 22:13:35.704 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][27/77] Data 0.028 (0.028) Batch 0.068 (0.070) Remain 00:00:30 loss: 0.2188 data: -0.0085 Lr: 0.70617
2024-08-21 22:13:35.704 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][27/77] Data 0.028 (0.028) Batch 0.068 (0.070) Remain 00:00:30 loss: 0.2188 data: -0.0077 Lr: 0.70617
2024-08-21 22:13:35.777 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][28/77] Data 0.029 (0.028) Batch 0.073 (0.071) Remain 00:00:30 loss: 0.1970 data: -0.0011 Lr: 0.70455
2024-08-21 22:13:35.777 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][28/77] Data 0.028 (0.028) Batch 0.073 (0.071) Remain 00:00:30 loss: 0.1970 data: 0.0013 Lr: 0.70455
2024-08-21 22:13:35.852 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][29/77] Data 0.029 (0.028) Batch 0.075 (0.071) Remain 00:00:30 loss: 0.1990 data: 0.0135 Lr: 0.70292
2024-08-21 22:13:35.852 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][29/77] Data 0.029 (0.028) Batch 0.075 (0.071) Remain 00:00:30 loss: 0.1990 data: -0.0093 Lr: 0.70292
2024-08-21 22:13:35.927 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][30/77] Data 0.030 (0.028) Batch 0.075 (0.071) Remain 00:00:30 loss: 0.1589 data: 0.0096 Lr: 0.70130
2024-08-21 22:13:35.927 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][30/77] Data 0.029 (0.028) Batch 0.075 (0.071) Remain 00:00:30 loss: 0.1589 data: -0.0269 Lr: 0.70130
2024-08-21 22:13:36.001 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][31/77] Data 0.030 (0.028) Batch 0.074 (0.071) Remain 00:00:30 loss: 0.1930 data: 0.0054 Lr: 0.69968
2024-08-21 22:13:36.002 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][31/77] Data 0.029 (0.028) Batch 0.074 (0.071) Remain 00:00:30 loss: 0.1930 data: 0.0053 Lr: 0.69968
2024-08-21 22:13:36.075 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][32/77] Data 0.029 (0.028) Batch 0.073 (0.071) Remain 00:00:30 loss: 0.1771 data: -0.0337 Lr: 0.69805
2024-08-21 22:13:36.075 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][32/77] Data 0.029 (0.028) Batch 0.074 (0.071) Remain 00:00:30 loss: 0.1771 data: -0.0045 Lr: 0.69805
2024-08-21 22:13:36.148 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][33/77] Data 0.029 (0.028) Batch 0.073 (0.071) Remain 00:00:30 loss: 0.2450 data: -0.0012 Lr: 0.69643
2024-08-21 22:13:36.148 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][33/77] Data 0.029 (0.028) Batch 0.073 (0.071) Remain 00:00:30 loss: 0.2450 data: 0.0139 Lr: 0.69643
2024-08-21 22:13:36.228 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][34/77] Data 0.029 (0.028) Batch 0.080 (0.071) Remain 00:00:30 loss: 0.1901 data: 0.0190 Lr: 0.69481
2024-08-21 22:13:36.228 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][34/77] Data 0.029 (0.028) Batch 0.080 (0.071) Remain 00:00:30 loss: 0.1901 data: 0.0132 Lr: 0.69481
2024-08-21 22:13:36.303 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][35/77] Data 0.029 (0.028) Batch 0.075 (0.071) Remain 00:00:30 loss: 0.1869 data: -0.0044 Lr: 0.69318
2024-08-21 22:13:36.304 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][35/77] Data 0.030 (0.028) Batch 0.075 (0.072) Remain 00:00:30 loss: 0.1869 data: 0.0115 Lr: 0.69318
2024-08-21 22:13:36.377 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][36/77] Data 0.029 (0.028) Batch 0.074 (0.072) Remain 00:00:30 loss: 0.1736 data: -0.0074 Lr: 0.69156
2024-08-21 22:13:36.377 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][36/77] Data 0.029 (0.028) Batch 0.074 (0.072) Remain 00:00:30 loss: 0.1736 data: -0.0134 Lr: 0.69156
2024-08-21 22:13:36.450 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][37/77] Data 0.029 (0.028) Batch 0.073 (0.072) Remain 00:00:30 loss: 0.2019 data: -0.0128 Lr: 0.68994
2024-08-21 22:13:36.450 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][37/77] Data 0.029 (0.028) Batch 0.073 (0.072) Remain 00:00:30 loss: 0.2019 data: -0.0070 Lr: 0.68994
2024-08-21 22:13:36.521 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][38/77] Data 0.029 (0.028) Batch 0.072 (0.072) Remain 00:00:30 loss: 0.2536 data: 0.0128 Lr: 0.68831
2024-08-21 22:13:36.522 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][38/77] Data 0.029 (0.028) Batch 0.072 (0.072) Remain 00:00:30 loss: 0.2536 data: 0.0175 Lr: 0.68831
2024-08-21 22:13:36.593 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][39/77] Data 0.029 (0.028) Batch 0.071 (0.072) Remain 00:00:30 loss: 0.2233 data: 0.0293 Lr: 0.68669
2024-08-21 22:13:36.593 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][39/77] Data 0.029 (0.028) Batch 0.071 (0.072) Remain 00:00:30 loss: 0.2233 data: 0.0159 Lr: 0.68669
2024-08-21 22:13:36.664 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][40/77] Data 0.029 (0.028) Batch 0.071 (0.072) Remain 00:00:30 loss: 0.2450 data: -0.0067 Lr: 0.68506
2024-08-21 22:13:36.664 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][40/77] Data 0.029 (0.028) Batch 0.071 (0.072) Remain 00:00:30 loss: 0.2450 data: -0.0013 Lr: 0.68506
2024-08-21 22:13:36.737 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][41/77] Data 0.029 (0.028) Batch 0.073 (0.072) Remain 00:00:30 loss: 0.1964 data: -0.0067 Lr: 0.68344
2024-08-21 22:13:36.737 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][41/77] Data 0.029 (0.028) Batch 0.073 (0.072) Remain 00:00:30 loss: 0.1964 data: -0.0098 Lr: 0.68344
2024-08-21 22:13:36.812 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][42/77] Data 0.030 (0.029) Batch 0.075 (0.072) Remain 00:00:30 loss: 0.2412 data: -0.0034 Lr: 0.68182
2024-08-21 22:13:36.813 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][42/77] Data 0.029 (0.028) Batch 0.076 (0.072) Remain 00:00:30 loss: 0.2412 data: 0.0099 Lr: 0.68182
2024-08-21 22:13:36.881 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][43/77] Data 0.022 (0.028) Batch 0.068 (0.072) Remain 00:00:30 loss: 0.1330 data: -0.0086 Lr: 0.68019
2024-08-21 22:13:36.881 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][43/77] Data 0.029 (0.029) Batch 0.069 (0.072) Remain 00:00:30 loss: 0.1330 data: -0.0002 Lr: 0.68019
2024-08-21 22:13:36.948 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][44/77] Data 0.023 (0.028) Batch 0.067 (0.072) Remain 00:00:29 loss: 0.2352 data: 0.0017 Lr: 0.67857
2024-08-21 22:13:36.948 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][44/77] Data 0.028 (0.029) Batch 0.067 (0.072) Remain 00:00:29 loss: 0.2352 data: 0.0015 Lr: 0.67857
2024-08-21 22:13:37.019 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][45/77] Data 0.025 (0.028) Batch 0.072 (0.072) Remain 00:00:29 loss: 0.1543 data: -0.0113 Lr: 0.67695
2024-08-21 22:13:37.020 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][45/77] Data 0.032 (0.029) Batch 0.072 (0.072) Remain 00:00:29 loss: 0.1543 data: -0.0039 Lr: 0.67695
2024-08-21 22:13:37.084 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][46/77] Data 0.023 (0.028) Batch 0.065 (0.071) Remain 00:00:29 loss: 0.2102 data: -0.0087 Lr: 0.67532
2024-08-21 22:13:37.084 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_200
2024-08-21 22:13:37.084 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][46/77] Data 0.027 (0.029) Batch 0.065 (0.071) Remain 00:00:29 loss: 0.2102 data: -0.0039 Lr: 0.67532
2024-08-21 22:13:37.085 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_200
2024-08-21 22:13:37.106 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -475.5408630371094
2024-08-21 22:13:37.106 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -222.51181030273438
2024-08-21 22:13:37.106 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -475.5408630371094
2024-08-21 22:13:37.106 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -253.029052734375
2024-08-21 22:13:37.172 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][47/77] Data 0.043 (0.028) Batch 0.088 (0.072) Remain 00:00:29 loss: 0.1650 data: 0.0070 Lr: 0.67370
2024-08-21 22:13:37.172 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][47/77] Data 0.049 (0.029) Batch 0.088 (0.072) Remain 00:00:29 loss: 0.1650 data: 0.0089 Lr: 0.67370
2024-08-21 22:13:37.237 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][48/77] Data 0.023 (0.028) Batch 0.065 (0.072) Remain 00:00:29 loss: 0.1767 data: -0.0072 Lr: 0.67208
2024-08-21 22:13:37.237 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][48/77] Data 0.027 (0.029) Batch 0.065 (0.072) Remain 00:00:29 loss: 0.1767 data: -0.0044 Lr: 0.67208
2024-08-21 22:13:37.303 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][49/77] Data 0.023 (0.028) Batch 0.066 (0.071) Remain 00:00:29 loss: 0.1702 data: 0.0042 Lr: 0.67045
2024-08-21 22:13:37.303 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][49/77] Data 0.027 (0.029) Batch 0.066 (0.071) Remain 00:00:29 loss: 0.1702 data: 0.0119 Lr: 0.67045
2024-08-21 22:13:37.368 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][50/77] Data 0.022 (0.028) Batch 0.065 (0.071) Remain 00:00:29 loss: 0.1572 data: -0.0272 Lr: 0.66883
2024-08-21 22:13:37.368 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][50/77] Data 0.027 (0.029) Batch 0.065 (0.071) Remain 00:00:29 loss: 0.1572 data: -0.0082 Lr: 0.66883
2024-08-21 22:13:37.432 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][51/77] Data 0.024 (0.028) Batch 0.064 (0.071) Remain 00:00:29 loss: 0.1499 data: -0.0063 Lr: 0.66721
2024-08-21 22:13:37.432 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][51/77] Data 0.027 (0.029) Batch 0.064 (0.071) Remain 00:00:29 loss: 0.1499 data: 0.0005 Lr: 0.66721
2024-08-21 22:13:37.496 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][52/77] Data 0.023 (0.028) Batch 0.063 (0.071) Remain 00:00:29 loss: 0.1313 data: -0.0056 Lr: 0.66558
2024-08-21 22:13:37.496 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][52/77] Data 0.027 (0.029) Batch 0.063 (0.071) Remain 00:00:29 loss: 0.1313 data: 0.0022 Lr: 0.66558
2024-08-21 22:13:37.561 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][53/77] Data 0.023 (0.028) Batch 0.065 (0.071) Remain 00:00:29 loss: 0.2138 data: 0.0066 Lr: 0.66396
2024-08-21 22:13:37.561 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][53/77] Data 0.027 (0.029) Batch 0.065 (0.071) Remain 00:00:29 loss: 0.2138 data: 0.0006 Lr: 0.66396
2024-08-21 22:13:37.627 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][54/77] Data 0.022 (0.028) Batch 0.066 (0.071) Remain 00:00:28 loss: 0.1629 data: 0.0115 Lr: 0.66234
2024-08-21 22:13:37.627 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][54/77] Data 0.027 (0.029) Batch 0.065 (0.071) Remain 00:00:28 loss: 0.1629 data: 0.0042 Lr: 0.66234
2024-08-21 22:13:37.691 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][55/77] Data 0.022 (0.027) Batch 0.065 (0.071) Remain 00:00:28 loss: 0.1104 data: 0.0112 Lr: 0.66071
2024-08-21 22:13:37.691 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][55/77] Data 0.027 (0.029) Batch 0.065 (0.071) Remain 00:00:28 loss: 0.1104 data: 0.0062 Lr: 0.66071
2024-08-21 22:13:37.757 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][56/77] Data 0.023 (0.027) Batch 0.066 (0.071) Remain 00:00:28 loss: 0.2505 data: 0.0039 Lr: 0.65909
2024-08-21 22:13:37.757 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][56/77] Data 0.027 (0.029) Batch 0.066 (0.071) Remain 00:00:28 loss: 0.2505 data: -0.0098 Lr: 0.65909
2024-08-21 22:13:37.822 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][57/77] Data 0.023 (0.027) Batch 0.065 (0.071) Remain 00:00:28 loss: 0.1393 data: -0.0106 Lr: 0.65747
2024-08-21 22:13:37.822 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][57/77] Data 0.027 (0.029) Batch 0.065 (0.071) Remain 00:00:28 loss: 0.1393 data: -0.0118 Lr: 0.65747
2024-08-21 22:13:37.887 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][58/77] Data 0.023 (0.027) Batch 0.065 (0.070) Remain 00:00:28 loss: 0.1816 data: -0.0061 Lr: 0.65584
2024-08-21 22:13:37.887 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][58/77] Data 0.027 (0.029) Batch 0.065 (0.070) Remain 00:00:28 loss: 0.1816 data: -0.0058 Lr: 0.65584
2024-08-21 22:13:37.953 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][59/77] Data 0.022 (0.027) Batch 0.066 (0.070) Remain 00:00:28 loss: 0.1829 data: 0.0112 Lr: 0.65422
2024-08-21 22:13:37.953 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][59/77] Data 0.028 (0.029) Batch 0.066 (0.070) Remain 00:00:28 loss: 0.1829 data: 0.0194 Lr: 0.65422
2024-08-21 22:13:38.019 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][60/77] Data 0.023 (0.027) Batch 0.066 (0.070) Remain 00:00:28 loss: 0.1219 data: -0.0083 Lr: 0.65260
2024-08-21 22:13:38.019 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][60/77] Data 0.027 (0.029) Batch 0.066 (0.070) Remain 00:00:28 loss: 0.1219 data: -0.0125 Lr: 0.65260
2024-08-21 22:13:38.087 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][61/77] Data 0.022 (0.027) Batch 0.068 (0.070) Remain 00:00:28 loss: 0.1026 data: 0.0105 Lr: 0.65097
2024-08-21 22:13:38.087 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][61/77] Data 0.027 (0.029) Batch 0.068 (0.070) Remain 00:00:28 loss: 0.1026 data: -0.0034 Lr: 0.65097
2024-08-21 22:13:38.154 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][62/77] Data 0.028 (0.027) Batch 0.067 (0.070) Remain 00:00:28 loss: 0.3017 data: 0.0028 Lr: 0.64935
2024-08-21 22:13:38.154 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][62/77] Data 0.027 (0.029) Batch 0.067 (0.070) Remain 00:00:28 loss: 0.3017 data: 0.0012 Lr: 0.64935
2024-08-21 22:13:38.236 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][63/77] Data 0.029 (0.027) Batch 0.082 (0.070) Remain 00:00:28 loss: 0.1439 data: 0.0114 Lr: 0.64773
2024-08-21 22:13:38.236 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][63/77] Data 0.038 (0.029) Batch 0.082 (0.070) Remain 00:00:28 loss: 0.1439 data: -0.0043 Lr: 0.64773
2024-08-21 22:13:38.306 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][64/77] Data 0.030 (0.027) Batch 0.069 (0.070) Remain 00:00:28 loss: 0.1950 data: -0.0030 Lr: 0.64610
2024-08-21 22:13:38.306 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][64/77] Data 0.029 (0.029) Batch 0.069 (0.070) Remain 00:00:28 loss: 0.1950 data: 0.0092 Lr: 0.64610
2024-08-21 22:13:38.371 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][65/77] Data 0.028 (0.027) Batch 0.066 (0.070) Remain 00:00:27 loss: 0.1481 data: 0.0200 Lr: 0.64448
2024-08-21 22:13:38.372 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][65/77] Data 0.027 (0.029) Batch 0.066 (0.070) Remain 00:00:27 loss: 0.1481 data: 0.0101 Lr: 0.64448
2024-08-21 22:13:38.449 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][66/77] Data 0.038 (0.027) Batch 0.078 (0.070) Remain 00:00:27 loss: 0.1708 data: -0.0093 Lr: 0.64286
2024-08-21 22:13:38.449 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][66/77] Data 0.027 (0.029) Batch 0.078 (0.070) Remain 00:00:27 loss: 0.1708 data: 0.0158 Lr: 0.64286
2024-08-21 22:13:38.516 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][67/77] Data 0.028 (0.027) Batch 0.067 (0.070) Remain 00:00:27 loss: 0.1717 data: -0.0030 Lr: 0.64123
2024-08-21 22:13:38.516 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][67/77] Data 0.027 (0.029) Batch 0.067 (0.070) Remain 00:00:27 loss: 0.1717 data: -0.0095 Lr: 0.64123
2024-08-21 22:13:38.587 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][68/77] Data 0.033 (0.027) Batch 0.071 (0.070) Remain 00:00:27 loss: 0.1176 data: -0.0148 Lr: 0.63961
2024-08-21 22:13:38.587 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][68/77] Data 0.027 (0.029) Batch 0.071 (0.070) Remain 00:00:27 loss: 0.1176 data: -0.0122 Lr: 0.63961
2024-08-21 22:13:38.653 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][69/77] Data 0.028 (0.027) Batch 0.066 (0.070) Remain 00:00:27 loss: 0.1610 data: -0.0094 Lr: 0.63799
2024-08-21 22:13:38.654 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][69/77] Data 0.027 (0.029) Batch 0.067 (0.070) Remain 00:00:27 loss: 0.1610 data: -0.0098 Lr: 0.63799
2024-08-21 22:13:38.723 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][70/77] Data 0.028 (0.027) Batch 0.070 (0.070) Remain 00:00:27 loss: 0.1315 data: -0.0026 Lr: 0.63636
2024-08-21 22:13:38.723 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][70/77] Data 0.032 (0.029) Batch 0.070 (0.070) Remain 00:00:27 loss: 0.1315 data: -0.0017 Lr: 0.63636
2024-08-21 22:13:38.790 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][71/77] Data 0.027 (0.027) Batch 0.067 (0.070) Remain 00:00:27 loss: 0.1535 data: 0.0111 Lr: 0.63474
2024-08-21 22:13:38.790 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][71/77] Data 0.028 (0.029) Batch 0.067 (0.070) Remain 00:00:27 loss: 0.1535 data: -0.0179 Lr: 0.63474
2024-08-21 22:13:38.866 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][72/77] Data 0.028 (0.027) Batch 0.076 (0.070) Remain 00:00:27 loss: 0.1682 data: 0.0086 Lr: 0.63312
2024-08-21 22:13:38.866 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][72/77] Data 0.035 (0.029) Batch 0.076 (0.070) Remain 00:00:27 loss: 0.1682 data: -0.0067 Lr: 0.63312
2024-08-21 22:13:38.938 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][73/77] Data 0.029 (0.027) Batch 0.071 (0.070) Remain 00:00:27 loss: 0.1353 data: 0.0017 Lr: 0.63149
2024-08-21 22:13:38.938 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][73/77] Data 0.028 (0.029) Batch 0.072 (0.070) Remain 00:00:27 loss: 0.1353 data: -0.0150 Lr: 0.63149
2024-08-21 22:13:39.009 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][74/77] Data 0.029 (0.027) Batch 0.072 (0.070) Remain 00:00:27 loss: 0.1715 data: -0.0004 Lr: 0.62987
2024-08-21 22:13:39.010 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][74/77] Data 0.029 (0.029) Batch 0.072 (0.070) Remain 00:00:27 loss: 0.1715 data: 0.0011 Lr: 0.62987
2024-08-21 22:13:39.082 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][75/77] Data 0.030 (0.027) Batch 0.072 (0.070) Remain 00:00:27 loss: 0.1643 data: -0.0026 Lr: 0.62825
2024-08-21 22:13:39.082 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][75/77] Data 0.028 (0.029) Batch 0.073 (0.070) Remain 00:00:27 loss: 0.1643 data: 0.0077 Lr: 0.62825
2024-08-21 22:13:39.157 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][76/77] Data 0.028 (0.029) Batch 0.075 (0.070) Remain 00:00:27 loss: 0.1376 data: 0.0026 Lr: 0.62662
2024-08-21 22:13:39.157 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][76/77] Data 0.029 (0.027) Batch 0.075 (0.070) Remain 00:00:27 loss: 0.1376 data: -0.0095 Lr: 0.62662
2024-08-21 22:13:39.214 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][77/77] Data 0.039 (0.028) Batch 0.058 (0.070) Remain 00:00:27 loss: 0.2094 data: -0.0129 Lr: 0.62500
2024-08-21 22:13:39.215 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 22:13:39.215 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][77/77] Data 0.032 (0.029) Batch 0.058 (0.070) Remain 00:00:27 loss: 0.2094 data: 0.0030 Lr: 0.62500
2024-08-21 22:13:39.215 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 22:13:43.154 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0486, Accuracy: 0.9843
2024-08-21 22:13:43.154 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0486, Accuracy: 0.9843
2024-08-21 22:13:43.155 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 22:13:43.155 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 22:13:43.155 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 22:13:43.155 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 22:13:43.252 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][1/77] Data 0.056 (0.056) Batch 0.096 (0.096) Remain 00:00:37 loss: 0.1434 data: -0.0047 Lr: 0.62338
2024-08-21 22:13:43.252 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][1/77] Data 0.042 (0.042) Batch 0.097 (0.097) Remain 00:00:37 loss: 0.1434 data: 0.0073 Lr: 0.62338
2024-08-21 22:13:43.316 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][2/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:24 loss: 0.1587 data: -0.0086 Lr: 0.62175
2024-08-21 22:13:43.316 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][2/77] Data 0.022 (0.022) Batch 0.065 (0.065) Remain 00:00:24 loss: 0.1587 data: -0.0142 Lr: 0.62175
2024-08-21 22:13:43.382 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][3/77] Data 0.027 (0.027) Batch 0.066 (0.065) Remain 00:00:25 loss: 0.1448 data: 0.0005 Lr: 0.62013
2024-08-21 22:13:43.382 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][3/77] Data 0.023 (0.022) Batch 0.066 (0.065) Remain 00:00:25 loss: 0.1448 data: -0.0071 Lr: 0.62013
2024-08-21 22:13:43.447 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][4/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:24 loss: 0.2313 data: -0.0244 Lr: 0.61851
2024-08-21 22:13:43.447 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][4/77] Data 0.022 (0.022) Batch 0.065 (0.065) Remain 00:00:24 loss: 0.2313 data: -0.0053 Lr: 0.61851
2024-08-21 22:13:43.512 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][5/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:24 loss: 0.0985 data: 0.0004 Lr: 0.61688
2024-08-21 22:13:43.512 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][5/77] Data 0.021 (0.022) Batch 0.065 (0.065) Remain 00:00:24 loss: 0.0985 data: 0.0133 Lr: 0.61688
2024-08-21 22:13:43.577 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][6/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:24 loss: 0.1920 data: 0.0166 Lr: 0.61526
2024-08-21 22:13:43.577 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][6/77] Data 0.021 (0.021) Batch 0.065 (0.065) Remain 00:00:24 loss: 0.1920 data: 0.0016 Lr: 0.61526
2024-08-21 22:13:43.642 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][7/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:24 loss: 0.1572 data: -0.0050 Lr: 0.61364
2024-08-21 22:13:43.642 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][7/77] Data 0.022 (0.021) Batch 0.065 (0.065) Remain 00:00:24 loss: 0.1572 data: -0.0014 Lr: 0.61364
2024-08-21 22:13:43.705 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][8/77] Data 0.027 (0.027) Batch 0.063 (0.065) Remain 00:00:24 loss: 0.1722 data: -0.0056 Lr: 0.61201
2024-08-21 22:13:43.705 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][8/77] Data 0.021 (0.021) Batch 0.063 (0.065) Remain 00:00:24 loss: 0.1722 data: -0.0114 Lr: 0.61201
2024-08-21 22:13:43.789 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][9/77] Data 0.036 (0.028) Batch 0.084 (0.067) Remain 00:00:25 loss: 0.1131 data: 0.0029 Lr: 0.61039
2024-08-21 22:13:43.789 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][9/77] Data 0.036 (0.023) Batch 0.084 (0.067) Remain 00:00:25 loss: 0.1131 data: 0.0002 Lr: 0.61039
2024-08-21 22:13:43.873 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][10/77] Data 0.037 (0.029) Batch 0.084 (0.069) Remain 00:00:25 loss: 0.1186 data: 0.0060 Lr: 0.60877
2024-08-21 22:13:43.873 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][10/77] Data 0.037 (0.025) Batch 0.084 (0.069) Remain 00:00:25 loss: 0.1186 data: -0.0066 Lr: 0.60877
2024-08-21 22:13:43.959 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][11/77] Data 0.037 (0.026) Batch 0.086 (0.071) Remain 00:00:26 loss: 0.2154 data: 0.0268 Lr: 0.60714
2024-08-21 22:13:43.959 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][11/77] Data 0.037 (0.030) Batch 0.086 (0.071) Remain 00:00:26 loss: 0.2154 data: 0.0084 Lr: 0.60714
2024-08-21 22:13:44.025 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][12/77] Data 0.028 (0.030) Batch 0.066 (0.070) Remain 00:00:26 loss: 0.1222 data: 0.0143 Lr: 0.60552
2024-08-21 22:13:44.025 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][12/77] Data 0.022 (0.026) Batch 0.066 (0.070) Remain 00:00:26 loss: 0.1222 data: -0.0109 Lr: 0.60552
2024-08-21 22:13:44.091 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][13/77] Data 0.027 (0.030) Batch 0.066 (0.070) Remain 00:00:26 loss: 0.1540 data: 0.0112 Lr: 0.60390
2024-08-21 22:13:44.091 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][13/77] Data 0.022 (0.025) Batch 0.067 (0.070) Remain 00:00:26 loss: 0.1540 data: -0.0053 Lr: 0.60390
2024-08-21 22:13:44.157 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][14/77] Data 0.027 (0.030) Batch 0.065 (0.070) Remain 00:00:25 loss: 0.1585 data: 0.0104 Lr: 0.60227
2024-08-21 22:13:44.157 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][14/77] Data 0.021 (0.025) Batch 0.065 (0.070) Remain 00:00:25 loss: 0.1585 data: -0.0045 Lr: 0.60227
2024-08-21 22:13:44.222 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][15/77] Data 0.027 (0.029) Batch 0.065 (0.069) Remain 00:00:25 loss: 0.1526 data: -0.0038 Lr: 0.60065
2024-08-21 22:13:44.222 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][15/77] Data 0.022 (0.025) Batch 0.065 (0.069) Remain 00:00:25 loss: 0.1526 data: -0.0167 Lr: 0.60065
2024-08-21 22:13:44.287 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][16/77] Data 0.027 (0.029) Batch 0.065 (0.069) Remain 00:00:25 loss: 0.1369 data: -0.0055 Lr: 0.59903
2024-08-21 22:13:44.287 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][16/77] Data 0.022 (0.025) Batch 0.065 (0.069) Remain 00:00:25 loss: 0.1369 data: 0.0192 Lr: 0.59903
2024-08-21 22:13:44.351 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][17/77] Data 0.027 (0.029) Batch 0.064 (0.069) Remain 00:00:25 loss: 0.1479 data: -0.0110 Lr: 0.59740
2024-08-21 22:13:44.351 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][17/77] Data 0.022 (0.024) Batch 0.064 (0.069) Remain 00:00:25 loss: 0.1479 data: -0.0025 Lr: 0.59740
2024-08-21 22:13:44.416 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][18/77] Data 0.027 (0.029) Batch 0.065 (0.069) Remain 00:00:25 loss: 0.1236 data: 0.0192 Lr: 0.59578
2024-08-21 22:13:44.416 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][18/77] Data 0.022 (0.024) Batch 0.065 (0.069) Remain 00:00:25 loss: 0.1236 data: 0.0059 Lr: 0.59578
2024-08-21 22:13:44.483 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][19/77] Data 0.027 (0.029) Batch 0.066 (0.068) Remain 00:00:25 loss: 0.1158 data: 0.0032 Lr: 0.59416
2024-08-21 22:13:44.483 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_250
2024-08-21 22:13:44.483 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][19/77] Data 0.022 (0.024) Batch 0.066 (0.068) Remain 00:00:25 loss: 0.1158 data: 0.0162 Lr: 0.59416
2024-08-21 22:13:44.483 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_250
2024-08-21 22:13:44.501 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -496.0804443359375
2024-08-21 22:13:44.502 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -247.06309509277344
2024-08-21 22:13:44.502 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -496.0804443359375
2024-08-21 22:13:44.502 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -249.0173797607422
2024-08-21 22:13:44.568 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][20/77] Data 0.047 (0.030) Batch 0.085 (0.069) Remain 00:00:25 loss: 0.1299 data: 0.0024 Lr: 0.59253
2024-08-21 22:13:44.568 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][20/77] Data 0.040 (0.025) Batch 0.085 (0.069) Remain 00:00:25 loss: 0.1299 data: 0.0002 Lr: 0.59253
2024-08-21 22:13:44.636 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][21/77] Data 0.027 (0.030) Batch 0.068 (0.069) Remain 00:00:25 loss: 0.1526 data: -0.0058 Lr: 0.59091
2024-08-21 22:13:44.636 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][21/77] Data 0.023 (0.025) Batch 0.068 (0.069) Remain 00:00:25 loss: 0.1526 data: -0.0005 Lr: 0.59091
2024-08-21 22:13:44.701 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][22/77] Data 0.027 (0.030) Batch 0.065 (0.069) Remain 00:00:25 loss: 0.1205 data: 0.0166 Lr: 0.58929
2024-08-21 22:13:44.701 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][22/77] Data 0.023 (0.025) Batch 0.065 (0.069) Remain 00:00:25 loss: 0.1205 data: -0.0062 Lr: 0.58929
2024-08-21 22:13:44.766 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][23/77] Data 0.027 (0.029) Batch 0.065 (0.069) Remain 00:00:24 loss: 0.1125 data: 0.0021 Lr: 0.58766
2024-08-21 22:13:44.766 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][23/77] Data 0.022 (0.025) Batch 0.065 (0.069) Remain 00:00:24 loss: 0.1125 data: -0.0117 Lr: 0.58766
2024-08-21 22:13:44.832 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][24/77] Data 0.027 (0.029) Batch 0.066 (0.069) Remain 00:00:24 loss: 0.0835 data: -0.0047 Lr: 0.58604
2024-08-21 22:13:44.832 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][24/77] Data 0.022 (0.025) Batch 0.066 (0.069) Remain 00:00:24 loss: 0.0835 data: -0.0134 Lr: 0.58604
2024-08-21 22:13:44.898 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][25/77] Data 0.027 (0.029) Batch 0.066 (0.069) Remain 00:00:24 loss: 0.1311 data: -0.0039 Lr: 0.58442
2024-08-21 22:13:44.898 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][25/77] Data 0.022 (0.024) Batch 0.066 (0.069) Remain 00:00:24 loss: 0.1311 data: -0.0034 Lr: 0.58442
2024-08-21 22:13:44.992 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][26/77] Data 0.027 (0.029) Batch 0.094 (0.070) Remain 00:00:25 loss: 0.1240 data: 0.0041 Lr: 0.58279
2024-08-21 22:13:44.992 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][26/77] Data 0.041 (0.025) Batch 0.094 (0.070) Remain 00:00:25 loss: 0.1240 data: -0.0055 Lr: 0.58279
2024-08-21 22:13:45.087 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][27/77] Data 0.027 (0.029) Batch 0.095 (0.071) Remain 00:00:25 loss: 0.1754 data: -0.0054 Lr: 0.58117
2024-08-21 22:13:45.087 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][27/77] Data 0.042 (0.026) Batch 0.095 (0.071) Remain 00:00:25 loss: 0.1754 data: -0.0092 Lr: 0.58117
2024-08-21 22:13:45.177 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][28/77] Data 0.027 (0.029) Batch 0.089 (0.071) Remain 00:00:25 loss: 0.2012 data: 0.0072 Lr: 0.57955
2024-08-21 22:13:45.177 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][28/77] Data 0.041 (0.026) Batch 0.089 (0.071) Remain 00:00:25 loss: 0.2012 data: -0.0027 Lr: 0.57955
2024-08-21 22:13:45.262 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][29/77] Data 0.027 (0.029) Batch 0.086 (0.072) Remain 00:00:25 loss: 0.1287 data: -0.0074 Lr: 0.57792
2024-08-21 22:13:45.262 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][29/77] Data 0.035 (0.027) Batch 0.086 (0.072) Remain 00:00:25 loss: 0.1287 data: 0.0011 Lr: 0.57792
2024-08-21 22:13:45.334 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][30/77] Data 0.027 (0.029) Batch 0.072 (0.072) Remain 00:00:25 loss: 0.2373 data: 0.0148 Lr: 0.57630
2024-08-21 22:13:45.335 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][30/77] Data 0.032 (0.027) Batch 0.072 (0.072) Remain 00:00:25 loss: 0.2373 data: -0.0176 Lr: 0.57630
2024-08-21 22:13:45.408 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][31/77] Data 0.028 (0.029) Batch 0.074 (0.072) Remain 00:00:25 loss: 0.0588 data: -0.0003 Lr: 0.57468
2024-08-21 22:13:45.408 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][31/77] Data 0.032 (0.027) Batch 0.074 (0.072) Remain 00:00:25 loss: 0.0588 data: 0.0021 Lr: 0.57468
2024-08-21 22:13:45.489 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][32/77] Data 0.029 (0.029) Batch 0.080 (0.072) Remain 00:00:25 loss: 0.0871 data: -0.0023 Lr: 0.57305
2024-08-21 22:13:45.489 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][32/77] Data 0.029 (0.027) Batch 0.080 (0.072) Remain 00:00:25 loss: 0.0871 data: -0.0045 Lr: 0.57305
2024-08-21 22:13:45.554 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][33/77] Data 0.027 (0.029) Batch 0.065 (0.072) Remain 00:00:25 loss: 0.1276 data: -0.0035 Lr: 0.57143
2024-08-21 22:13:45.554 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][33/77] Data 0.027 (0.027) Batch 0.065 (0.072) Remain 00:00:25 loss: 0.1276 data: -0.0185 Lr: 0.57143
2024-08-21 22:13:45.620 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][34/77] Data 0.027 (0.029) Batch 0.067 (0.072) Remain 00:00:25 loss: 0.0901 data: -0.0073 Lr: 0.56981
2024-08-21 22:13:45.621 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][34/77] Data 0.027 (0.027) Batch 0.067 (0.072) Remain 00:00:25 loss: 0.0901 data: 0.0006 Lr: 0.56981
2024-08-21 22:13:45.685 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][35/77] Data 0.028 (0.029) Batch 0.065 (0.072) Remain 00:00:25 loss: 0.1068 data: 0.0062 Lr: 0.56818
2024-08-21 22:13:45.686 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][35/77] Data 0.027 (0.027) Batch 0.065 (0.072) Remain 00:00:25 loss: 0.1068 data: -0.0006 Lr: 0.56818
2024-08-21 22:13:45.751 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][36/77] Data 0.027 (0.029) Batch 0.066 (0.071) Remain 00:00:24 loss: 0.1094 data: -0.0185 Lr: 0.56656
2024-08-21 22:13:45.751 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][36/77] Data 0.027 (0.027) Batch 0.066 (0.071) Remain 00:00:24 loss: 0.1094 data: 0.0126 Lr: 0.56656
2024-08-21 22:13:45.831 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][37/77] Data 0.033 (0.029) Batch 0.079 (0.072) Remain 00:00:25 loss: 0.1363 data: 0.0076 Lr: 0.56494
2024-08-21 22:13:45.831 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][37/77] Data 0.028 (0.027) Batch 0.079 (0.072) Remain 00:00:25 loss: 0.1363 data: 0.0157 Lr: 0.56494
2024-08-21 22:13:45.915 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][38/77] Data 0.036 (0.029) Batch 0.084 (0.072) Remain 00:00:25 loss: 0.1554 data: 0.0169 Lr: 0.56331
2024-08-21 22:13:45.915 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][38/77] Data 0.027 (0.027) Batch 0.084 (0.072) Remain 00:00:25 loss: 0.1554 data: -0.0074 Lr: 0.56331
2024-08-21 22:13:45.982 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][39/77] Data 0.029 (0.029) Batch 0.067 (0.072) Remain 00:00:24 loss: 0.1537 data: 0.0041 Lr: 0.56169
2024-08-21 22:13:45.982 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][39/77] Data 0.027 (0.027) Batch 0.067 (0.072) Remain 00:00:24 loss: 0.1537 data: -0.0038 Lr: 0.56169
2024-08-21 22:13:46.054 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][40/77] Data 0.027 (0.029) Batch 0.072 (0.072) Remain 00:00:24 loss: 0.0960 data: -0.0162 Lr: 0.56006
2024-08-21 22:13:46.054 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][40/77] Data 0.027 (0.027) Batch 0.072 (0.072) Remain 00:00:24 loss: 0.0960 data: 0.0020 Lr: 0.56006
2024-08-21 22:13:46.129 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][41/77] Data 0.029 (0.029) Batch 0.075 (0.072) Remain 00:00:24 loss: 0.2661 data: -0.0118 Lr: 0.55844
2024-08-21 22:13:46.129 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][41/77] Data 0.028 (0.027) Batch 0.075 (0.072) Remain 00:00:24 loss: 0.2661 data: -0.0029 Lr: 0.55844
2024-08-21 22:13:46.209 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][42/77] Data 0.034 (0.029) Batch 0.080 (0.072) Remain 00:00:24 loss: 0.1841 data: 0.0077 Lr: 0.55682
2024-08-21 22:13:46.209 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][42/77] Data 0.027 (0.027) Batch 0.080 (0.072) Remain 00:00:24 loss: 0.1841 data: -0.0087 Lr: 0.55682
2024-08-21 22:13:46.282 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][43/77] Data 0.034 (0.029) Batch 0.072 (0.072) Remain 00:00:24 loss: 0.1256 data: 0.0034 Lr: 0.55519
2024-08-21 22:13:46.282 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][43/77] Data 0.027 (0.027) Batch 0.072 (0.072) Remain 00:00:24 loss: 0.1256 data: -0.0074 Lr: 0.55519
2024-08-21 22:13:46.354 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][44/77] Data 0.027 (0.029) Batch 0.072 (0.072) Remain 00:00:24 loss: 0.1482 data: -0.0086 Lr: 0.55357
2024-08-21 22:13:46.354 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][44/77] Data 0.027 (0.027) Batch 0.072 (0.072) Remain 00:00:24 loss: 0.1482 data: -0.0160 Lr: 0.55357
2024-08-21 22:13:46.425 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][45/77] Data 0.031 (0.029) Batch 0.072 (0.072) Remain 00:00:24 loss: 0.1248 data: 0.0038 Lr: 0.55195
2024-08-21 22:13:46.425 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][45/77] Data 0.027 (0.027) Batch 0.072 (0.072) Remain 00:00:24 loss: 0.1248 data: -0.0082 Lr: 0.55195
2024-08-21 22:13:46.497 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][46/77] Data 0.031 (0.029) Batch 0.071 (0.072) Remain 00:00:24 loss: 0.1889 data: -0.0025 Lr: 0.55032
2024-08-21 22:13:46.497 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][46/77] Data 0.028 (0.027) Batch 0.071 (0.072) Remain 00:00:24 loss: 0.1889 data: -0.0056 Lr: 0.55032
2024-08-21 22:13:46.575 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][47/77] Data 0.031 (0.029) Batch 0.079 (0.072) Remain 00:00:24 loss: 0.1761 data: 0.0173 Lr: 0.54870
2024-08-21 22:13:46.575 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][47/77] Data 0.027 (0.027) Batch 0.079 (0.072) Remain 00:00:24 loss: 0.1761 data: 0.0061 Lr: 0.54870
2024-08-21 22:13:46.645 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][48/77] Data 0.028 (0.029) Batch 0.069 (0.072) Remain 00:00:24 loss: 0.1540 data: -0.0020 Lr: 0.54708
2024-08-21 22:13:46.645 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][48/77] Data 0.028 (0.027) Batch 0.069 (0.072) Remain 00:00:24 loss: 0.1540 data: 0.0014 Lr: 0.54708
2024-08-21 22:13:46.718 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][49/77] Data 0.028 (0.029) Batch 0.073 (0.072) Remain 00:00:24 loss: 0.1151 data: -0.0034 Lr: 0.54545
2024-08-21 22:13:46.718 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][49/77] Data 0.029 (0.027) Batch 0.073 (0.072) Remain 00:00:24 loss: 0.1151 data: -0.0062 Lr: 0.54545
2024-08-21 22:13:46.785 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][50/77] Data 0.027 (0.029) Batch 0.068 (0.072) Remain 00:00:24 loss: 0.1674 data: -0.0050 Lr: 0.54383
2024-08-21 22:13:46.786 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][50/77] Data 0.028 (0.027) Batch 0.068 (0.072) Remain 00:00:24 loss: 0.1674 data: -0.0072 Lr: 0.54383
2024-08-21 22:13:46.853 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][51/77] Data 0.027 (0.029) Batch 0.068 (0.072) Remain 00:00:24 loss: 0.2378 data: -0.0151 Lr: 0.54221
2024-08-21 22:13:46.854 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][51/77] Data 0.027 (0.027) Batch 0.068 (0.072) Remain 00:00:24 loss: 0.2378 data: -0.0041 Lr: 0.54221
2024-08-21 22:13:46.926 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][52/77] Data 0.028 (0.029) Batch 0.072 (0.072) Remain 00:00:24 loss: 0.1498 data: 0.0062 Lr: 0.54058
2024-08-21 22:13:46.926 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][52/77] Data 0.028 (0.027) Batch 0.072 (0.072) Remain 00:00:24 loss: 0.1498 data: -0.0049 Lr: 0.54058
2024-08-21 22:13:46.997 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][53/77] Data 0.029 (0.029) Batch 0.071 (0.072) Remain 00:00:23 loss: 0.1655 data: -0.0032 Lr: 0.53896
2024-08-21 22:13:46.997 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][53/77] Data 0.029 (0.027) Batch 0.071 (0.072) Remain 00:00:23 loss: 0.1655 data: 0.0042 Lr: 0.53896
2024-08-21 22:13:47.064 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][54/77] Data 0.028 (0.029) Batch 0.067 (0.072) Remain 00:00:23 loss: 0.1077 data: 0.0097 Lr: 0.53734
2024-08-21 22:13:47.064 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][54/77] Data 0.027 (0.027) Batch 0.067 (0.072) Remain 00:00:23 loss: 0.1077 data: 0.0007 Lr: 0.53734
2024-08-21 22:13:47.134 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][55/77] Data 0.028 (0.029) Batch 0.071 (0.072) Remain 00:00:23 loss: 0.1042 data: -0.0165 Lr: 0.53571
2024-08-21 22:13:47.134 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][55/77] Data 0.028 (0.027) Batch 0.071 (0.072) Remain 00:00:23 loss: 0.1042 data: -0.0001 Lr: 0.53571
2024-08-21 22:13:47.226 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][56/77] Data 0.027 (0.029) Batch 0.091 (0.072) Remain 00:00:23 loss: 0.0695 data: -0.0279 Lr: 0.53409
2024-08-21 22:13:47.226 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][56/77] Data 0.027 (0.027) Batch 0.091 (0.072) Remain 00:00:23 loss: 0.0695 data: 0.0034 Lr: 0.53409
2024-08-21 22:13:47.339 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][57/77] Data 0.060 (0.030) Batch 0.113 (0.073) Remain 00:00:24 loss: 0.0841 data: -0.0152 Lr: 0.53247
2024-08-21 22:13:47.339 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][57/77] Data 0.073 (0.028) Batch 0.113 (0.073) Remain 00:00:24 loss: 0.0841 data: -0.0163 Lr: 0.53247
2024-08-21 22:13:47.426 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][58/77] Data 0.037 (0.030) Batch 0.087 (0.073) Remain 00:00:24 loss: 0.1493 data: -0.0062 Lr: 0.53084
2024-08-21 22:13:47.426 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][58/77] Data 0.041 (0.028) Batch 0.087 (0.073) Remain 00:00:24 loss: 0.1493 data: 0.0013 Lr: 0.53084
2024-08-21 22:13:47.503 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][59/77] Data 0.032 (0.030) Batch 0.076 (0.073) Remain 00:00:23 loss: 0.2542 data: -0.0070 Lr: 0.52922
2024-08-21 22:13:47.503 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][59/77] Data 0.027 (0.028) Batch 0.077 (0.073) Remain 00:00:23 loss: 0.2542 data: 0.0049 Lr: 0.52922
2024-08-21 22:13:47.570 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][60/77] Data 0.028 (0.030) Batch 0.067 (0.073) Remain 00:00:23 loss: 0.1579 data: -0.0038 Lr: 0.52760
2024-08-21 22:13:47.570 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][60/77] Data 0.028 (0.028) Batch 0.067 (0.073) Remain 00:00:23 loss: 0.1579 data: 0.0050 Lr: 0.52760
2024-08-21 22:13:47.636 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][61/77] Data 0.027 (0.030) Batch 0.066 (0.073) Remain 00:00:23 loss: 0.1805 data: 0.0051 Lr: 0.52597
2024-08-21 22:13:47.636 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][61/77] Data 0.027 (0.028) Batch 0.066 (0.073) Remain 00:00:23 loss: 0.1805 data: 0.0004 Lr: 0.52597
2024-08-21 22:13:47.706 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][62/77] Data 0.028 (0.030) Batch 0.070 (0.073) Remain 00:00:23 loss: 0.1287 data: -0.0009 Lr: 0.52435
2024-08-21 22:13:47.706 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][62/77] Data 0.028 (0.028) Batch 0.070 (0.073) Remain 00:00:23 loss: 0.1287 data: 0.0161 Lr: 0.52435
2024-08-21 22:13:47.772 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][63/77] Data 0.027 (0.030) Batch 0.066 (0.073) Remain 00:00:23 loss: 0.0480 data: -0.0041 Lr: 0.52273
2024-08-21 22:13:47.772 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][63/77] Data 0.027 (0.028) Batch 0.066 (0.073) Remain 00:00:23 loss: 0.0480 data: 0.0031 Lr: 0.52273
2024-08-21 22:13:47.838 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][64/77] Data 0.028 (0.030) Batch 0.066 (0.073) Remain 00:00:23 loss: 0.0777 data: -0.0126 Lr: 0.52110
2024-08-21 22:13:47.838 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][64/77] Data 0.028 (0.028) Batch 0.066 (0.073) Remain 00:00:23 loss: 0.0777 data: 0.0043 Lr: 0.52110
