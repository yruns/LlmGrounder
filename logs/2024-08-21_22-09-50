2024-08-21 22:09:50.413 | INFO     | __main__:configure_model:71 - => creating model ...
2024-08-21 22:09:50.434 | INFO     | __main__:configure_model:71 - => creating model ...
2024-08-21 22:09:50.454 | INFO     | __main__:configure_model:74 - Number of parameters: 1199882
2024-08-21 22:09:50.456 | INFO     | __main__:configure_model:74 - Number of parameters: 1199882
2024-08-21 22:09:52.571 | INFO     | trim.callbacks.misc:resume:179 - => Resuming from checkpoint: output/step_100
2024-08-21 22:09:52.607 | INFO     | trim.callbacks.misc:resume:184 - ### Model weight: -330.6004333496094
2024-08-21 22:09:52.608 | INFO     | trim.callbacks.misc:resume:185 - ### Optimizer weight: -164.9852294921875
2024-08-21 22:09:52.608 | INFO     | trim.callbacks.misc:on_training_phase_start:70 - Namespace(block_size=None, checkpointing_steps='300', gamma=0.7, load_best_model=False, log_project='accl_test', log_tag='init_1', lr=1.0, lr_scheduler_type='linear', num_train_epochs=8, num_warmup_steps=0, output_dir='./output', per_device_eval_batch_size=1, per_device_train_batch_size=196, preprocessing_num_workers=None, report_to='all', resume_from_checkpoint='output/step_300', save_path='output/', seed=1, weight_decay=0.0, with_tracking=False)
2024-08-21 22:09:52.608 | INFO     | trim.engine.trainer:fit:125 - >>>>>>>>>>>>>>>> Start Training >>>>>>>>>>>>>>>>
2024-08-21 22:09:53.235 | INFO     | trim.callbacks.misc:resume:179 - => Resuming from checkpoint: output/step_100
2024-08-21 22:09:53.825 | INFO     | trim.callbacks.misc:resume:184 - ### Model weight: -330.6004333496094
2024-08-21 22:09:53.825 | INFO     | trim.callbacks.misc:resume:185 - ### Optimizer weight: -165.61517333984375
2024-08-21 22:09:53.826 | INFO     | trim.callbacks.misc:on_training_phase_start:70 - Namespace(block_size=None, checkpointing_steps='300', gamma=0.7, load_best_model=False, log_project='accl_test', log_tag='init_1', lr=1.0, lr_scheduler_type='linear', num_train_epochs=8, num_warmup_steps=0, output_dir='./output', per_device_eval_batch_size=1, per_device_train_batch_size=196, preprocessing_num_workers=None, report_to='all', resume_from_checkpoint='output/step_300', save_path='output/', seed=1, weight_decay=0.0, with_tracking=False)
2024-08-21 22:09:53.826 | INFO     | trim.engine.trainer:fit:125 - >>>>>>>>>>>>>>>> Start Training >>>>>>>>>>>>>>>>
2024-08-21 22:09:54.647 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][24/77] Data 0.067 (0.067) Batch 0.819 (0.819) Remain 00:07:02 loss: 0.3816 data: -0.0083 Lr: 0.83604
2024-08-21 22:09:54.647 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][24/77] Data 1.285 (1.285) Batch 2.039 (2.039) Remain 00:17:32 loss: 0.3816 data: 0.0015 Lr: 0.83604
2024-08-21 22:09:54.736 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][25/77] Data 0.035 (0.035) Batch 0.088 (0.088) Remain 00:00:45 loss: 0.3851 data: 0.0146 Lr: 0.83442
2024-08-21 22:09:54.736 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][25/77] Data 0.032 (0.032) Batch 0.090 (0.090) Remain 00:00:46 loss: 0.3851 data: 0.0073 Lr: 0.83442
2024-08-21 22:09:54.818 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][26/77] Data 0.039 (0.037) Batch 0.083 (0.085) Remain 00:00:43 loss: 0.3202 data: 0.0036 Lr: 0.83279
2024-08-21 22:09:54.819 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][26/77] Data 0.030 (0.031) Batch 0.083 (0.087) Remain 00:00:44 loss: 0.3202 data: -0.0063 Lr: 0.83279
2024-08-21 22:09:54.897 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][27/77] Data 0.029 (0.035) Batch 0.078 (0.083) Remain 00:00:42 loss: 0.2140 data: 0.0060 Lr: 0.83117
2024-08-21 22:09:54.897 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][27/77] Data 0.029 (0.030) Batch 0.078 (0.084) Remain 00:00:43 loss: 0.2140 data: -0.0078 Lr: 0.83117
2024-08-21 22:09:54.975 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][28/77] Data 0.032 (0.034) Batch 0.079 (0.082) Remain 00:00:41 loss: 0.2768 data: -0.0034 Lr: 0.82955
2024-08-21 22:09:54.975 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][28/77] Data 0.029 (0.030) Batch 0.079 (0.083) Remain 00:00:42 loss: 0.2768 data: 0.0087 Lr: 0.82955
2024-08-21 22:09:55.051 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][29/77] Data 0.034 (0.034) Batch 0.076 (0.081) Remain 00:00:41 loss: 0.2005 data: 0.0020 Lr: 0.82792
2024-08-21 22:09:55.051 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][29/77] Data 0.029 (0.030) Batch 0.076 (0.081) Remain 00:00:41 loss: 0.2005 data: -0.0066 Lr: 0.82792
2024-08-21 22:09:55.128 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][30/77] Data 0.029 (0.033) Batch 0.077 (0.080) Remain 00:00:40 loss: 0.1882 data: -0.0166 Lr: 0.82630
2024-08-21 22:09:55.128 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][30/77] Data 0.029 (0.030) Batch 0.077 (0.080) Remain 00:00:41 loss: 0.1882 data: 0.0025 Lr: 0.82630
2024-08-21 22:09:55.201 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][31/77] Data 0.029 (0.033) Batch 0.073 (0.079) Remain 00:00:40 loss: 0.3319 data: -0.0011 Lr: 0.82468
2024-08-21 22:09:55.201 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][31/77] Data 0.029 (0.030) Batch 0.073 (0.079) Remain 00:00:40 loss: 0.3319 data: -0.0099 Lr: 0.82468
2024-08-21 22:09:55.273 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][32/77] Data 0.029 (0.032) Batch 0.072 (0.078) Remain 00:00:39 loss: 0.3298 data: -0.0060 Lr: 0.82305
2024-08-21 22:09:55.273 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][32/77] Data 0.029 (0.030) Batch 0.072 (0.079) Remain 00:00:39 loss: 0.3298 data: 0.0004 Lr: 0.82305
2024-08-21 22:09:55.345 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][33/77] Data 0.029 (0.032) Batch 0.072 (0.078) Remain 00:00:39 loss: 0.2418 data: 0.0095 Lr: 0.82143
2024-08-21 22:09:55.346 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][33/77] Data 0.029 (0.029) Batch 0.072 (0.078) Remain 00:00:39 loss: 0.2418 data: 0.0082 Lr: 0.82143
2024-08-21 22:09:55.425 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][34/77] Data 0.035 (0.032) Batch 0.080 (0.078) Remain 00:00:39 loss: 0.2901 data: 0.0002 Lr: 0.81981
2024-08-21 22:09:55.425 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][34/77] Data 0.029 (0.029) Batch 0.080 (0.078) Remain 00:00:39 loss: 0.2901 data: -0.0063 Lr: 0.81981
2024-08-21 22:09:55.499 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][35/77] Data 0.029 (0.032) Batch 0.073 (0.077) Remain 00:00:39 loss: 0.1968 data: -0.0063 Lr: 0.81818
2024-08-21 22:09:55.499 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][35/77] Data 0.029 (0.029) Batch 0.073 (0.078) Remain 00:00:39 loss: 0.1968 data: 0.0032 Lr: 0.81818
2024-08-21 22:09:55.571 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][36/77] Data 0.029 (0.031) Batch 0.072 (0.077) Remain 00:00:38 loss: 0.2877 data: 0.0109 Lr: 0.81656
2024-08-21 22:09:55.571 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][36/77] Data 0.029 (0.029) Batch 0.072 (0.077) Remain 00:00:38 loss: 0.2877 data: 0.0116 Lr: 0.81656
2024-08-21 22:09:55.641 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][37/77] Data 0.029 (0.031) Batch 0.070 (0.076) Remain 00:00:38 loss: 0.2840 data: -0.0038 Lr: 0.81494
2024-08-21 22:09:55.641 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][37/77] Data 0.029 (0.029) Batch 0.071 (0.077) Remain 00:00:38 loss: 0.2840 data: -0.0030 Lr: 0.81494
2024-08-21 22:09:55.712 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][38/77] Data 0.029 (0.031) Batch 0.071 (0.076) Remain 00:00:38 loss: 0.2424 data: 0.0153 Lr: 0.81331
2024-08-21 22:09:55.712 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][38/77] Data 0.029 (0.029) Batch 0.071 (0.076) Remain 00:00:38 loss: 0.2424 data: -0.0051 Lr: 0.81331
2024-08-21 22:09:55.779 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][39/77] Data 0.027 (0.031) Batch 0.067 (0.075) Remain 00:00:37 loss: 0.2652 data: 0.0052 Lr: 0.81169
2024-08-21 22:09:55.779 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][39/77] Data 0.028 (0.029) Batch 0.067 (0.076) Remain 00:00:37 loss: 0.2652 data: -0.0065 Lr: 0.81169
2024-08-21 22:09:55.855 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][40/77] Data 0.028 (0.031) Batch 0.076 (0.075) Remain 00:00:37 loss: 0.2455 data: -0.0181 Lr: 0.81006
2024-08-21 22:09:55.855 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][40/77] Data 0.028 (0.029) Batch 0.076 (0.076) Remain 00:00:37 loss: 0.2455 data: 0.0227 Lr: 0.81006
2024-08-21 22:09:55.936 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][41/77] Data 0.031 (0.031) Batch 0.081 (0.076) Remain 00:00:37 loss: 0.2049 data: 0.0090 Lr: 0.80844
2024-08-21 22:09:55.936 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][41/77] Data 0.028 (0.029) Batch 0.081 (0.076) Remain 00:00:37 loss: 0.2049 data: -0.0145 Lr: 0.80844
2024-08-21 22:09:56.020 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][42/77] Data 0.027 (0.030) Batch 0.084 (0.076) Remain 00:00:37 loss: 0.2821 data: -0.0162 Lr: 0.80682
2024-08-21 22:09:56.020 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][42/77] Data 0.036 (0.029) Batch 0.084 (0.076) Remain 00:00:38 loss: 0.2821 data: -0.0042 Lr: 0.80682
2024-08-21 22:09:56.113 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][43/77] Data 0.037 (0.030) Batch 0.093 (0.077) Remain 00:00:38 loss: 0.2447 data: 0.0085 Lr: 0.80519
2024-08-21 22:09:56.113 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][43/77] Data 0.028 (0.030) Batch 0.093 (0.077) Remain 00:00:38 loss: 0.2447 data: -0.0053 Lr: 0.80519
2024-08-21 22:09:56.183 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][44/77] Data 0.030 (0.030) Batch 0.070 (0.077) Remain 00:00:38 loss: 0.2781 data: 0.0113 Lr: 0.80357
2024-08-21 22:09:56.183 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][44/77] Data 0.028 (0.030) Batch 0.071 (0.077) Remain 00:00:38 loss: 0.2781 data: -0.0014 Lr: 0.80357
2024-08-21 22:09:56.255 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][45/77] Data 0.028 (0.030) Batch 0.072 (0.077) Remain 00:00:37 loss: 0.1962 data: -0.0135 Lr: 0.80195
2024-08-21 22:09:56.255 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][45/77] Data 0.028 (0.030) Batch 0.072 (0.077) Remain 00:00:37 loss: 0.1962 data: 0.0041 Lr: 0.80195
2024-08-21 22:09:56.341 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][46/77] Data 0.029 (0.030) Batch 0.086 (0.077) Remain 00:00:38 loss: 0.1671 data: 0.0038 Lr: 0.80032
2024-08-21 22:09:56.341 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][46/77] Data 0.042 (0.030) Batch 0.086 (0.077) Remain 00:00:38 loss: 0.1671 data: 0.0097 Lr: 0.80032
2024-08-21 22:09:56.414 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][47/77] Data 0.029 (0.030) Batch 0.072 (0.077) Remain 00:00:37 loss: 0.1700 data: -0.0026 Lr: 0.79870
2024-08-21 22:09:56.414 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][47/77] Data 0.029 (0.030) Batch 0.072 (0.077) Remain 00:00:37 loss: 0.1700 data: 0.0149 Lr: 0.79870
2024-08-21 22:09:56.485 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][48/77] Data 0.029 (0.030) Batch 0.072 (0.077) Remain 00:00:37 loss: 0.3486 data: 0.0026 Lr: 0.79708
2024-08-21 22:09:56.486 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][48/77] Data 0.029 (0.030) Batch 0.072 (0.077) Remain 00:00:37 loss: 0.3486 data: 0.0116 Lr: 0.79708
2024-08-21 22:09:56.557 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][49/77] Data 0.028 (0.030) Batch 0.071 (0.076) Remain 00:00:37 loss: 0.2034 data: 0.0012 Lr: 0.79545
2024-08-21 22:09:56.557 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][49/77] Data 0.029 (0.030) Batch 0.071 (0.076) Remain 00:00:37 loss: 0.2034 data: 0.0008 Lr: 0.79545
2024-08-21 22:09:56.628 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][50/77] Data 0.029 (0.030) Batch 0.071 (0.076) Remain 00:00:37 loss: 0.2048 data: 0.0113 Lr: 0.79383
2024-08-21 22:09:56.629 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][50/77] Data 0.029 (0.030) Batch 0.071 (0.076) Remain 00:00:37 loss: 0.2048 data: 0.0039 Lr: 0.79383
2024-08-21 22:09:56.701 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][51/77] Data 0.028 (0.030) Batch 0.072 (0.076) Remain 00:00:37 loss: 0.1710 data: 0.0059 Lr: 0.79221
2024-08-21 22:09:56.701 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][51/77] Data 0.029 (0.030) Batch 0.072 (0.076) Remain 00:00:37 loss: 0.1710 data: 0.0113 Lr: 0.79221
2024-08-21 22:09:56.773 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][52/77] Data 0.028 (0.030) Batch 0.072 (0.076) Remain 00:00:37 loss: 0.1742 data: 0.0255 Lr: 0.79058
2024-08-21 22:09:56.773 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][52/77] Data 0.029 (0.030) Batch 0.072 (0.076) Remain 00:00:37 loss: 0.1742 data: -0.0018 Lr: 0.79058
2024-08-21 22:09:56.843 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][53/77] Data 0.028 (0.030) Batch 0.071 (0.076) Remain 00:00:36 loss: 0.3221 data: -0.0151 Lr: 0.78896
2024-08-21 22:09:56.843 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][53/77] Data 0.029 (0.030) Batch 0.071 (0.076) Remain 00:00:36 loss: 0.3221 data: 0.0226 Lr: 0.78896
2024-08-21 22:09:56.930 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][54/77] Data 0.034 (0.030) Batch 0.087 (0.076) Remain 00:00:36 loss: 0.2002 data: -0.0040 Lr: 0.78734
2024-08-21 22:09:56.930 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][54/77] Data 0.029 (0.030) Batch 0.087 (0.076) Remain 00:00:37 loss: 0.2002 data: -0.0010 Lr: 0.78734
2024-08-21 22:09:57.017 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][55/77] Data 0.040 (0.030) Batch 0.087 (0.076) Remain 00:00:37 loss: 0.2423 data: 0.0194 Lr: 0.78571
2024-08-21 22:09:57.017 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][55/77] Data 0.029 (0.030) Batch 0.087 (0.077) Remain 00:00:37 loss: 0.2423 data: 0.0157 Lr: 0.78571
2024-08-21 22:09:57.100 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][56/77] Data 0.036 (0.030) Batch 0.083 (0.077) Remain 00:00:37 loss: 0.1732 data: -0.0027 Lr: 0.78409
2024-08-21 22:09:57.100 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][56/77] Data 0.029 (0.030) Batch 0.083 (0.077) Remain 00:00:37 loss: 0.1732 data: -0.0060 Lr: 0.78409
2024-08-21 22:09:57.176 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][57/77] Data 0.028 (0.030) Batch 0.076 (0.077) Remain 00:00:37 loss: 0.1966 data: -0.0072 Lr: 0.78247
2024-08-21 22:09:57.176 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][57/77] Data 0.029 (0.030) Batch 0.076 (0.077) Remain 00:00:37 loss: 0.1966 data: -0.0121 Lr: 0.78247
2024-08-21 22:09:57.267 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][58/77] Data 0.038 (0.031) Batch 0.091 (0.077) Remain 00:00:37 loss: 0.2412 data: 0.0202 Lr: 0.78084
2024-08-21 22:09:57.267 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][58/77] Data 0.029 (0.030) Batch 0.091 (0.077) Remain 00:00:37 loss: 0.2412 data: -0.0011 Lr: 0.78084
2024-08-21 22:09:57.348 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][59/77] Data 0.038 (0.031) Batch 0.081 (0.077) Remain 00:00:37 loss: 0.2276 data: -0.0228 Lr: 0.77922
2024-08-21 22:09:57.348 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][59/77] Data 0.029 (0.030) Batch 0.081 (0.077) Remain 00:00:37 loss: 0.2276 data: -0.0205 Lr: 0.77922
2024-08-21 22:09:57.418 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][60/77] Data 0.028 (0.031) Batch 0.070 (0.077) Remain 00:00:36 loss: 0.2267 data: 0.0153 Lr: 0.77760
2024-08-21 22:09:57.418 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][60/77] Data 0.029 (0.030) Batch 0.070 (0.077) Remain 00:00:36 loss: 0.2267 data: 0.0115 Lr: 0.77760
2024-08-21 22:09:57.483 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][61/77] Data 0.027 (0.031) Batch 0.066 (0.077) Remain 00:00:36 loss: 0.3384 data: 0.0110 Lr: 0.77597
2024-08-21 22:09:57.484 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][61/77] Data 0.027 (0.030) Batch 0.066 (0.077) Remain 00:00:36 loss: 0.3384 data: -0.0004 Lr: 0.77597
2024-08-21 22:09:57.549 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][62/77] Data 0.027 (0.031) Batch 0.065 (0.076) Remain 00:00:36 loss: 0.2547 data: 0.0010 Lr: 0.77435
2024-08-21 22:09:57.549 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][62/77] Data 0.027 (0.030) Batch 0.065 (0.076) Remain 00:00:36 loss: 0.2547 data: -0.0151 Lr: 0.77435
2024-08-21 22:09:57.616 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][63/77] Data 0.027 (0.030) Batch 0.068 (0.076) Remain 00:00:36 loss: 0.2844 data: 0.0141 Lr: 0.77273
2024-08-21 22:09:57.617 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][63/77] Data 0.027 (0.029) Batch 0.068 (0.076) Remain 00:00:36 loss: 0.2844 data: 0.0257 Lr: 0.77273
2024-08-21 22:09:57.683 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][64/77] Data 0.027 (0.030) Batch 0.067 (0.076) Remain 00:00:36 loss: 0.2325 data: 0.0234 Lr: 0.77110
2024-08-21 22:09:57.684 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][64/77] Data 0.027 (0.029) Batch 0.067 (0.076) Remain 00:00:36 loss: 0.2325 data: -0.0081 Lr: 0.77110
2024-08-21 22:09:57.754 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][65/77] Data 0.028 (0.030) Batch 0.071 (0.076) Remain 00:00:35 loss: 0.3061 data: 0.0046 Lr: 0.76948
2024-08-21 22:09:57.754 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][65/77] Data 0.028 (0.029) Batch 0.071 (0.076) Remain 00:00:36 loss: 0.3061 data: -0.0014 Lr: 0.76948
2024-08-21 22:09:57.822 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][66/77] Data 0.028 (0.030) Batch 0.068 (0.076) Remain 00:00:35 loss: 0.1681 data: -0.0121 Lr: 0.76786
2024-08-21 22:09:57.822 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][66/77] Data 0.029 (0.029) Batch 0.068 (0.076) Remain 00:00:35 loss: 0.1681 data: -0.0012 Lr: 0.76786
2024-08-21 22:09:57.888 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][67/77] Data 0.027 (0.030) Batch 0.067 (0.075) Remain 00:00:35 loss: 0.2189 data: -0.0150 Lr: 0.76623
2024-08-21 22:09:57.889 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][67/77] Data 0.027 (0.029) Batch 0.067 (0.075) Remain 00:00:35 loss: 0.2189 data: 0.0110 Lr: 0.76623
2024-08-21 22:09:57.961 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][68/77] Data 0.028 (0.030) Batch 0.072 (0.075) Remain 00:00:35 loss: 0.2278 data: -0.0077 Lr: 0.76461
2024-08-21 22:09:57.961 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][68/77] Data 0.028 (0.029) Batch 0.073 (0.075) Remain 00:00:35 loss: 0.2278 data: -0.0059 Lr: 0.76461
2024-08-21 22:09:58.033 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][69/77] Data 0.028 (0.030) Batch 0.072 (0.075) Remain 00:00:35 loss: 0.1635 data: 0.0118 Lr: 0.76299
2024-08-21 22:09:58.033 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][69/77] Data 0.029 (0.029) Batch 0.072 (0.075) Remain 00:00:35 loss: 0.1635 data: -0.0006 Lr: 0.76299
2024-08-21 22:09:58.101 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][70/77] Data 0.028 (0.030) Batch 0.068 (0.075) Remain 00:00:35 loss: 0.1840 data: -0.0061 Lr: 0.76136
2024-08-21 22:09:58.101 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][70/77] Data 0.029 (0.029) Batch 0.068 (0.075) Remain 00:00:35 loss: 0.1840 data: -0.0068 Lr: 0.76136
2024-08-21 22:09:58.166 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][71/77] Data 0.027 (0.030) Batch 0.065 (0.075) Remain 00:00:35 loss: 0.1901 data: 0.0023 Lr: 0.75974
2024-08-21 22:09:58.166 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][71/77] Data 0.027 (0.029) Batch 0.065 (0.075) Remain 00:00:35 loss: 0.1901 data: 0.0071 Lr: 0.75974
2024-08-21 22:09:58.244 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][72/77] Data 0.027 (0.030) Batch 0.078 (0.075) Remain 00:00:35 loss: 0.2607 data: -0.0090 Lr: 0.75812
2024-08-21 22:09:58.244 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][72/77] Data 0.027 (0.029) Batch 0.078 (0.075) Remain 00:00:35 loss: 0.2607 data: -0.0034 Lr: 0.75812
2024-08-21 22:09:58.323 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][73/77] Data 0.033 (0.030) Batch 0.078 (0.075) Remain 00:00:35 loss: 0.2247 data: 0.0081 Lr: 0.75649
2024-08-21 22:09:58.323 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_150
2024-08-21 22:09:58.323 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][73/77] Data 0.029 (0.029) Batch 0.078 (0.075) Remain 00:00:35 loss: 0.2247 data: -0.0024 Lr: 0.75649
2024-08-21 22:09:58.323 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_150
2024-08-21 22:09:58.356 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -401.701171875
2024-08-21 22:09:58.356 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -401.701171875
2024-08-21 22:09:58.356 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -188.16018676757812
2024-08-21 22:09:58.356 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -213.541015625
2024-08-21 22:09:58.429 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][74/77] Data 0.064 (0.031) Batch 0.106 (0.076) Remain 00:00:35 loss: 0.1340 data: -0.0020 Lr: 0.75487
2024-08-21 22:09:58.429 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][74/77] Data 0.061 (0.030) Batch 0.106 (0.076) Remain 00:00:35 loss: 0.1340 data: 0.0154 Lr: 0.75487
2024-08-21 22:09:58.501 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][75/77] Data 0.031 (0.031) Batch 0.072 (0.076) Remain 00:00:35 loss: 0.1775 data: 0.0045 Lr: 0.75325
2024-08-21 22:09:58.501 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][75/77] Data 0.028 (0.030) Batch 0.072 (0.076) Remain 00:00:35 loss: 0.1775 data: -0.0077 Lr: 0.75325
2024-08-21 22:09:58.572 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][76/77] Data 0.031 (0.031) Batch 0.071 (0.075) Remain 00:00:35 loss: 0.2486 data: 0.0079 Lr: 0.75162
2024-08-21 22:09:58.572 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][76/77] Data 0.028 (0.030) Batch 0.071 (0.076) Remain 00:00:35 loss: 0.2486 data: 0.0015 Lr: 0.75162
2024-08-21 22:09:58.618 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][77/77] Data 0.031 (0.030) Batch 0.046 (0.075) Remain 00:00:34 loss: 0.1978 data: -0.0010 Lr: 0.75000
2024-08-21 22:09:58.618 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][77/77] Data 0.034 (0.031) Batch 0.046 (0.075) Remain 00:00:34 loss: 0.1978 data: -0.0059 Lr: 0.75000
2024-08-21 22:09:58.619 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 22:09:58.619 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 22:10:02.985 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0537, Accuracy: 0.9826
2024-08-21 22:10:02.985 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0537, Accuracy: 0.9826
2024-08-21 22:10:02.986 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 22:10:02.986 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 22:10:02.991 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 22:10:02.991 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 22:10:03.089 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][1/77] Data 0.057 (0.057) Batch 0.098 (0.098) Remain 00:00:45 loss: 0.1433 data: 0.0100 Lr: 0.74838
2024-08-21 22:10:03.089 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][1/77] Data 0.057 (0.057) Batch 0.098 (0.098) Remain 00:00:45 loss: 0.1433 data: 0.0028 Lr: 0.74838
2024-08-21 22:10:03.154 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][2/77] Data 0.028 (0.028) Batch 0.064 (0.064) Remain 00:00:29 loss: 0.2010 data: -0.0113 Lr: 0.74675
2024-08-21 22:10:03.154 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][2/77] Data 0.028 (0.028) Batch 0.064 (0.064) Remain 00:00:29 loss: 0.2010 data: 0.0047 Lr: 0.74675
2024-08-21 22:10:03.221 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][3/77] Data 0.027 (0.028) Batch 0.067 (0.066) Remain 00:00:30 loss: 0.1559 data: -0.0137 Lr: 0.74513
2024-08-21 22:10:03.221 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][3/77] Data 0.028 (0.028) Batch 0.067 (0.066) Remain 00:00:30 loss: 0.1559 data: 0.0043 Lr: 0.74513
2024-08-21 22:10:03.288 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][4/77] Data 0.028 (0.028) Batch 0.067 (0.066) Remain 00:00:30 loss: 0.2485 data: 0.0291 Lr: 0.74351
2024-08-21 22:10:03.288 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][4/77] Data 0.028 (0.028) Batch 0.067 (0.066) Remain 00:00:30 loss: 0.2485 data: -0.0114 Lr: 0.74351
2024-08-21 22:10:03.355 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][5/77] Data 0.028 (0.028) Batch 0.067 (0.066) Remain 00:00:30 loss: 0.1985 data: 0.0033 Lr: 0.74188
2024-08-21 22:10:03.355 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][5/77] Data 0.028 (0.028) Batch 0.067 (0.066) Remain 00:00:30 loss: 0.1985 data: 0.0068 Lr: 0.74188
2024-08-21 22:10:03.422 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][6/77] Data 0.028 (0.028) Batch 0.067 (0.067) Remain 00:00:30 loss: 0.1360 data: 0.0031 Lr: 0.74026
2024-08-21 22:10:03.422 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][6/77] Data 0.028 (0.028) Batch 0.067 (0.067) Remain 00:00:30 loss: 0.1360 data: -0.0023 Lr: 0.74026
2024-08-21 22:10:03.490 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][7/77] Data 0.028 (0.028) Batch 0.067 (0.067) Remain 00:00:30 loss: 0.2289 data: 0.0027 Lr: 0.73864
2024-08-21 22:10:03.490 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][7/77] Data 0.028 (0.028) Batch 0.067 (0.067) Remain 00:00:30 loss: 0.2289 data: -0.0093 Lr: 0.73864
2024-08-21 22:10:03.556 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][8/77] Data 0.027 (0.028) Batch 0.066 (0.067) Remain 00:00:30 loss: 0.1295 data: -0.0128 Lr: 0.73701
2024-08-21 22:10:03.556 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][8/77] Data 0.027 (0.028) Batch 0.066 (0.067) Remain 00:00:30 loss: 0.1295 data: -0.0005 Lr: 0.73701
2024-08-21 22:10:03.622 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][9/77] Data 0.028 (0.028) Batch 0.066 (0.067) Remain 00:00:30 loss: 0.1605 data: -0.0067 Lr: 0.73539
2024-08-21 22:10:03.622 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][9/77] Data 0.028 (0.028) Batch 0.066 (0.067) Remain 00:00:30 loss: 0.1605 data: -0.0131 Lr: 0.73539
2024-08-21 22:10:03.693 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][10/77] Data 0.028 (0.028) Batch 0.071 (0.067) Remain 00:00:30 loss: 0.2317 data: -0.0018 Lr: 0.73377
2024-08-21 22:10:03.693 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][10/77] Data 0.028 (0.028) Batch 0.071 (0.067) Remain 00:00:30 loss: 0.2317 data: 0.0008 Lr: 0.73377
2024-08-21 22:10:03.773 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][11/77] Data 0.027 (0.028) Batch 0.080 (0.068) Remain 00:00:30 loss: 0.2180 data: -0.0030 Lr: 0.73214
2024-08-21 22:10:03.773 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][11/77] Data 0.035 (0.029) Batch 0.080 (0.068) Remain 00:00:30 loss: 0.2180 data: -0.0072 Lr: 0.73214
2024-08-21 22:10:03.847 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][12/77] Data 0.028 (0.028) Batch 0.075 (0.069) Remain 00:00:31 loss: 0.1579 data: 0.0053 Lr: 0.73052
2024-08-21 22:10:03.847 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][12/77] Data 0.034 (0.029) Batch 0.075 (0.069) Remain 00:00:31 loss: 0.1579 data: 0.0176 Lr: 0.73052
2024-08-21 22:10:03.914 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][13/77] Data 0.028 (0.028) Batch 0.067 (0.069) Remain 00:00:30 loss: 0.1533 data: 0.0083 Lr: 0.72890
2024-08-21 22:10:03.914 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][13/77] Data 0.028 (0.029) Batch 0.067 (0.069) Remain 00:00:30 loss: 0.1533 data: 0.0099 Lr: 0.72890
2024-08-21 22:10:03.980 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][14/77] Data 0.028 (0.028) Batch 0.066 (0.069) Remain 00:00:30 loss: 0.1511 data: -0.0109 Lr: 0.72727
2024-08-21 22:10:03.981 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][14/77] Data 0.028 (0.029) Batch 0.066 (0.069) Remain 00:00:30 loss: 0.1511 data: -0.0031 Lr: 0.72727
2024-08-21 22:10:04.047 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][15/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:30 loss: 0.1364 data: -0.0187 Lr: 0.72565
2024-08-21 22:10:04.047 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][15/77] Data 0.028 (0.029) Batch 0.066 (0.068) Remain 00:00:30 loss: 0.1364 data: -0.0017 Lr: 0.72565
2024-08-21 22:10:04.113 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][16/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:30 loss: 0.1758 data: -0.0092 Lr: 0.72403
2024-08-21 22:10:04.113 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][16/77] Data 0.028 (0.029) Batch 0.066 (0.068) Remain 00:00:30 loss: 0.1758 data: -0.0074 Lr: 0.72403
2024-08-21 22:10:04.179 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][17/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:30 loss: 0.1885 data: 0.0063 Lr: 0.72240
2024-08-21 22:10:04.179 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][17/77] Data 0.027 (0.029) Batch 0.066 (0.068) Remain 00:00:30 loss: 0.1885 data: 0.0055 Lr: 0.72240
2024-08-21 22:10:04.245 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][18/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:30 loss: 0.1692 data: -0.0094 Lr: 0.72078
2024-08-21 22:10:04.245 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][18/77] Data 0.028 (0.028) Batch 0.066 (0.068) Remain 00:00:30 loss: 0.1692 data: -0.0130 Lr: 0.72078
2024-08-21 22:10:04.310 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][19/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:30 loss: 0.1134 data: -0.0153 Lr: 0.71916
2024-08-21 22:10:04.311 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][19/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:30 loss: 0.1134 data: 0.0081 Lr: 0.71916
2024-08-21 22:10:04.377 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][20/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:30 loss: 0.3109 data: -0.0177 Lr: 0.71753
2024-08-21 22:10:04.377 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][20/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:30 loss: 0.3109 data: -0.0091 Lr: 0.71753
2024-08-21 22:10:04.443 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][21/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:29 loss: 0.2134 data: -0.0022 Lr: 0.71591
2024-08-21 22:10:04.443 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][21/77] Data 0.028 (0.028) Batch 0.066 (0.068) Remain 00:00:29 loss: 0.2134 data: -0.0036 Lr: 0.71591
2024-08-21 22:10:04.509 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][22/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:29 loss: 0.1862 data: -0.0025 Lr: 0.71429
2024-08-21 22:10:04.509 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][22/77] Data 0.028 (0.028) Batch 0.066 (0.068) Remain 00:00:29 loss: 0.1862 data: 0.0054 Lr: 0.71429
2024-08-21 22:10:04.574 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][23/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:29 loss: 0.1769 data: -0.0095 Lr: 0.71266
2024-08-21 22:10:04.574 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][23/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:29 loss: 0.1769 data: -0.0070 Lr: 0.71266
2024-08-21 22:10:04.639 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][24/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:29 loss: 0.1459 data: -0.0063 Lr: 0.71104
2024-08-21 22:10:04.640 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][24/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:29 loss: 0.1459 data: 0.0036 Lr: 0.71104
2024-08-21 22:10:04.711 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][25/77] Data 0.027 (0.028) Batch 0.071 (0.068) Remain 00:00:29 loss: 0.1575 data: 0.0165 Lr: 0.70942
2024-08-21 22:10:04.711 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][25/77] Data 0.033 (0.028) Batch 0.071 (0.068) Remain 00:00:29 loss: 0.1575 data: 0.0106 Lr: 0.70942
2024-08-21 22:10:04.777 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][26/77] Data 0.027 (0.028) Batch 0.066 (0.067) Remain 00:00:29 loss: 0.1438 data: -0.0164 Lr: 0.70779
2024-08-21 22:10:04.777 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][26/77] Data 0.027 (0.028) Batch 0.066 (0.067) Remain 00:00:29 loss: 0.1438 data: -0.0029 Lr: 0.70779
2024-08-21 22:10:04.844 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][27/77] Data 0.027 (0.027) Batch 0.067 (0.067) Remain 00:00:29 loss: 0.2162 data: 0.0028 Lr: 0.70617
2024-08-21 22:10:04.844 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][27/77] Data 0.028 (0.028) Batch 0.067 (0.067) Remain 00:00:29 loss: 0.2162 data: 0.0169 Lr: 0.70617
2024-08-21 22:10:04.915 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][28/77] Data 0.029 (0.028) Batch 0.071 (0.068) Remain 00:00:29 loss: 0.1600 data: -0.0073 Lr: 0.70455
2024-08-21 22:10:04.915 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][28/77] Data 0.028 (0.028) Batch 0.071 (0.068) Remain 00:00:29 loss: 0.1600 data: -0.0012 Lr: 0.70455
2024-08-21 22:10:04.986 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][29/77] Data 0.029 (0.028) Batch 0.071 (0.068) Remain 00:00:29 loss: 0.3079 data: -0.0012 Lr: 0.70292
2024-08-21 22:10:04.986 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][29/77] Data 0.029 (0.028) Batch 0.071 (0.068) Remain 00:00:29 loss: 0.3079 data: -0.0033 Lr: 0.70292
2024-08-21 22:10:05.057 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][30/77] Data 0.029 (0.028) Batch 0.071 (0.068) Remain 00:00:29 loss: 0.2315 data: -0.0090 Lr: 0.70130
2024-08-21 22:10:05.057 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][30/77] Data 0.029 (0.028) Batch 0.071 (0.068) Remain 00:00:29 loss: 0.2315 data: -0.0072 Lr: 0.70130
2024-08-21 22:10:05.136 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][31/77] Data 0.029 (0.028) Batch 0.079 (0.068) Remain 00:00:29 loss: 0.2315 data: 0.0136 Lr: 0.69968
2024-08-21 22:10:05.136 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][31/77] Data 0.028 (0.028) Batch 0.079 (0.068) Remain 00:00:29 loss: 0.2315 data: -0.0186 Lr: 0.69968
2024-08-21 22:10:05.208 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][32/77] Data 0.029 (0.028) Batch 0.072 (0.068) Remain 00:00:29 loss: 0.1440 data: 0.0071 Lr: 0.69805
2024-08-21 22:10:05.208 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][32/77] Data 0.029 (0.028) Batch 0.072 (0.068) Remain 00:00:29 loss: 0.1440 data: -0.0037 Lr: 0.69805
2024-08-21 22:10:05.279 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][33/77] Data 0.029 (0.028) Batch 0.071 (0.068) Remain 00:00:29 loss: 0.1274 data: 0.0149 Lr: 0.69643
2024-08-21 22:10:05.279 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][33/77] Data 0.029 (0.028) Batch 0.071 (0.068) Remain 00:00:29 loss: 0.1274 data: -0.0027 Lr: 0.69643
2024-08-21 22:10:05.355 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][34/77] Data 0.028 (0.028) Batch 0.077 (0.069) Remain 00:00:29 loss: 0.2085 data: 0.0117 Lr: 0.69481
2024-08-21 22:10:05.356 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][34/77] Data 0.029 (0.028) Batch 0.077 (0.069) Remain 00:00:29 loss: 0.2085 data: -0.0164 Lr: 0.69481
2024-08-21 22:10:05.431 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][35/77] Data 0.028 (0.028) Batch 0.075 (0.069) Remain 00:00:29 loss: 0.2336 data: 0.0046 Lr: 0.69318
2024-08-21 22:10:05.431 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][35/77] Data 0.030 (0.028) Batch 0.075 (0.069) Remain 00:00:29 loss: 0.2336 data: 0.0049 Lr: 0.69318
2024-08-21 22:10:05.499 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][36/77] Data 0.029 (0.028) Batch 0.068 (0.069) Remain 00:00:29 loss: 0.2035 data: -0.0107 Lr: 0.69156
2024-08-21 22:10:05.499 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][36/77] Data 0.028 (0.028) Batch 0.068 (0.069) Remain 00:00:29 loss: 0.2035 data: -0.0141 Lr: 0.69156
2024-08-21 22:10:05.569 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][37/77] Data 0.027 (0.028) Batch 0.071 (0.069) Remain 00:00:29 loss: 0.1134 data: -0.0088 Lr: 0.68994
2024-08-21 22:10:05.570 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][37/77] Data 0.029 (0.028) Batch 0.071 (0.069) Remain 00:00:29 loss: 0.1134 data: 0.0044 Lr: 0.68994
2024-08-21 22:10:05.639 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][38/77] Data 0.028 (0.028) Batch 0.070 (0.069) Remain 00:00:29 loss: 0.1201 data: 0.0169 Lr: 0.68831
2024-08-21 22:10:05.639 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][38/77] Data 0.031 (0.028) Batch 0.070 (0.069) Remain 00:00:29 loss: 0.1201 data: 0.0043 Lr: 0.68831
2024-08-21 22:10:05.712 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][39/77] Data 0.027 (0.028) Batch 0.072 (0.069) Remain 00:00:29 loss: 0.1722 data: 0.0070 Lr: 0.68669
2024-08-21 22:10:05.712 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][39/77] Data 0.028 (0.028) Batch 0.072 (0.069) Remain 00:00:29 loss: 0.1722 data: 0.0037 Lr: 0.68669
2024-08-21 22:10:05.785 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][40/77] Data 0.028 (0.028) Batch 0.073 (0.069) Remain 00:00:29 loss: 0.0761 data: 0.0044 Lr: 0.68506
2024-08-21 22:10:05.785 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][40/77] Data 0.032 (0.029) Batch 0.073 (0.069) Remain 00:00:29 loss: 0.0761 data: -0.0223 Lr: 0.68506
2024-08-21 22:10:05.850 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][41/77] Data 0.028 (0.028) Batch 0.065 (0.069) Remain 00:00:29 loss: 0.1449 data: 0.0002 Lr: 0.68344
2024-08-21 22:10:05.850 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][41/77] Data 0.027 (0.029) Batch 0.065 (0.069) Remain 00:00:29 loss: 0.1449 data: 0.0003 Lr: 0.68344
2024-08-21 22:10:05.917 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][42/77] Data 0.027 (0.028) Batch 0.066 (0.069) Remain 00:00:29 loss: 0.1473 data: -0.0108 Lr: 0.68182
2024-08-21 22:10:05.917 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][42/77] Data 0.028 (0.029) Batch 0.066 (0.069) Remain 00:00:29 loss: 0.1473 data: -0.0043 Lr: 0.68182
2024-08-21 22:10:05.983 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][43/77] Data 0.027 (0.028) Batch 0.066 (0.069) Remain 00:00:28 loss: 0.2381 data: -0.0178 Lr: 0.68019
2024-08-21 22:10:05.983 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][43/77] Data 0.027 (0.028) Batch 0.066 (0.069) Remain 00:00:28 loss: 0.2381 data: -0.0002 Lr: 0.68019
2024-08-21 22:10:06.049 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][44/77] Data 0.027 (0.028) Batch 0.066 (0.069) Remain 00:00:28 loss: 0.2339 data: -0.0085 Lr: 0.67857
2024-08-21 22:10:06.049 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][44/77] Data 0.028 (0.028) Batch 0.066 (0.069) Remain 00:00:28 loss: 0.2339 data: 0.0022 Lr: 0.67857
2024-08-21 22:10:06.118 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][45/77] Data 0.027 (0.028) Batch 0.069 (0.069) Remain 00:00:28 loss: 0.1920 data: -0.0076 Lr: 0.67695
2024-08-21 22:10:06.118 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][45/77] Data 0.028 (0.028) Batch 0.069 (0.069) Remain 00:00:28 loss: 0.1920 data: -0.0029 Lr: 0.67695
2024-08-21 22:10:06.189 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][46/77] Data 0.027 (0.028) Batch 0.071 (0.069) Remain 00:00:28 loss: 0.2487 data: -0.0116 Lr: 0.67532
2024-08-21 22:10:06.189 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_200
2024-08-21 22:10:06.189 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][46/77] Data 0.031 (0.029) Batch 0.071 (0.069) Remain 00:00:28 loss: 0.2487 data: 0.0039 Lr: 0.67532
2024-08-21 22:10:06.189 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_200
2024-08-21 22:10:06.224 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -479.5213623046875
2024-08-21 22:10:06.224 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -234.31944274902344
2024-08-21 22:10:06.224 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -479.5213623046875
2024-08-21 22:10:06.225 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -245.20193481445312
2024-08-21 22:10:06.308 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][47/77] Data 0.064 (0.029) Batch 0.120 (0.070) Remain 00:00:29 loss: 0.2121 data: -0.0083 Lr: 0.67370
2024-08-21 22:10:06.309 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][47/77] Data 0.069 (0.029) Batch 0.120 (0.070) Remain 00:00:29 loss: 0.2121 data: 0.0109 Lr: 0.67370
2024-08-21 22:10:06.387 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][48/77] Data 0.028 (0.029) Batch 0.078 (0.070) Remain 00:00:29 loss: 0.1426 data: -0.0018 Lr: 0.67208
2024-08-21 22:10:06.387 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][48/77] Data 0.037 (0.030) Batch 0.078 (0.070) Remain 00:00:29 loss: 0.1426 data: 0.0121 Lr: 0.67208
2024-08-21 22:10:06.467 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][49/77] Data 0.028 (0.029) Batch 0.080 (0.070) Remain 00:00:29 loss: 0.2015 data: -0.0057 Lr: 0.67045
2024-08-21 22:10:06.467 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][49/77] Data 0.029 (0.030) Batch 0.080 (0.070) Remain 00:00:29 loss: 0.2015 data: -0.0056 Lr: 0.67045
2024-08-21 22:10:06.545 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][50/77] Data 0.028 (0.029) Batch 0.078 (0.071) Remain 00:00:29 loss: 0.1773 data: -0.0059 Lr: 0.66883
2024-08-21 22:10:06.545 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][50/77] Data 0.029 (0.030) Batch 0.078 (0.071) Remain 00:00:29 loss: 0.1773 data: -0.0177 Lr: 0.66883
2024-08-21 22:10:06.623 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][51/77] Data 0.028 (0.029) Batch 0.078 (0.071) Remain 00:00:29 loss: 0.1563 data: 0.0019 Lr: 0.66721
2024-08-21 22:10:06.623 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][51/77] Data 0.029 (0.030) Batch 0.078 (0.071) Remain 00:00:29 loss: 0.1563 data: 0.0120 Lr: 0.66721
2024-08-21 22:10:06.695 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][52/77] Data 0.029 (0.030) Batch 0.072 (0.071) Remain 00:00:29 loss: 0.2295 data: 0.0017 Lr: 0.66558
2024-08-21 22:10:06.695 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][52/77] Data 0.028 (0.028) Batch 0.072 (0.071) Remain 00:00:29 loss: 0.2295 data: -0.0249 Lr: 0.66558
2024-08-21 22:10:06.769 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][53/77] Data 0.030 (0.029) Batch 0.074 (0.071) Remain 00:00:29 loss: 0.1232 data: -0.0014 Lr: 0.66396
2024-08-21 22:10:06.769 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][53/77] Data 0.029 (0.029) Batch 0.074 (0.071) Remain 00:00:29 loss: 0.1232 data: 0.0034 Lr: 0.66396
2024-08-21 22:10:06.851 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][54/77] Data 0.029 (0.029) Batch 0.082 (0.071) Remain 00:00:29 loss: 0.1935 data: 0.0218 Lr: 0.66234
2024-08-21 22:10:06.851 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][54/77] Data 0.029 (0.029) Batch 0.082 (0.071) Remain 00:00:29 loss: 0.1935 data: 0.0069 Lr: 0.66234
2024-08-21 22:10:06.922 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][55/77] Data 0.029 (0.029) Batch 0.071 (0.071) Remain 00:00:28 loss: 0.1449 data: 0.0029 Lr: 0.66071
2024-08-21 22:10:06.922 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][55/77] Data 0.030 (0.029) Batch 0.071 (0.071) Remain 00:00:28 loss: 0.1449 data: 0.0152 Lr: 0.66071
2024-08-21 22:10:06.990 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][56/77] Data 0.027 (0.029) Batch 0.067 (0.071) Remain 00:00:28 loss: 0.2166 data: 0.0070 Lr: 0.65909
2024-08-21 22:10:06.990 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][56/77] Data 0.027 (0.029) Batch 0.067 (0.071) Remain 00:00:28 loss: 0.2166 data: -0.0028 Lr: 0.65909
2024-08-21 22:10:07.056 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][57/77] Data 0.027 (0.029) Batch 0.067 (0.071) Remain 00:00:28 loss: 0.1608 data: 0.0044 Lr: 0.65747
2024-08-21 22:10:07.057 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][57/77] Data 0.028 (0.029) Batch 0.067 (0.071) Remain 00:00:28 loss: 0.1608 data: -0.0048 Lr: 0.65747
2024-08-21 22:10:07.124 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][58/77] Data 0.027 (0.028) Batch 0.068 (0.071) Remain 00:00:28 loss: 0.0859 data: 0.0036 Lr: 0.65584
2024-08-21 22:10:07.124 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][58/77] Data 0.027 (0.029) Batch 0.068 (0.071) Remain 00:00:28 loss: 0.0859 data: 0.0017 Lr: 0.65584
2024-08-21 22:10:07.193 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][59/77] Data 0.028 (0.028) Batch 0.069 (0.071) Remain 00:00:28 loss: 0.2018 data: -0.0058 Lr: 0.65422
2024-08-21 22:10:07.193 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][59/77] Data 0.028 (0.029) Batch 0.069 (0.071) Remain 00:00:28 loss: 0.2018 data: 0.0104 Lr: 0.65422
2024-08-21 22:10:07.265 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][60/77] Data 0.028 (0.028) Batch 0.072 (0.071) Remain 00:00:28 loss: 0.2311 data: 0.0062 Lr: 0.65260
2024-08-21 22:10:07.266 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][60/77] Data 0.028 (0.029) Batch 0.072 (0.071) Remain 00:00:28 loss: 0.2311 data: -0.0078 Lr: 0.65260
2024-08-21 22:10:07.338 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][61/77] Data 0.029 (0.028) Batch 0.073 (0.071) Remain 00:00:28 loss: 0.2292 data: -0.0027 Lr: 0.65097
2024-08-21 22:10:07.338 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][61/77] Data 0.030 (0.029) Batch 0.073 (0.071) Remain 00:00:28 loss: 0.2292 data: 0.0060 Lr: 0.65097
2024-08-21 22:10:07.407 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][62/77] Data 0.028 (0.029) Batch 0.068 (0.071) Remain 00:00:28 loss: 0.2088 data: -0.0130 Lr: 0.64935
2024-08-21 22:10:07.407 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][62/77] Data 0.028 (0.028) Batch 0.069 (0.071) Remain 00:00:28 loss: 0.2088 data: 0.0132 Lr: 0.64935
2024-08-21 22:10:07.477 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][63/77] Data 0.033 (0.029) Batch 0.070 (0.071) Remain 00:00:28 loss: 0.1704 data: 0.0036 Lr: 0.64773
2024-08-21 22:10:07.477 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][63/77] Data 0.027 (0.029) Batch 0.071 (0.071) Remain 00:00:28 loss: 0.1704 data: 0.0162 Lr: 0.64773
2024-08-21 22:10:07.542 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][64/77] Data 0.027 (0.029) Batch 0.064 (0.071) Remain 00:00:28 loss: 0.1535 data: 0.0171 Lr: 0.64610
2024-08-21 22:10:07.542 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][64/77] Data 0.027 (0.029) Batch 0.064 (0.071) Remain 00:00:28 loss: 0.1535 data: 0.0150 Lr: 0.64610
2024-08-21 22:10:07.608 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][65/77] Data 0.027 (0.028) Batch 0.067 (0.071) Remain 00:00:28 loss: 0.1796 data: -0.0064 Lr: 0.64448
2024-08-21 22:10:07.608 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][65/77] Data 0.029 (0.029) Batch 0.067 (0.071) Remain 00:00:28 loss: 0.1796 data: 0.0033 Lr: 0.64448
2024-08-21 22:10:07.681 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][66/77] Data 0.027 (0.028) Batch 0.073 (0.071) Remain 00:00:28 loss: 0.0967 data: 0.0051 Lr: 0.64286
2024-08-21 22:10:07.681 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][66/77] Data 0.028 (0.029) Batch 0.073 (0.071) Remain 00:00:28 loss: 0.0967 data: 0.0126 Lr: 0.64286
2024-08-21 22:10:07.788 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][67/77] Data 0.028 (0.029) Batch 0.107 (0.071) Remain 00:00:28 loss: 0.0975 data: 0.0009 Lr: 0.64123
2024-08-21 22:10:07.788 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][67/77] Data 0.044 (0.029) Batch 0.107 (0.071) Remain 00:00:28 loss: 0.0975 data: -0.0006 Lr: 0.64123
2024-08-21 22:10:07.864 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][68/77] Data 0.035 (0.029) Batch 0.076 (0.071) Remain 00:00:28 loss: 0.1222 data: -0.0102 Lr: 0.63961
2024-08-21 22:10:07.864 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][68/77] Data 0.028 (0.029) Batch 0.076 (0.071) Remain 00:00:28 loss: 0.1222 data: -0.0143 Lr: 0.63961
2024-08-21 22:10:07.936 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][69/77] Data 0.028 (0.029) Batch 0.073 (0.071) Remain 00:00:28 loss: 0.2069 data: -0.0094 Lr: 0.63799
2024-08-21 22:10:07.936 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][69/77] Data 0.028 (0.029) Batch 0.073 (0.071) Remain 00:00:28 loss: 0.2069 data: 0.0056 Lr: 0.63799
2024-08-21 22:10:08.008 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][70/77] Data 0.030 (0.029) Batch 0.072 (0.071) Remain 00:00:28 loss: 0.1681 data: 0.0183 Lr: 0.63636
2024-08-21 22:10:08.008 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][70/77] Data 0.029 (0.029) Batch 0.071 (0.071) Remain 00:00:28 loss: 0.1681 data: -0.0179 Lr: 0.63636
2024-08-21 22:10:08.074 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][71/77] Data 0.028 (0.029) Batch 0.067 (0.071) Remain 00:00:27 loss: 0.1130 data: 0.0079 Lr: 0.63474
2024-08-21 22:10:08.075 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][71/77] Data 0.028 (0.029) Batch 0.067 (0.071) Remain 00:00:27 loss: 0.1130 data: 0.0003 Lr: 0.63474
2024-08-21 22:10:08.141 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][72/77] Data 0.027 (0.029) Batch 0.066 (0.071) Remain 00:00:27 loss: 0.0997 data: -0.0032 Lr: 0.63312
2024-08-21 22:10:08.141 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][72/77] Data 0.027 (0.029) Batch 0.066 (0.071) Remain 00:00:27 loss: 0.0997 data: 0.0072 Lr: 0.63312
2024-08-21 22:10:08.206 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][73/77] Data 0.027 (0.029) Batch 0.065 (0.071) Remain 00:00:27 loss: 0.1011 data: -0.0001 Lr: 0.63149
2024-08-21 22:10:08.206 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][73/77] Data 0.027 (0.029) Batch 0.065 (0.071) Remain 00:00:27 loss: 0.1011 data: -0.0136 Lr: 0.63149
2024-08-21 22:10:08.294 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][74/77] Data 0.028 (0.029) Batch 0.086 (0.071) Remain 00:00:27 loss: 0.2075 data: 0.0015 Lr: 0.62987
2024-08-21 22:10:08.295 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][74/77] Data 0.028 (0.029) Batch 0.088 (0.071) Remain 00:00:27 loss: 0.2075 data: -0.0041 Lr: 0.62987
2024-08-21 22:10:08.371 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][75/77] Data 0.029 (0.029) Batch 0.078 (0.071) Remain 00:00:27 loss: 0.2411 data: 0.0104 Lr: 0.62825
2024-08-21 22:10:08.371 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][75/77] Data 0.029 (0.029) Batch 0.076 (0.071) Remain 00:00:27 loss: 0.2411 data: -0.0017 Lr: 0.62825
2024-08-21 22:10:08.441 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][76/77] Data 0.027 (0.029) Batch 0.071 (0.071) Remain 00:00:27 loss: 0.0762 data: -0.0117 Lr: 0.62662
2024-08-21 22:10:08.442 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][76/77] Data 0.027 (0.029) Batch 0.071 (0.071) Remain 00:00:27 loss: 0.0762 data: 0.0095 Lr: 0.62662
2024-08-21 22:10:08.491 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][77/77] Data 0.031 (0.029) Batch 0.049 (0.071) Remain 00:00:27 loss: 0.1908 data: -0.0003 Lr: 0.62500
2024-08-21 22:10:08.491 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 22:10:08.492 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][77/77] Data 0.036 (0.029) Batch 0.050 (0.071) Remain 00:00:27 loss: 0.1908 data: -0.0051 Lr: 0.62500
2024-08-21 22:10:08.492 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 22:10:12.905 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0426, Accuracy: 0.9857
2024-08-21 22:10:12.905 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0426, Accuracy: 0.9857
2024-08-21 22:10:12.905 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 22:10:12.905 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 22:10:12.905 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 22:10:12.905 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 22:10:13.016 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][1/77] Data 0.071 (0.071) Batch 0.110 (0.110) Remain 00:00:42 loss: 0.1194 data: -0.0045 Lr: 0.62338
2024-08-21 22:10:13.016 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][1/77] Data 0.045 (0.045) Batch 0.110 (0.110) Remain 00:00:42 loss: 0.1194 data: -0.0011 Lr: 0.62338
2024-08-21 22:10:13.078 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][2/77] Data 0.030 (0.030) Batch 0.062 (0.062) Remain 00:00:23 loss: 0.1810 data: 0.0096 Lr: 0.62175
2024-08-21 22:10:13.078 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][2/77] Data 0.022 (0.022) Batch 0.062 (0.062) Remain 00:00:23 loss: 0.1810 data: 0.0084 Lr: 0.62175
2024-08-21 22:10:13.138 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][3/77] Data 0.024 (0.027) Batch 0.060 (0.061) Remain 00:00:23 loss: 0.1426 data: -0.0070 Lr: 0.62013
2024-08-21 22:10:13.138 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][3/77] Data 0.022 (0.022) Batch 0.060 (0.061) Remain 00:00:23 loss: 0.1426 data: 0.0068 Lr: 0.62013
2024-08-21 22:10:13.207 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][4/77] Data 0.031 (0.028) Batch 0.069 (0.064) Remain 00:00:24 loss: 0.1050 data: 0.0159 Lr: 0.61851
2024-08-21 22:10:13.207 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][4/77] Data 0.022 (0.022) Batch 0.069 (0.064) Remain 00:00:24 loss: 0.1050 data: 0.0033 Lr: 0.61851
2024-08-21 22:10:13.274 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][5/77] Data 0.028 (0.028) Batch 0.067 (0.065) Remain 00:00:24 loss: 0.1430 data: -0.0087 Lr: 0.61688
2024-08-21 22:10:13.274 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][5/77] Data 0.022 (0.022) Batch 0.067 (0.065) Remain 00:00:24 loss: 0.1430 data: 0.0059 Lr: 0.61688
2024-08-21 22:10:13.340 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][6/77] Data 0.028 (0.028) Batch 0.066 (0.065) Remain 00:00:24 loss: 0.2123 data: 0.0066 Lr: 0.61526
2024-08-21 22:10:13.340 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][6/77] Data 0.022 (0.022) Batch 0.066 (0.065) Remain 00:00:24 loss: 0.2123 data: 0.0003 Lr: 0.61526
2024-08-21 22:10:13.406 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][7/77] Data 0.027 (0.028) Batch 0.066 (0.065) Remain 00:00:24 loss: 0.2287 data: 0.0009 Lr: 0.61364
2024-08-21 22:10:13.406 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][7/77] Data 0.021 (0.022) Batch 0.066 (0.065) Remain 00:00:24 loss: 0.2287 data: -0.0049 Lr: 0.61364
2024-08-21 22:10:13.470 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][8/77] Data 0.027 (0.028) Batch 0.064 (0.065) Remain 00:00:24 loss: 0.1689 data: 0.0145 Lr: 0.61201
2024-08-21 22:10:13.470 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][8/77] Data 0.023 (0.022) Batch 0.064 (0.065) Remain 00:00:24 loss: 0.1689 data: 0.0171 Lr: 0.61201
2024-08-21 22:10:13.534 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][9/77] Data 0.027 (0.028) Batch 0.064 (0.065) Remain 00:00:24 loss: 0.1394 data: -0.0157 Lr: 0.61039
2024-08-21 22:10:13.534 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][9/77] Data 0.024 (0.022) Batch 0.064 (0.065) Remain 00:00:24 loss: 0.1394 data: 0.0071 Lr: 0.61039
2024-08-21 22:10:13.631 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][10/77] Data 0.027 (0.028) Batch 0.097 (0.068) Remain 00:00:25 loss: 0.1747 data: -0.0009 Lr: 0.60877
2024-08-21 22:10:13.631 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][10/77] Data 0.023 (0.022) Batch 0.097 (0.068) Remain 00:00:25 loss: 0.1747 data: -0.0004 Lr: 0.60877
2024-08-21 22:10:13.705 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][11/77] Data 0.034 (0.028) Batch 0.074 (0.069) Remain 00:00:25 loss: 0.1520 data: 0.0062 Lr: 0.60714
2024-08-21 22:10:13.705 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][11/77] Data 0.035 (0.024) Batch 0.074 (0.069) Remain 00:00:25 loss: 0.1520 data: -0.0147 Lr: 0.60714
2024-08-21 22:10:13.772 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][12/77] Data 0.027 (0.028) Batch 0.067 (0.069) Remain 00:00:25 loss: 0.2289 data: 0.0201 Lr: 0.60552
2024-08-21 22:10:13.772 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][12/77] Data 0.028 (0.024) Batch 0.067 (0.069) Remain 00:00:25 loss: 0.2289 data: 0.0268 Lr: 0.60552
2024-08-21 22:10:13.839 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][13/77] Data 0.028 (0.028) Batch 0.067 (0.069) Remain 00:00:25 loss: 0.1609 data: 0.0123 Lr: 0.60390
2024-08-21 22:10:13.839 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][13/77] Data 0.027 (0.024) Batch 0.067 (0.069) Remain 00:00:25 loss: 0.1609 data: -0.0012 Lr: 0.60390
2024-08-21 22:10:13.905 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][14/77] Data 0.028 (0.028) Batch 0.066 (0.068) Remain 00:00:25 loss: 0.1138 data: -0.0102 Lr: 0.60227
2024-08-21 22:10:13.905 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][14/77] Data 0.028 (0.025) Batch 0.066 (0.068) Remain 00:00:25 loss: 0.1138 data: -0.0023 Lr: 0.60227
2024-08-21 22:10:13.971 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][15/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:25 loss: 0.1198 data: -0.0138 Lr: 0.60065
2024-08-21 22:10:13.971 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][15/77] Data 0.027 (0.025) Batch 0.066 (0.068) Remain 00:00:25 loss: 0.1198 data: -0.0008 Lr: 0.60065
2024-08-21 22:10:14.037 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][16/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:25 loss: 0.1127 data: 0.0008 Lr: 0.59903
2024-08-21 22:10:14.037 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][16/77] Data 0.027 (0.025) Batch 0.066 (0.068) Remain 00:00:25 loss: 0.1127 data: -0.0084 Lr: 0.59903
2024-08-21 22:10:14.103 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][17/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:25 loss: 0.1132 data: 0.0065 Lr: 0.59740
2024-08-21 22:10:14.103 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][17/77] Data 0.027 (0.025) Batch 0.066 (0.068) Remain 00:00:25 loss: 0.1132 data: 0.0021 Lr: 0.59740
2024-08-21 22:10:14.169 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][18/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:24 loss: 0.1562 data: 0.0004 Lr: 0.59578
2024-08-21 22:10:14.169 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][18/77] Data 0.027 (0.025) Batch 0.066 (0.068) Remain 00:00:24 loss: 0.1562 data: -0.0179 Lr: 0.59578
2024-08-21 22:10:14.238 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][19/77] Data 0.028 (0.028) Batch 0.069 (0.068) Remain 00:00:24 loss: 0.2066 data: -0.0039 Lr: 0.59416
2024-08-21 22:10:14.238 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_250
2024-08-21 22:10:14.238 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][19/77] Data 0.029 (0.025) Batch 0.069 (0.068) Remain 00:00:24 loss: 0.2066 data: 0.0078 Lr: 0.59416
2024-08-21 22:10:14.238 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_250
2024-08-21 22:10:14.265 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -495.68267822265625
2024-08-21 22:10:14.265 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -495.68267822265625
2024-08-21 22:10:14.265 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -239.9841766357422
2024-08-21 22:10:14.265 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -255.69851684570312
2024-08-21 22:10:14.342 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][20/77] Data 0.056 (0.029) Batch 0.105 (0.070) Remain 00:00:25 loss: 0.1373 data: -0.0017 Lr: 0.59253
2024-08-21 22:10:14.342 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][20/77] Data 0.055 (0.027) Batch 0.104 (0.070) Remain 00:00:25 loss: 0.1373 data: 0.0066 Lr: 0.59253
2024-08-21 22:10:14.418 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][21/77] Data 0.030 (0.029) Batch 0.076 (0.070) Remain 00:00:25 loss: 0.1312 data: -0.0026 Lr: 0.59091
2024-08-21 22:10:14.419 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][21/77] Data 0.028 (0.027) Batch 0.076 (0.070) Remain 00:00:25 loss: 0.1312 data: 0.0077 Lr: 0.59091
2024-08-21 22:10:14.491 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][22/77] Data 0.031 (0.030) Batch 0.072 (0.070) Remain 00:00:25 loss: 0.1652 data: -0.0002 Lr: 0.58929
2024-08-21 22:10:14.491 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][22/77] Data 0.028 (0.027) Batch 0.072 (0.070) Remain 00:00:25 loss: 0.1652 data: -0.0077 Lr: 0.58929
2024-08-21 22:10:14.567 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][23/77] Data 0.033 (0.030) Batch 0.076 (0.071) Remain 00:00:25 loss: 0.1202 data: -0.0122 Lr: 0.58766
2024-08-21 22:10:14.567 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][23/77] Data 0.027 (0.027) Batch 0.076 (0.071) Remain 00:00:25 loss: 0.1202 data: 0.0054 Lr: 0.58766
2024-08-21 22:10:14.641 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][24/77] Data 0.032 (0.030) Batch 0.074 (0.071) Remain 00:00:25 loss: 0.1661 data: 0.0237 Lr: 0.58604
2024-08-21 22:10:14.641 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][24/77] Data 0.034 (0.027) Batch 0.074 (0.071) Remain 00:00:25 loss: 0.1661 data: 0.0091 Lr: 0.58604
2024-08-21 22:10:14.719 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][25/77] Data 0.028 (0.030) Batch 0.078 (0.071) Remain 00:00:25 loss: 0.1448 data: -0.0124 Lr: 0.58442
2024-08-21 22:10:14.719 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][25/77] Data 0.032 (0.028) Batch 0.078 (0.071) Remain 00:00:25 loss: 0.1448 data: 0.0053 Lr: 0.58442
2024-08-21 22:10:14.786 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][26/77] Data 0.028 (0.030) Batch 0.067 (0.071) Remain 00:00:25 loss: 0.1037 data: 0.0091 Lr: 0.58279
2024-08-21 22:10:14.786 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][26/77] Data 0.028 (0.028) Batch 0.067 (0.071) Remain 00:00:25 loss: 0.1037 data: 0.0113 Lr: 0.58279
2024-08-21 22:10:14.851 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][27/77] Data 0.027 (0.030) Batch 0.066 (0.071) Remain 00:00:25 loss: 0.1205 data: -0.0085 Lr: 0.58117
2024-08-21 22:10:14.851 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][27/77] Data 0.028 (0.028) Batch 0.066 (0.071) Remain 00:00:25 loss: 0.1205 data: -0.0077 Lr: 0.58117
2024-08-21 22:10:14.918 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][28/77] Data 0.027 (0.029) Batch 0.067 (0.070) Remain 00:00:25 loss: 0.1910 data: -0.0011 Lr: 0.57955
2024-08-21 22:10:14.918 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][28/77] Data 0.028 (0.028) Batch 0.067 (0.070) Remain 00:00:25 loss: 0.1910 data: 0.0013 Lr: 0.57955
2024-08-21 22:10:14.995 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][29/77] Data 0.035 (0.030) Batch 0.077 (0.071) Remain 00:00:25 loss: 0.1772 data: 0.0135 Lr: 0.57792
2024-08-21 22:10:14.996 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][29/77] Data 0.028 (0.028) Batch 0.077 (0.071) Remain 00:00:25 loss: 0.1772 data: -0.0093 Lr: 0.57792
2024-08-21 22:10:15.061 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][30/77] Data 0.027 (0.030) Batch 0.066 (0.071) Remain 00:00:25 loss: 0.1598 data: 0.0096 Lr: 0.57630
2024-08-21 22:10:15.061 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][30/77] Data 0.027 (0.028) Batch 0.066 (0.071) Remain 00:00:25 loss: 0.1598 data: -0.0269 Lr: 0.57630
2024-08-21 22:10:15.127 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][31/77] Data 0.027 (0.029) Batch 0.066 (0.070) Remain 00:00:24 loss: 0.1546 data: 0.0054 Lr: 0.57468
2024-08-21 22:10:15.127 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][31/77] Data 0.027 (0.028) Batch 0.066 (0.070) Remain 00:00:24 loss: 0.1546 data: 0.0053 Lr: 0.57468
2024-08-21 22:10:15.192 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][32/77] Data 0.027 (0.029) Batch 0.065 (0.070) Remain 00:00:24 loss: 0.2125 data: -0.0337 Lr: 0.57305
2024-08-21 22:10:15.192 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][32/77] Data 0.028 (0.028) Batch 0.065 (0.070) Remain 00:00:24 loss: 0.2125 data: -0.0045 Lr: 0.57305
2024-08-21 22:10:15.262 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][33/77] Data 0.027 (0.029) Batch 0.070 (0.070) Remain 00:00:24 loss: 0.1514 data: -0.0012 Lr: 0.57143
2024-08-21 22:10:15.262 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][33/77] Data 0.027 (0.028) Batch 0.070 (0.070) Remain 00:00:24 loss: 0.1514 data: 0.0139 Lr: 0.57143
2024-08-21 22:10:15.342 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][34/77] Data 0.034 (0.029) Batch 0.080 (0.070) Remain 00:00:24 loss: 0.1582 data: 0.0190 Lr: 0.56981
2024-08-21 22:10:15.342 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][34/77] Data 0.028 (0.028) Batch 0.080 (0.070) Remain 00:00:24 loss: 0.1582 data: 0.0132 Lr: 0.56981
2024-08-21 22:10:15.416 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][35/77] Data 0.034 (0.030) Batch 0.074 (0.071) Remain 00:00:24 loss: 0.2014 data: -0.0044 Lr: 0.56818
2024-08-21 22:10:15.416 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][35/77] Data 0.027 (0.028) Batch 0.074 (0.071) Remain 00:00:24 loss: 0.2014 data: 0.0115 Lr: 0.56818
2024-08-21 22:10:15.484 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][36/77] Data 0.031 (0.030) Batch 0.069 (0.071) Remain 00:00:24 loss: 0.1279 data: -0.0074 Lr: 0.56656
2024-08-21 22:10:15.485 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][36/77] Data 0.027 (0.028) Batch 0.069 (0.071) Remain 00:00:24 loss: 0.1279 data: -0.0134 Lr: 0.56656
2024-08-21 22:10:15.558 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][37/77] Data 0.028 (0.030) Batch 0.073 (0.071) Remain 00:00:24 loss: 0.1749 data: -0.0128 Lr: 0.56494
2024-08-21 22:10:15.558 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][37/77] Data 0.027 (0.028) Batch 0.073 (0.071) Remain 00:00:24 loss: 0.1749 data: -0.0070 Lr: 0.56494
2024-08-21 22:10:15.624 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][38/77] Data 0.027 (0.030) Batch 0.067 (0.071) Remain 00:00:24 loss: 0.1884 data: 0.0128 Lr: 0.56331
2024-08-21 22:10:15.624 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][38/77] Data 0.028 (0.028) Batch 0.067 (0.071) Remain 00:00:24 loss: 0.1884 data: 0.0175 Lr: 0.56331
2024-08-21 22:10:15.695 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][39/77] Data 0.028 (0.030) Batch 0.071 (0.071) Remain 00:00:24 loss: 0.2354 data: 0.0293 Lr: 0.56169
2024-08-21 22:10:15.695 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][39/77] Data 0.028 (0.028) Batch 0.071 (0.071) Remain 00:00:24 loss: 0.2354 data: 0.0159 Lr: 0.56169
2024-08-21 22:10:15.761 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][40/77] Data 0.027 (0.029) Batch 0.067 (0.070) Remain 00:00:24 loss: 0.1226 data: -0.0067 Lr: 0.56006
2024-08-21 22:10:15.762 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][40/77] Data 0.028 (0.028) Batch 0.066 (0.070) Remain 00:00:24 loss: 0.1226 data: -0.0013 Lr: 0.56006
2024-08-21 22:10:15.830 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][41/77] Data 0.027 (0.029) Batch 0.069 (0.070) Remain 00:00:24 loss: 0.1597 data: -0.0067 Lr: 0.55844
2024-08-21 22:10:15.831 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][41/77] Data 0.027 (0.028) Batch 0.069 (0.070) Remain 00:00:24 loss: 0.1597 data: -0.0098 Lr: 0.55844
2024-08-21 22:10:15.900 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][42/77] Data 0.028 (0.029) Batch 0.070 (0.070) Remain 00:00:24 loss: 0.1840 data: 0.0099 Lr: 0.55682
2024-08-21 22:10:15.900 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][42/77] Data 0.023 (0.027) Batch 0.070 (0.070) Remain 00:00:24 loss: 0.1840 data: -0.0034 Lr: 0.55682
2024-08-21 22:10:15.965 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][43/77] Data 0.027 (0.029) Batch 0.065 (0.070) Remain 00:00:24 loss: 0.1220 data: -0.0086 Lr: 0.55519
2024-08-21 22:10:15.965 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][43/77] Data 0.027 (0.027) Batch 0.064 (0.070) Remain 00:00:24 loss: 0.1220 data: -0.0002 Lr: 0.55519
2024-08-21 22:10:16.031 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][44/77] Data 0.027 (0.029) Batch 0.066 (0.070) Remain 00:00:23 loss: 0.1684 data: 0.0017 Lr: 0.55357
2024-08-21 22:10:16.031 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][44/77] Data 0.022 (0.027) Batch 0.066 (0.070) Remain 00:00:23 loss: 0.1684 data: 0.0015 Lr: 0.55357
2024-08-21 22:10:16.095 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][45/77] Data 0.028 (0.029) Batch 0.064 (0.070) Remain 00:00:23 loss: 0.0876 data: -0.0113 Lr: 0.55195
2024-08-21 22:10:16.095 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][45/77] Data 0.023 (0.027) Batch 0.064 (0.070) Remain 00:00:23 loss: 0.0876 data: -0.0039 Lr: 0.55195
2024-08-21 22:10:16.162 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][46/77] Data 0.022 (0.027) Batch 0.067 (0.070) Remain 00:00:23 loss: 0.1402 data: -0.0039 Lr: 0.55032
2024-08-21 22:10:16.162 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][46/77] Data 0.027 (0.029) Batch 0.067 (0.070) Remain 00:00:23 loss: 0.1402 data: -0.0087 Lr: 0.55032
2024-08-21 22:10:16.228 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][47/77] Data 0.029 (0.029) Batch 0.066 (0.070) Remain 00:00:23 loss: 0.0923 data: 0.0070 Lr: 0.54870
2024-08-21 22:10:16.228 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][47/77] Data 0.023 (0.027) Batch 0.066 (0.070) Remain 00:00:23 loss: 0.0923 data: 0.0089 Lr: 0.54870
2024-08-21 22:10:16.293 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][48/77] Data 0.028 (0.029) Batch 0.065 (0.070) Remain 00:00:23 loss: 0.1501 data: -0.0072 Lr: 0.54708
2024-08-21 22:10:16.293 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][48/77] Data 0.022 (0.027) Batch 0.065 (0.070) Remain 00:00:23 loss: 0.1501 data: -0.0044 Lr: 0.54708
2024-08-21 22:10:16.358 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][49/77] Data 0.027 (0.029) Batch 0.065 (0.070) Remain 00:00:23 loss: 0.1401 data: 0.0042 Lr: 0.54545
2024-08-21 22:10:16.359 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][49/77] Data 0.022 (0.027) Batch 0.065 (0.070) Remain 00:00:23 loss: 0.1401 data: 0.0119 Lr: 0.54545
2024-08-21 22:10:16.424 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][50/77] Data 0.028 (0.029) Batch 0.067 (0.070) Remain 00:00:23 loss: 0.1192 data: -0.0272 Lr: 0.54383
2024-08-21 22:10:16.425 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][50/77] Data 0.029 (0.027) Batch 0.066 (0.070) Remain 00:00:23 loss: 0.1192 data: -0.0082 Lr: 0.54383
2024-08-21 22:10:16.491 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][51/77] Data 0.027 (0.029) Batch 0.067 (0.070) Remain 00:00:23 loss: 0.1244 data: -0.0063 Lr: 0.54221
2024-08-21 22:10:16.492 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][51/77] Data 0.027 (0.027) Batch 0.067 (0.070) Remain 00:00:23 loss: 0.1244 data: 0.0005 Lr: 0.54221
2024-08-21 22:10:16.557 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][52/77] Data 0.027 (0.029) Batch 0.066 (0.069) Remain 00:00:23 loss: 0.1308 data: -0.0056 Lr: 0.54058
2024-08-21 22:10:16.557 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][52/77] Data 0.027 (0.027) Batch 0.066 (0.069) Remain 00:00:23 loss: 0.1308 data: 0.0022 Lr: 0.54058
2024-08-21 22:10:16.624 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][53/77] Data 0.027 (0.029) Batch 0.067 (0.069) Remain 00:00:23 loss: 0.1907 data: 0.0066 Lr: 0.53896
2024-08-21 22:10:16.624 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][53/77] Data 0.028 (0.027) Batch 0.067 (0.069) Remain 00:00:23 loss: 0.1907 data: 0.0006 Lr: 0.53896
2024-08-21 22:10:16.691 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][54/77] Data 0.027 (0.029) Batch 0.067 (0.069) Remain 00:00:23 loss: 0.1711 data: 0.0115 Lr: 0.53734
2024-08-21 22:10:16.691 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][54/77] Data 0.028 (0.027) Batch 0.067 (0.069) Remain 00:00:23 loss: 0.1711 data: 0.0042 Lr: 0.53734
2024-08-21 22:10:16.760 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][55/77] Data 0.028 (0.029) Batch 0.069 (0.069) Remain 00:00:22 loss: 0.1076 data: 0.0112 Lr: 0.53571
2024-08-21 22:10:16.760 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][55/77] Data 0.028 (0.027) Batch 0.069 (0.069) Remain 00:00:22 loss: 0.1076 data: 0.0062 Lr: 0.53571
2024-08-21 22:10:16.826 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][56/77] Data 0.028 (0.029) Batch 0.067 (0.069) Remain 00:00:22 loss: 0.1857 data: 0.0039 Lr: 0.53409
2024-08-21 22:10:16.827 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][56/77] Data 0.028 (0.027) Batch 0.067 (0.069) Remain 00:00:22 loss: 0.1857 data: -0.0098 Lr: 0.53409
2024-08-21 22:10:16.894 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][57/77] Data 0.028 (0.029) Batch 0.067 (0.069) Remain 00:00:22 loss: 0.1281 data: -0.0106 Lr: 0.53247
2024-08-21 22:10:16.894 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][57/77] Data 0.028 (0.027) Batch 0.067 (0.069) Remain 00:00:22 loss: 0.1281 data: -0.0118 Lr: 0.53247
2024-08-21 22:10:16.964 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][58/77] Data 0.028 (0.029) Batch 0.070 (0.069) Remain 00:00:22 loss: 0.1576 data: -0.0061 Lr: 0.53084
2024-08-21 22:10:16.964 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][58/77] Data 0.028 (0.027) Batch 0.070 (0.069) Remain 00:00:22 loss: 0.1576 data: -0.0058 Lr: 0.53084
2024-08-21 22:10:17.041 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][59/77] Data 0.029 (0.029) Batch 0.077 (0.069) Remain 00:00:22 loss: 0.0999 data: 0.0112 Lr: 0.52922
2024-08-21 22:10:17.041 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][59/77] Data 0.029 (0.027) Batch 0.077 (0.069) Remain 00:00:22 loss: 0.0999 data: 0.0194 Lr: 0.52922
2024-08-21 22:10:17.121 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][60/77] Data 0.037 (0.029) Batch 0.080 (0.070) Remain 00:00:22 loss: 0.1803 data: -0.0083 Lr: 0.52760
2024-08-21 22:10:17.121 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][60/77] Data 0.029 (0.027) Batch 0.080 (0.070) Remain 00:00:22 loss: 0.1803 data: -0.0125 Lr: 0.52760
2024-08-21 22:10:17.194 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][61/77] Data 0.029 (0.029) Batch 0.073 (0.070) Remain 00:00:22 loss: 0.1132 data: 0.0105 Lr: 0.52597
2024-08-21 22:10:17.194 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][61/77] Data 0.030 (0.027) Batch 0.073 (0.070) Remain 00:00:22 loss: 0.1132 data: -0.0034 Lr: 0.52597
2024-08-21 22:10:17.265 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][62/77] Data 0.028 (0.029) Batch 0.072 (0.070) Remain 00:00:22 loss: 0.2522 data: 0.0028 Lr: 0.52435
2024-08-21 22:10:17.265 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][62/77] Data 0.029 (0.027) Batch 0.072 (0.070) Remain 00:00:22 loss: 0.2522 data: 0.0012 Lr: 0.52435
2024-08-21 22:10:17.334 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][63/77] Data 0.028 (0.029) Batch 0.068 (0.070) Remain 00:00:22 loss: 0.1907 data: 0.0114 Lr: 0.52273
2024-08-21 22:10:17.334 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][63/77] Data 0.028 (0.027) Batch 0.068 (0.070) Remain 00:00:22 loss: 0.1907 data: -0.0043 Lr: 0.52273
2024-08-21 22:10:17.399 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][64/77] Data 0.027 (0.029) Batch 0.065 (0.070) Remain 00:00:22 loss: 0.1749 data: -0.0030 Lr: 0.52110
2024-08-21 22:10:17.399 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][64/77] Data 0.028 (0.027) Batch 0.065 (0.070) Remain 00:00:22 loss: 0.1749 data: 0.0092 Lr: 0.52110
2024-08-21 22:10:17.463 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][65/77] Data 0.027 (0.029) Batch 0.064 (0.069) Remain 00:00:22 loss: 0.1132 data: 0.0200 Lr: 0.51948
2024-08-21 22:10:17.463 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][65/77] Data 0.027 (0.027) Batch 0.064 (0.069) Remain 00:00:22 loss: 0.1132 data: 0.0101 Lr: 0.51948
2024-08-21 22:10:17.527 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][66/77] Data 0.027 (0.029) Batch 0.064 (0.069) Remain 00:00:22 loss: 0.1887 data: -0.0093 Lr: 0.51786
2024-08-21 22:10:17.527 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][66/77] Data 0.027 (0.027) Batch 0.064 (0.069) Remain 00:00:22 loss: 0.1887 data: 0.0158 Lr: 0.51786
2024-08-21 22:10:17.591 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][67/77] Data 0.027 (0.029) Batch 0.064 (0.069) Remain 00:00:22 loss: 0.1493 data: -0.0030 Lr: 0.51623
2024-08-21 22:10:17.591 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][67/77] Data 0.027 (0.027) Batch 0.064 (0.069) Remain 00:00:22 loss: 0.1493 data: -0.0095 Lr: 0.51623
2024-08-21 22:10:17.656 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][68/77] Data 0.027 (0.029) Batch 0.065 (0.069) Remain 00:00:22 loss: 0.1096 data: -0.0148 Lr: 0.51461
2024-08-21 22:10:17.656 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][68/77] Data 0.027 (0.027) Batch 0.065 (0.069) Remain 00:00:22 loss: 0.1096 data: -0.0122 Lr: 0.51461
2024-08-21 22:10:17.720 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][69/77] Data 0.027 (0.029) Batch 0.064 (0.069) Remain 00:00:21 loss: 0.1319 data: -0.0094 Lr: 0.51299
2024-08-21 22:10:17.720 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_300
2024-08-21 22:10:17.720 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][69/77] Data 0.027 (0.027) Batch 0.064 (0.069) Remain 00:00:21 loss: 0.1319 data: -0.0098 Lr: 0.51299
2024-08-21 22:10:17.720 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_300
2024-08-21 22:10:17.742 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -519.4959716796875
2024-08-21 22:10:17.742 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -519.4959716796875
2024-08-21 22:10:17.742 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -245.52789306640625
2024-08-21 22:10:17.743 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -273.9681091308594
2024-08-21 22:10:17.807 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][70/77] Data 0.050 (0.029) Batch 0.087 (0.069) Remain 00:00:21 loss: 0.1060 data: -0.0026 Lr: 0.51136
2024-08-21 22:10:17.807 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][70/77] Data 0.050 (0.027) Batch 0.087 (0.069) Remain 00:00:21 loss: 0.1060 data: -0.0017 Lr: 0.51136
2024-08-21 22:10:17.872 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][71/77] Data 0.027 (0.029) Batch 0.065 (0.069) Remain 00:00:21 loss: 0.1030 data: 0.0111 Lr: 0.50974
2024-08-21 22:10:17.872 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][71/77] Data 0.027 (0.027) Batch 0.065 (0.069) Remain 00:00:21 loss: 0.1030 data: -0.0179 Lr: 0.50974
2024-08-21 22:10:17.940 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][72/77] Data 0.027 (0.029) Batch 0.069 (0.069) Remain 00:00:21 loss: 0.1598 data: 0.0086 Lr: 0.50812
2024-08-21 22:10:17.940 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][72/77] Data 0.028 (0.027) Batch 0.069 (0.069) Remain 00:00:21 loss: 0.1598 data: -0.0067 Lr: 0.50812
2024-08-21 22:10:18.008 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][73/77] Data 0.027 (0.029) Batch 0.068 (0.069) Remain 00:00:21 loss: 0.1040 data: 0.0017 Lr: 0.50649
2024-08-21 22:10:18.008 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][73/77] Data 0.028 (0.027) Batch 0.068 (0.069) Remain 00:00:21 loss: 0.1040 data: -0.0150 Lr: 0.50649
2024-08-21 22:10:18.076 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][74/77] Data 0.027 (0.029) Batch 0.068 (0.069) Remain 00:00:21 loss: 0.1445 data: -0.0004 Lr: 0.50487
2024-08-21 22:10:18.076 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][74/77] Data 0.028 (0.027) Batch 0.068 (0.069) Remain 00:00:21 loss: 0.1445 data: 0.0011 Lr: 0.50487
2024-08-21 22:10:18.142 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][75/77] Data 0.027 (0.029) Batch 0.065 (0.069) Remain 00:00:21 loss: 0.1172 data: -0.0026 Lr: 0.50325
2024-08-21 22:10:18.142 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][75/77] Data 0.027 (0.027) Batch 0.065 (0.069) Remain 00:00:21 loss: 0.1172 data: 0.0077 Lr: 0.50325
2024-08-21 22:10:18.210 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][76/77] Data 0.027 (0.029) Batch 0.069 (0.069) Remain 00:00:21 loss: 0.1216 data: -0.0095 Lr: 0.50162
2024-08-21 22:10:18.211 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][76/77] Data 0.029 (0.027) Batch 0.069 (0.069) Remain 00:00:21 loss: 0.1216 data: 0.0026 Lr: 0.50162
2024-08-21 22:10:18.251 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][77/77] Data 0.030 (0.029) Batch 0.041 (0.069) Remain 00:00:21 loss: 0.1432 data: -0.0129 Lr: 0.50000
2024-08-21 22:10:18.251 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 22:10:18.251 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][77/77] Data 0.031 (0.028) Batch 0.041 (0.069) Remain 00:00:21 loss: 0.1432 data: 0.0030 Lr: 0.50000
2024-08-21 22:10:18.252 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 22:10:22.882 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0381, Accuracy: 0.9864
2024-08-21 22:10:22.882 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 22:10:22.882 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0381, Accuracy: 0.9864
2024-08-21 22:10:22.883 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 22:10:22.883 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 22:10:22.883 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 22:10:22.974 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][1/77] Data 0.043 (0.043) Batch 0.092 (0.092) Remain 00:00:28 loss: 0.0787 data: -0.0047 Lr: 0.49838
2024-08-21 22:10:22.975 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][1/77] Data 0.055 (0.055) Batch 0.092 (0.092) Remain 00:00:28 loss: 0.0787 data: 0.0073 Lr: 0.49838
2024-08-21 22:10:23.038 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][2/77] Data 0.022 (0.022) Batch 0.064 (0.064) Remain 00:00:19 loss: 0.1132 data: -0.0086 Lr: 0.49675
2024-08-21 22:10:23.038 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][2/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:19 loss: 0.1132 data: -0.0142 Lr: 0.49675
2024-08-21 22:10:23.102 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][3/77] Data 0.022 (0.022) Batch 0.064 (0.064) Remain 00:00:19 loss: 0.1303 data: 0.0005 Lr: 0.49513
2024-08-21 22:10:23.102 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][3/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:19 loss: 0.1303 data: -0.0071 Lr: 0.49513
2024-08-21 22:10:23.166 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][4/77] Data 0.021 (0.022) Batch 0.064 (0.064) Remain 00:00:19 loss: 0.1276 data: -0.0244 Lr: 0.49351
2024-08-21 22:10:23.166 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][4/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:19 loss: 0.1276 data: -0.0053 Lr: 0.49351
2024-08-21 22:10:23.230 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][5/77] Data 0.021 (0.022) Batch 0.064 (0.064) Remain 00:00:19 loss: 0.1238 data: 0.0004 Lr: 0.49188
2024-08-21 22:10:23.230 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][5/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:19 loss: 0.1238 data: 0.0133 Lr: 0.49188
2024-08-21 22:10:23.294 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][6/77] Data 0.021 (0.021) Batch 0.064 (0.064) Remain 00:00:19 loss: 0.1084 data: 0.0166 Lr: 0.49026
2024-08-21 22:10:23.294 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][6/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:19 loss: 0.1084 data: 0.0016 Lr: 0.49026
2024-08-21 22:10:23.358 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][7/77] Data 0.021 (0.021) Batch 0.064 (0.064) Remain 00:00:19 loss: 0.1257 data: -0.0050 Lr: 0.48864
2024-08-21 22:10:23.358 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][7/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:19 loss: 0.1257 data: -0.0014 Lr: 0.48864
2024-08-21 22:10:23.422 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][8/77] Data 0.021 (0.021) Batch 0.064 (0.064) Remain 00:00:19 loss: 0.1516 data: -0.0056 Lr: 0.48701
2024-08-21 22:10:23.422 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][8/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:19 loss: 0.1516 data: -0.0114 Lr: 0.48701
2024-08-21 22:10:23.486 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][9/77] Data 0.021 (0.021) Batch 0.064 (0.064) Remain 00:00:19 loss: 0.1032 data: 0.0029 Lr: 0.48539
2024-08-21 22:10:23.486 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][9/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:19 loss: 0.1032 data: 0.0002 Lr: 0.48539
2024-08-21 22:10:23.550 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][10/77] Data 0.023 (0.021) Batch 0.064 (0.064) Remain 00:00:19 loss: 0.0612 data: 0.0060 Lr: 0.48377
2024-08-21 22:10:23.550 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][10/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:19 loss: 0.0612 data: -0.0066 Lr: 0.48377
2024-08-21 22:10:23.613 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][11/77] Data 0.021 (0.021) Batch 0.064 (0.064) Remain 00:00:19 loss: 0.1639 data: 0.0084 Lr: 0.48214
2024-08-21 22:10:23.614 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][11/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:19 loss: 0.1639 data: 0.0268 Lr: 0.48214
2024-08-21 22:10:23.677 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][12/77] Data 0.021 (0.021) Batch 0.064 (0.064) Remain 00:00:18 loss: 0.0554 data: 0.0143 Lr: 0.48052
2024-08-21 22:10:23.678 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][12/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:18 loss: 0.0554 data: -0.0109 Lr: 0.48052
2024-08-21 22:10:23.740 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][13/77] Data 0.021 (0.021) Batch 0.062 (0.064) Remain 00:00:18 loss: 0.1502 data: 0.0112 Lr: 0.47890
2024-08-21 22:10:23.740 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][13/77] Data 0.027 (0.027) Batch 0.062 (0.064) Remain 00:00:18 loss: 0.1502 data: -0.0053 Lr: 0.47890
2024-08-21 22:10:23.804 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][14/77] Data 0.022 (0.021) Batch 0.065 (0.064) Remain 00:00:18 loss: 0.1621 data: 0.0104 Lr: 0.47727
2024-08-21 22:10:23.804 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][14/77] Data 0.027 (0.027) Batch 0.065 (0.064) Remain 00:00:18 loss: 0.1621 data: -0.0045 Lr: 0.47727
2024-08-21 22:10:23.867 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][15/77] Data 0.021 (0.021) Batch 0.063 (0.064) Remain 00:00:18 loss: 0.0993 data: -0.0038 Lr: 0.47565
2024-08-21 22:10:23.868 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][15/77] Data 0.027 (0.027) Batch 0.063 (0.064) Remain 00:00:18 loss: 0.0993 data: -0.0167 Lr: 0.47565
2024-08-21 22:10:23.930 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][16/77] Data 0.022 (0.021) Batch 0.063 (0.064) Remain 00:00:18 loss: 0.1453 data: -0.0055 Lr: 0.47403
2024-08-21 22:10:23.930 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][16/77] Data 0.027 (0.027) Batch 0.063 (0.064) Remain 00:00:18 loss: 0.1453 data: 0.0192 Lr: 0.47403
2024-08-21 22:10:23.995 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][17/77] Data 0.022 (0.021) Batch 0.065 (0.064) Remain 00:00:18 loss: 0.0935 data: -0.0110 Lr: 0.47240
2024-08-21 22:10:23.995 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][17/77] Data 0.027 (0.027) Batch 0.065 (0.064) Remain 00:00:18 loss: 0.0935 data: -0.0025 Lr: 0.47240
2024-08-21 22:10:24.059 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][18/77] Data 0.022 (0.021) Batch 0.064 (0.064) Remain 00:00:18 loss: 0.1338 data: 0.0192 Lr: 0.47078
2024-08-21 22:10:24.059 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][18/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:18 loss: 0.1338 data: 0.0059 Lr: 0.47078
2024-08-21 22:10:24.123 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][19/77] Data 0.022 (0.021) Batch 0.064 (0.064) Remain 00:00:18 loss: 0.0726 data: 0.0032 Lr: 0.46916
2024-08-21 22:10:24.123 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][19/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:18 loss: 0.0726 data: 0.0162 Lr: 0.46916
2024-08-21 22:10:24.187 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][20/77] Data 0.022 (0.022) Batch 0.064 (0.064) Remain 00:00:18 loss: 0.0975 data: 0.0024 Lr: 0.46753
2024-08-21 22:10:24.187 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][20/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:18 loss: 0.0975 data: 0.0002 Lr: 0.46753
2024-08-21 22:10:24.251 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][21/77] Data 0.022 (0.022) Batch 0.064 (0.064) Remain 00:00:18 loss: 0.1161 data: -0.0058 Lr: 0.46591
2024-08-21 22:10:24.251 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][21/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:18 loss: 0.1161 data: -0.0005 Lr: 0.46591
2024-08-21 22:10:24.315 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][22/77] Data 0.022 (0.022) Batch 0.064 (0.064) Remain 00:00:18 loss: 0.1098 data: 0.0166 Lr: 0.46429
2024-08-21 22:10:24.315 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][22/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:18 loss: 0.1098 data: -0.0062 Lr: 0.46429
2024-08-21 22:10:24.379 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][23/77] Data 0.022 (0.022) Batch 0.064 (0.064) Remain 00:00:18 loss: 0.0882 data: 0.0021 Lr: 0.46266
2024-08-21 22:10:24.379 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][23/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:18 loss: 0.0882 data: -0.0117 Lr: 0.46266
2024-08-21 22:10:24.443 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][24/77] Data 0.022 (0.022) Batch 0.064 (0.064) Remain 00:00:18 loss: 0.0894 data: -0.0047 Lr: 0.46104
2024-08-21 22:10:24.443 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][24/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:18 loss: 0.0894 data: -0.0134 Lr: 0.46104
2024-08-21 22:10:24.507 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][25/77] Data 0.022 (0.022) Batch 0.064 (0.064) Remain 00:00:18 loss: 0.1411 data: -0.0039 Lr: 0.45942
2024-08-21 22:10:24.507 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][25/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:18 loss: 0.1411 data: -0.0034 Lr: 0.45942
2024-08-21 22:10:24.571 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][26/77] Data 0.022 (0.022) Batch 0.064 (0.064) Remain 00:00:18 loss: 0.0635 data: 0.0041 Lr: 0.45779
2024-08-21 22:10:24.571 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][26/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:18 loss: 0.0635 data: -0.0055 Lr: 0.45779
2024-08-21 22:10:24.635 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][27/77] Data 0.022 (0.022) Batch 0.064 (0.064) Remain 00:00:18 loss: 0.1136 data: -0.0054 Lr: 0.45617
2024-08-21 22:10:24.635 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][27/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:18 loss: 0.1136 data: -0.0092 Lr: 0.45617
2024-08-21 22:10:24.699 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][28/77] Data 0.022 (0.022) Batch 0.064 (0.064) Remain 00:00:17 loss: 0.1507 data: 0.0072 Lr: 0.45455
2024-08-21 22:10:24.699 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][28/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:17 loss: 0.1507 data: -0.0027 Lr: 0.45455
2024-08-21 22:10:24.763 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][29/77] Data 0.022 (0.022) Batch 0.064 (0.064) Remain 00:00:17 loss: 0.1031 data: -0.0074 Lr: 0.45292
2024-08-21 22:10:24.763 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][29/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:17 loss: 0.1031 data: 0.0011 Lr: 0.45292
2024-08-21 22:10:24.827 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][30/77] Data 0.022 (0.022) Batch 0.064 (0.064) Remain 00:00:17 loss: 0.1499 data: 0.0148 Lr: 0.45130
2024-08-21 22:10:24.827 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][30/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:17 loss: 0.1499 data: -0.0176 Lr: 0.45130
2024-08-21 22:10:24.891 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][31/77] Data 0.022 (0.022) Batch 0.064 (0.064) Remain 00:00:17 loss: 0.0419 data: -0.0003 Lr: 0.44968
2024-08-21 22:10:24.891 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][31/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:17 loss: 0.0419 data: 0.0021 Lr: 0.44968
2024-08-21 22:10:24.953 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][32/77] Data 0.022 (0.022) Batch 0.062 (0.064) Remain 00:00:17 loss: 0.0714 data: -0.0023 Lr: 0.44805
2024-08-21 22:10:24.953 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][32/77] Data 0.027 (0.027) Batch 0.062 (0.064) Remain 00:00:17 loss: 0.0714 data: -0.0045 Lr: 0.44805
2024-08-21 22:10:25.016 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][33/77] Data 0.022 (0.022) Batch 0.063 (0.064) Remain 00:00:17 loss: 0.1179 data: -0.0035 Lr: 0.44643
2024-08-21 22:10:25.016 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][33/77] Data 0.027 (0.027) Batch 0.063 (0.064) Remain 00:00:17 loss: 0.1179 data: -0.0185 Lr: 0.44643
2024-08-21 22:10:25.085 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][34/77] Data 0.022 (0.022) Batch 0.069 (0.064) Remain 00:00:17 loss: 0.0608 data: -0.0073 Lr: 0.44481
2024-08-21 22:10:25.085 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][34/77] Data 0.027 (0.027) Batch 0.069 (0.064) Remain 00:00:17 loss: 0.0608 data: 0.0006 Lr: 0.44481
2024-08-21 22:10:25.151 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][35/77] Data 0.022 (0.022) Batch 0.066 (0.064) Remain 00:00:17 loss: 0.0830 data: 0.0062 Lr: 0.44318
2024-08-21 22:10:25.151 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][35/77] Data 0.029 (0.027) Batch 0.066 (0.064) Remain 00:00:17 loss: 0.0830 data: -0.0006 Lr: 0.44318
2024-08-21 22:10:25.217 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][36/77] Data 0.021 (0.022) Batch 0.066 (0.064) Remain 00:00:17 loss: 0.0629 data: -0.0185 Lr: 0.44156
2024-08-21 22:10:25.217 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][36/77] Data 0.028 (0.027) Batch 0.066 (0.064) Remain 00:00:17 loss: 0.0629 data: 0.0126 Lr: 0.44156
2024-08-21 22:10:25.282 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][37/77] Data 0.021 (0.022) Batch 0.065 (0.064) Remain 00:00:17 loss: 0.0854 data: 0.0076 Lr: 0.43994
2024-08-21 22:10:25.282 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][37/77] Data 0.028 (0.027) Batch 0.065 (0.064) Remain 00:00:17 loss: 0.0854 data: 0.0157 Lr: 0.43994
2024-08-21 22:10:25.347 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][38/77] Data 0.021 (0.022) Batch 0.065 (0.064) Remain 00:00:17 loss: 0.1393 data: 0.0169 Lr: 0.43831
2024-08-21 22:10:25.347 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][38/77] Data 0.027 (0.027) Batch 0.065 (0.064) Remain 00:00:17 loss: 0.1393 data: -0.0074 Lr: 0.43831
2024-08-21 22:10:25.412 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][39/77] Data 0.021 (0.022) Batch 0.065 (0.064) Remain 00:00:17 loss: 0.1369 data: 0.0041 Lr: 0.43669
2024-08-21 22:10:25.412 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][39/77] Data 0.027 (0.027) Batch 0.065 (0.064) Remain 00:00:17 loss: 0.1369 data: -0.0038 Lr: 0.43669
2024-08-21 22:10:25.477 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][40/77] Data 0.021 (0.022) Batch 0.065 (0.064) Remain 00:00:17 loss: 0.0674 data: -0.0162 Lr: 0.43506
2024-08-21 22:10:25.477 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][40/77] Data 0.027 (0.027) Batch 0.065 (0.064) Remain 00:00:17 loss: 0.0674 data: 0.0020 Lr: 0.43506
2024-08-21 22:10:25.542 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][41/77] Data 0.021 (0.022) Batch 0.065 (0.064) Remain 00:00:17 loss: 0.2266 data: -0.0118 Lr: 0.43344
2024-08-21 22:10:25.542 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][41/77] Data 0.027 (0.027) Batch 0.065 (0.064) Remain 00:00:17 loss: 0.2266 data: -0.0029 Lr: 0.43344
2024-08-21 22:10:25.604 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][42/77] Data 0.021 (0.022) Batch 0.062 (0.064) Remain 00:00:17 loss: 0.1080 data: 0.0077 Lr: 0.43182
2024-08-21 22:10:25.604 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_350
2024-08-21 22:10:25.604 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][42/77] Data 0.027 (0.027) Batch 0.062 (0.064) Remain 00:00:17 loss: 0.1080 data: -0.0087 Lr: 0.43182
2024-08-21 22:10:25.604 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_350
2024-08-21 22:10:25.635 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -525.4183349609375
2024-08-21 22:10:25.635 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -247.1734619140625
2024-08-21 22:10:25.635 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -525.4183349609375
2024-08-21 22:10:25.635 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -278.244873046875
2024-08-21 22:10:25.701 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][43/77] Data 0.052 (0.022) Batch 0.097 (0.065) Remain 00:00:17 loss: 0.1042 data: 0.0034 Lr: 0.43019
2024-08-21 22:10:25.701 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][43/77] Data 0.059 (0.028) Batch 0.097 (0.065) Remain 00:00:17 loss: 0.1042 data: -0.0074 Lr: 0.43019
2024-08-21 22:10:25.765 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][44/77] Data 0.023 (0.022) Batch 0.065 (0.065) Remain 00:00:17 loss: 0.0696 data: -0.0086 Lr: 0.42857
2024-08-21 22:10:25.766 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][44/77] Data 0.027 (0.028) Batch 0.065 (0.065) Remain 00:00:17 loss: 0.0696 data: -0.0160 Lr: 0.42857
2024-08-21 22:10:25.829 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][45/77] Data 0.021 (0.022) Batch 0.064 (0.065) Remain 00:00:17 loss: 0.1255 data: 0.0038 Lr: 0.42695
2024-08-21 22:10:25.830 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][45/77] Data 0.027 (0.028) Batch 0.064 (0.065) Remain 00:00:17 loss: 0.1255 data: -0.0082 Lr: 0.42695
2024-08-21 22:10:25.894 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][46/77] Data 0.021 (0.022) Batch 0.064 (0.065) Remain 00:00:17 loss: 0.1015 data: -0.0025 Lr: 0.42532
2024-08-21 22:10:25.894 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][46/77] Data 0.027 (0.028) Batch 0.064 (0.065) Remain 00:00:17 loss: 0.1015 data: -0.0056 Lr: 0.42532
2024-08-21 22:10:25.959 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][47/77] Data 0.020 (0.022) Batch 0.066 (0.065) Remain 00:00:16 loss: 0.1424 data: 0.0173 Lr: 0.42370
2024-08-21 22:10:25.959 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][47/77] Data 0.027 (0.028) Batch 0.066 (0.065) Remain 00:00:16 loss: 0.1424 data: 0.0061 Lr: 0.42370
2024-08-21 22:10:26.023 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][48/77] Data 0.020 (0.022) Batch 0.064 (0.065) Remain 00:00:16 loss: 0.1465 data: -0.0020 Lr: 0.42208
2024-08-21 22:10:26.024 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][48/77] Data 0.027 (0.028) Batch 0.064 (0.065) Remain 00:00:16 loss: 0.1465 data: 0.0014 Lr: 0.42208
2024-08-21 22:10:26.088 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][49/77] Data 0.020 (0.022) Batch 0.065 (0.065) Remain 00:00:16 loss: 0.1066 data: -0.0034 Lr: 0.42045
2024-08-21 22:10:26.088 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][49/77] Data 0.027 (0.028) Batch 0.065 (0.065) Remain 00:00:16 loss: 0.1066 data: -0.0062 Lr: 0.42045
2024-08-21 22:10:26.153 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][50/77] Data 0.020 (0.022) Batch 0.065 (0.065) Remain 00:00:16 loss: 0.1610 data: -0.0050 Lr: 0.41883
2024-08-21 22:10:26.153 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][50/77] Data 0.027 (0.028) Batch 0.065 (0.065) Remain 00:00:16 loss: 0.1610 data: -0.0072 Lr: 0.41883
2024-08-21 22:10:26.219 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][51/77] Data 0.021 (0.022) Batch 0.066 (0.065) Remain 00:00:16 loss: 0.1478 data: -0.0151 Lr: 0.41721
2024-08-21 22:10:26.219 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][51/77] Data 0.027 (0.028) Batch 0.066 (0.065) Remain 00:00:16 loss: 0.1478 data: -0.0041 Lr: 0.41721
2024-08-21 22:10:26.283 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][52/77] Data 0.021 (0.022) Batch 0.065 (0.065) Remain 00:00:16 loss: 0.1175 data: 0.0062 Lr: 0.41558
2024-08-21 22:10:26.283 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][52/77] Data 0.028 (0.028) Batch 0.065 (0.065) Remain 00:00:16 loss: 0.1175 data: -0.0049 Lr: 0.41558
2024-08-21 22:10:26.357 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][53/77] Data 0.030 (0.022) Batch 0.074 (0.065) Remain 00:00:16 loss: 0.2010 data: -0.0032 Lr: 0.41396
2024-08-21 22:10:26.358 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][53/77] Data 0.027 (0.028) Batch 0.074 (0.065) Remain 00:00:16 loss: 0.2010 data: 0.0042 Lr: 0.41396
2024-08-21 22:10:26.431 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][54/77] Data 0.028 (0.028) Batch 0.074 (0.065) Remain 00:00:16 loss: 0.0554 data: 0.0007 Lr: 0.41234
2024-08-21 22:10:26.432 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][54/77] Data 0.029 (0.022) Batch 0.074 (0.065) Remain 00:00:16 loss: 0.0554 data: 0.0097 Lr: 0.41234
2024-08-21 22:10:26.495 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][55/77] Data 0.022 (0.022) Batch 0.063 (0.065) Remain 00:00:16 loss: 0.0611 data: -0.0165 Lr: 0.41071
2024-08-21 22:10:26.495 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][55/77] Data 0.027 (0.028) Batch 0.063 (0.065) Remain 00:00:16 loss: 0.0611 data: -0.0001 Lr: 0.41071
2024-08-21 22:10:26.563 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][56/77] Data 0.029 (0.022) Batch 0.068 (0.065) Remain 00:00:16 loss: 0.0610 data: -0.0279 Lr: 0.40909
2024-08-21 22:10:26.563 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][56/77] Data 0.027 (0.028) Batch 0.068 (0.065) Remain 00:00:16 loss: 0.0610 data: 0.0034 Lr: 0.40909
2024-08-21 22:10:26.634 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][57/77] Data 0.028 (0.023) Batch 0.071 (0.065) Remain 00:00:16 loss: 0.0823 data: -0.0152 Lr: 0.40747
2024-08-21 22:10:26.634 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][57/77] Data 0.029 (0.028) Batch 0.070 (0.065) Remain 00:00:16 loss: 0.0823 data: -0.0163 Lr: 0.40747
2024-08-21 22:10:26.709 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][58/77] Data 0.028 (0.023) Batch 0.075 (0.066) Remain 00:00:16 loss: 0.1400 data: -0.0062 Lr: 0.40584
2024-08-21 22:10:26.709 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][58/77] Data 0.029 (0.028) Batch 0.075 (0.066) Remain 00:00:16 loss: 0.1400 data: 0.0013 Lr: 0.40584
2024-08-21 22:10:26.782 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][59/77] Data 0.032 (0.023) Batch 0.073 (0.066) Remain 00:00:16 loss: 0.1339 data: -0.0070 Lr: 0.40422
2024-08-21 22:10:26.782 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][59/77] Data 0.029 (0.028) Batch 0.073 (0.066) Remain 00:00:16 loss: 0.1339 data: 0.0049 Lr: 0.40422
2024-08-21 22:10:26.853 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][60/77] Data 0.028 (0.023) Batch 0.071 (0.066) Remain 00:00:16 loss: 0.1677 data: -0.0038 Lr: 0.40260
2024-08-21 22:10:26.853 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][60/77] Data 0.029 (0.028) Batch 0.071 (0.066) Remain 00:00:16 loss: 0.1677 data: 0.0050 Lr: 0.40260
2024-08-21 22:10:26.924 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][61/77] Data 0.028 (0.023) Batch 0.071 (0.066) Remain 00:00:16 loss: 0.1584 data: 0.0051 Lr: 0.40097
2024-08-21 22:10:26.924 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][61/77] Data 0.029 (0.028) Batch 0.071 (0.066) Remain 00:00:16 loss: 0.1584 data: 0.0004 Lr: 0.40097
2024-08-21 22:10:26.995 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][62/77] Data 0.028 (0.023) Batch 0.071 (0.066) Remain 00:00:16 loss: 0.0704 data: -0.0009 Lr: 0.39935
2024-08-21 22:10:26.995 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][62/77] Data 0.029 (0.028) Batch 0.071 (0.066) Remain 00:00:16 loss: 0.0704 data: 0.0161 Lr: 0.39935
2024-08-21 22:10:27.060 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][63/77] Data 0.028 (0.023) Batch 0.065 (0.066) Remain 00:00:16 loss: 0.0525 data: -0.0041 Lr: 0.39773
2024-08-21 22:10:27.061 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][63/77] Data 0.027 (0.028) Batch 0.065 (0.066) Remain 00:00:16 loss: 0.0525 data: 0.0031 Lr: 0.39773
2024-08-21 22:10:27.125 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][64/77] Data 0.027 (0.023) Batch 0.065 (0.066) Remain 00:00:16 loss: 0.0599 data: -0.0126 Lr: 0.39610
2024-08-21 22:10:27.126 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][64/77] Data 0.027 (0.028) Batch 0.065 (0.066) Remain 00:00:16 loss: 0.0599 data: 0.0043 Lr: 0.39610
2024-08-21 22:10:27.191 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][65/77] Data 0.027 (0.023) Batch 0.065 (0.066) Remain 00:00:16 loss: 0.0668 data: -0.0044 Lr: 0.39448
2024-08-21 22:10:27.191 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][65/77] Data 0.027 (0.028) Batch 0.065 (0.066) Remain 00:00:16 loss: 0.0668 data: 0.0093 Lr: 0.39448
2024-08-21 22:10:27.257 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][66/77] Data 0.027 (0.023) Batch 0.066 (0.066) Remain 00:00:16 loss: 0.0639 data: -0.0004 Lr: 0.39286
2024-08-21 22:10:27.257 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][66/77] Data 0.027 (0.028) Batch 0.066 (0.066) Remain 00:00:16 loss: 0.0639 data: -0.0055 Lr: 0.39286
2024-08-21 22:10:27.322 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][67/77] Data 0.027 (0.023) Batch 0.065 (0.066) Remain 00:00:15 loss: 0.0848 data: -0.0107 Lr: 0.39123
2024-08-21 22:10:27.322 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][67/77] Data 0.027 (0.028) Batch 0.065 (0.066) Remain 00:00:15 loss: 0.0848 data: -0.0045 Lr: 0.39123
2024-08-21 22:10:27.387 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][68/77] Data 0.027 (0.023) Batch 0.065 (0.066) Remain 00:00:15 loss: 0.1362 data: 0.0190 Lr: 0.38961
2024-08-21 22:10:27.387 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][68/77] Data 0.027 (0.028) Batch 0.065 (0.066) Remain 00:00:15 loss: 0.1362 data: -0.0070 Lr: 0.38961
2024-08-21 22:10:27.452 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][69/77] Data 0.027 (0.023) Batch 0.065 (0.066) Remain 00:00:15 loss: 0.2219 data: 0.0135 Lr: 0.38799
2024-08-21 22:10:27.452 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][69/77] Data 0.027 (0.028) Batch 0.065 (0.066) Remain 00:00:15 loss: 0.2219 data: -0.0104 Lr: 0.38799
2024-08-21 22:10:27.517 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][70/77] Data 0.027 (0.024) Batch 0.065 (0.066) Remain 00:00:15 loss: 0.1745 data: 0.0258 Lr: 0.38636
2024-08-21 22:10:27.517 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][70/77] Data 0.027 (0.028) Batch 0.065 (0.066) Remain 00:00:15 loss: 0.1745 data: -0.0014 Lr: 0.38636
2024-08-21 22:10:27.582 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][71/77] Data 0.027 (0.024) Batch 0.065 (0.066) Remain 00:00:15 loss: 0.0691 data: 0.0188 Lr: 0.38474
2024-08-21 22:10:27.582 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][71/77] Data 0.027 (0.028) Batch 0.065 (0.066) Remain 00:00:15 loss: 0.0691 data: -0.0099 Lr: 0.38474
2024-08-21 22:10:27.647 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][72/77] Data 0.027 (0.024) Batch 0.065 (0.066) Remain 00:00:15 loss: 0.0949 data: -0.0225 Lr: 0.38312
2024-08-21 22:10:27.647 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][72/77] Data 0.027 (0.028) Batch 0.065 (0.066) Remain 00:00:15 loss: 0.0949 data: 0.0022 Lr: 0.38312
2024-08-21 22:10:27.711 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][73/77] Data 0.027 (0.024) Batch 0.065 (0.066) Remain 00:00:15 loss: 0.1221 data: 0.0146 Lr: 0.38149
2024-08-21 22:10:27.711 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][73/77] Data 0.027 (0.028) Batch 0.065 (0.066) Remain 00:00:15 loss: 0.1221 data: 0.0028 Lr: 0.38149
2024-08-21 22:10:27.776 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][74/77] Data 0.027 (0.024) Batch 0.065 (0.066) Remain 00:00:15 loss: 0.1285 data: 0.0089 Lr: 0.37987
2024-08-21 22:10:27.777 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][74/77] Data 0.027 (0.028) Batch 0.065 (0.066) Remain 00:00:15 loss: 0.1285 data: 0.0203 Lr: 0.37987
2024-08-21 22:10:27.841 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][75/77] Data 0.027 (0.024) Batch 0.065 (0.066) Remain 00:00:15 loss: 0.0676 data: 0.0108 Lr: 0.37825
2024-08-21 22:10:27.841 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][75/77] Data 0.027 (0.028) Batch 0.065 (0.066) Remain 00:00:15 loss: 0.0676 data: 0.0153 Lr: 0.37825
2024-08-21 22:10:27.906 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][76/77] Data 0.027 (0.024) Batch 0.065 (0.066) Remain 00:00:15 loss: 0.1014 data: -0.0068 Lr: 0.37662
2024-08-21 22:10:27.906 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][76/77] Data 0.027 (0.028) Batch 0.065 (0.066) Remain 00:00:15 loss: 0.1014 data: -0.0105 Lr: 0.37662
2024-08-21 22:10:27.948 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][77/77] Data 0.029 (0.024) Batch 0.041 (0.065) Remain 00:00:15 loss: 0.0766 data: 0.0080 Lr: 0.37500
2024-08-21 22:10:27.948 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 22:10:27.948 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][77/77] Data 0.030 (0.028) Batch 0.041 (0.065) Remain 00:00:15 loss: 0.0766 data: 0.0143 Lr: 0.37500
2024-08-21 22:10:27.948 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 22:10:32.190 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0352, Accuracy: 0.9882
2024-08-21 22:10:32.190 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 22:10:32.190 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 22:10:32.190 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0352, Accuracy: 0.9882
2024-08-21 22:10:32.190 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 22:10:32.191 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 22:10:32.273 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][1/77] Data 0.044 (0.044) Batch 0.082 (0.082) Remain 00:00:18 loss: 0.1344 data: 0.0083 Lr: 0.37338
2024-08-21 22:10:32.273 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][1/77] Data 0.050 (0.050) Batch 0.082 (0.082) Remain 00:00:18 loss: 0.1344 data: 0.0052 Lr: 0.37338
2024-08-21 22:10:32.337 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][2/77] Data 0.022 (0.022) Batch 0.064 (0.064) Remain 00:00:14 loss: 0.0786 data: 0.0093 Lr: 0.37175
2024-08-21 22:10:32.337 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][2/77] Data 0.025 (0.025) Batch 0.064 (0.064) Remain 00:00:14 loss: 0.0786 data: 0.0029 Lr: 0.37175
2024-08-21 22:10:32.402 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][3/77] Data 0.022 (0.022) Batch 0.065 (0.065) Remain 00:00:14 loss: 0.1125 data: -0.0161 Lr: 0.37013
2024-08-21 22:10:32.402 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][3/77] Data 0.022 (0.024) Batch 0.065 (0.065) Remain 00:00:14 loss: 0.1125 data: -0.0005 Lr: 0.37013
2024-08-21 22:10:32.467 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][4/77] Data 0.021 (0.022) Batch 0.065 (0.065) Remain 00:00:14 loss: 0.1062 data: 0.0085 Lr: 0.36851
2024-08-21 22:10:32.467 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][4/77] Data 0.032 (0.027) Batch 0.065 (0.065) Remain 00:00:14 loss: 0.1062 data: -0.0001 Lr: 0.36851
2024-08-21 22:10:32.542 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][5/77] Data 0.024 (0.022) Batch 0.075 (0.067) Remain 00:00:15 loss: 0.0987 data: -0.0002 Lr: 0.36688
2024-08-21 22:10:32.542 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][5/77] Data 0.036 (0.029) Batch 0.074 (0.067) Remain 00:00:15 loss: 0.0987 data: 0.0131 Lr: 0.36688
2024-08-21 22:10:32.608 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][6/77] Data 0.022 (0.022) Batch 0.066 (0.067) Remain 00:00:15 loss: 0.1207 data: -0.0010 Lr: 0.36526
2024-08-21 22:10:32.608 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][6/77] Data 0.032 (0.030) Batch 0.066 (0.067) Remain 00:00:15 loss: 0.1207 data: 0.0141 Lr: 0.36526
2024-08-21 22:10:32.680 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][7/77] Data 0.023 (0.022) Batch 0.072 (0.068) Remain 00:00:15 loss: 0.0871 data: -0.0143 Lr: 0.36364
2024-08-21 22:10:32.680 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][7/77] Data 0.031 (0.030) Batch 0.072 (0.068) Remain 00:00:15 loss: 0.0871 data: -0.0092 Lr: 0.36364
2024-08-21 22:10:32.753 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][8/77] Data 0.021 (0.022) Batch 0.073 (0.069) Remain 00:00:15 loss: 0.0986 data: 0.0009 Lr: 0.36201
2024-08-21 22:10:32.753 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][8/77] Data 0.031 (0.030) Batch 0.073 (0.069) Remain 00:00:15 loss: 0.0986 data: -0.0091 Lr: 0.36201
2024-08-21 22:10:32.820 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][9/77] Data 0.021 (0.022) Batch 0.067 (0.068) Remain 00:00:15 loss: 0.0658 data: 0.0181 Lr: 0.36039
2024-08-21 22:10:32.820 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][9/77] Data 0.025 (0.029) Batch 0.067 (0.068) Remain 00:00:15 loss: 0.0658 data: 0.0132 Lr: 0.36039
2024-08-21 22:10:32.919 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][10/77] Data 0.032 (0.023) Batch 0.099 (0.072) Remain 00:00:15 loss: 0.0744 data: -0.0050 Lr: 0.35877
2024-08-21 22:10:32.919 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][10/77] Data 0.043 (0.031) Batch 0.099 (0.072) Remain 00:00:15 loss: 0.0744 data: 0.0072 Lr: 0.35877
2024-08-21 22:10:33.014 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][11/77] Data 0.028 (0.024) Batch 0.095 (0.074) Remain 00:00:16 loss: 0.0570 data: 0.0128 Lr: 0.35714
2024-08-21 22:10:33.014 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][11/77] Data 0.043 (0.032) Batch 0.095 (0.074) Remain 00:00:16 loss: 0.0570 data: 0.0006 Lr: 0.35714
2024-08-21 22:10:33.081 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][12/77] Data 0.028 (0.024) Batch 0.067 (0.073) Remain 00:00:16 loss: 0.1011 data: 0.0025 Lr: 0.35552
2024-08-21 22:10:33.081 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][12/77] Data 0.028 (0.032) Batch 0.067 (0.073) Remain 00:00:16 loss: 0.1011 data: -0.0087 Lr: 0.35552
2024-08-21 22:10:33.161 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][13/77] Data 0.027 (0.024) Batch 0.081 (0.074) Remain 00:00:16 loss: 0.0732 data: 0.0144 Lr: 0.35390
2024-08-21 22:10:33.161 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][13/77] Data 0.039 (0.032) Batch 0.081 (0.074) Remain 00:00:16 loss: 0.0732 data: 0.0046 Lr: 0.35390
2024-08-21 22:10:33.231 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][14/77] Data 0.027 (0.024) Batch 0.069 (0.074) Remain 00:00:16 loss: 0.1179 data: -0.0103 Lr: 0.35227
2024-08-21 22:10:33.231 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][14/77] Data 0.027 (0.032) Batch 0.069 (0.074) Remain 00:00:16 loss: 0.1179 data: 0.0026 Lr: 0.35227
2024-08-21 22:10:33.301 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][15/77] Data 0.027 (0.025) Batch 0.070 (0.073) Remain 00:00:15 loss: 0.1137 data: 0.0008 Lr: 0.35065
2024-08-21 22:10:33.301 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_400
2024-08-21 22:10:33.301 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][15/77] Data 0.027 (0.032) Batch 0.070 (0.073) Remain 00:00:15 loss: 0.1137 data: -0.0049 Lr: 0.35065
2024-08-21 22:10:33.301 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_400
2024-08-21 22:10:33.336 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -549.582763671875
2024-08-21 22:10:33.337 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -259.6573181152344
2024-08-21 22:10:33.337 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -549.582763671875
2024-08-21 22:10:33.337 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -289.9254455566406
2024-08-21 22:10:33.402 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][16/77] Data 0.063 (0.027) Batch 0.102 (0.075) Remain 00:00:16 loss: 0.0658 data: -0.0114 Lr: 0.34903
2024-08-21 22:10:33.403 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][16/77] Data 0.063 (0.034) Batch 0.102 (0.075) Remain 00:00:16 loss: 0.0658 data: 0.0060 Lr: 0.34903
2024-08-21 22:10:33.481 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][17/77] Data 0.035 (0.028) Batch 0.079 (0.076) Remain 00:00:16 loss: 0.1175 data: -0.0005 Lr: 0.34740
2024-08-21 22:10:33.482 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][17/77] Data 0.027 (0.033) Batch 0.079 (0.076) Remain 00:00:16 loss: 0.1175 data: -0.0153 Lr: 0.34740
2024-08-21 22:10:33.561 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][18/77] Data 0.032 (0.033) Batch 0.079 (0.076) Remain 00:00:16 loss: 0.0739 data: -0.0132 Lr: 0.34578
2024-08-21 22:10:33.561 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][18/77] Data 0.027 (0.028) Batch 0.080 (0.076) Remain 00:00:16 loss: 0.0739 data: 0.0003 Lr: 0.34578
2024-08-21 22:10:33.642 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][19/77] Data 0.022 (0.027) Batch 0.082 (0.076) Remain 00:00:16 loss: 0.1064 data: 0.0128 Lr: 0.34416
2024-08-21 22:10:33.643 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][19/77] Data 0.033 (0.033) Batch 0.082 (0.076) Remain 00:00:16 loss: 0.1064 data: 0.0028 Lr: 0.34416
2024-08-21 22:10:33.710 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][20/77] Data 0.021 (0.027) Batch 0.068 (0.076) Remain 00:00:16 loss: 0.0577 data: -0.0035 Lr: 0.34253
2024-08-21 22:10:33.710 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][20/77] Data 0.032 (0.033) Batch 0.068 (0.076) Remain 00:00:16 loss: 0.0577 data: -0.0049 Lr: 0.34253
2024-08-21 22:10:33.774 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][21/77] Data 0.021 (0.027) Batch 0.064 (0.075) Remain 00:00:15 loss: 0.1339 data: 0.0170 Lr: 0.34091
2024-08-21 22:10:33.775 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][21/77] Data 0.027 (0.033) Batch 0.064 (0.075) Remain 00:00:15 loss: 0.1339 data: 0.0059 Lr: 0.34091
2024-08-21 22:10:33.838 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][22/77] Data 0.022 (0.027) Batch 0.064 (0.075) Remain 00:00:15 loss: 0.2048 data: -0.0123 Lr: 0.33929
2024-08-21 22:10:33.838 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][22/77] Data 0.027 (0.033) Batch 0.064 (0.075) Remain 00:00:15 loss: 0.2048 data: -0.0073 Lr: 0.33929
2024-08-21 22:10:33.902 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][23/77] Data 0.021 (0.026) Batch 0.064 (0.074) Remain 00:00:15 loss: 0.0902 data: 0.0074 Lr: 0.33766
2024-08-21 22:10:33.902 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][23/77] Data 0.027 (0.032) Batch 0.064 (0.074) Remain 00:00:15 loss: 0.0902 data: -0.0111 Lr: 0.33766
2024-08-21 22:10:33.961 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][24/77] Data 0.021 (0.026) Batch 0.059 (0.073) Remain 00:00:15 loss: 0.0970 data: 0.0008 Lr: 0.33604
2024-08-21 22:10:33.962 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][24/77] Data 0.024 (0.032) Batch 0.059 (0.073) Remain 00:00:15 loss: 0.0970 data: 0.0007 Lr: 0.33604
2024-08-21 22:10:34.021 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][25/77] Data 0.022 (0.026) Batch 0.059 (0.073) Remain 00:00:15 loss: 0.2405 data: 0.0049 Lr: 0.33442
2024-08-21 22:10:34.021 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][25/77] Data 0.025 (0.032) Batch 0.059 (0.073) Remain 00:00:15 loss: 0.2405 data: 0.0033 Lr: 0.33442
2024-08-21 22:10:34.094 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][26/77] Data 0.023 (0.026) Batch 0.073 (0.073) Remain 00:00:15 loss: 0.1150 data: 0.0082 Lr: 0.33279
2024-08-21 22:10:34.094 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][26/77] Data 0.025 (0.031) Batch 0.073 (0.073) Remain 00:00:15 loss: 0.1150 data: 0.0042 Lr: 0.33279
2024-08-21 22:10:34.176 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][27/77] Data 0.021 (0.026) Batch 0.082 (0.073) Remain 00:00:15 loss: 0.1299 data: -0.0030 Lr: 0.33117
2024-08-21 22:10:34.177 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][27/77] Data 0.035 (0.032) Batch 0.082 (0.073) Remain 00:00:15 loss: 0.1299 data: 0.0168 Lr: 0.33117
2024-08-21 22:10:34.241 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][28/77] Data 0.021 (0.025) Batch 0.064 (0.073) Remain 00:00:14 loss: 0.0832 data: 0.0003 Lr: 0.32955
2024-08-21 22:10:34.241 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][28/77] Data 0.027 (0.031) Batch 0.064 (0.073) Remain 00:00:14 loss: 0.0832 data: -0.0069 Lr: 0.32955
2024-08-21 22:10:34.304 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][29/77] Data 0.025 (0.025) Batch 0.064 (0.073) Remain 00:00:14 loss: 0.1901 data: 0.0033 Lr: 0.32792
2024-08-21 22:10:34.304 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][29/77] Data 0.027 (0.031) Batch 0.063 (0.073) Remain 00:00:14 loss: 0.1901 data: 0.0206 Lr: 0.32792
2024-08-21 22:10:34.369 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][30/77] Data 0.022 (0.025) Batch 0.065 (0.072) Remain 00:00:14 loss: 0.0679 data: -0.0038 Lr: 0.32630
2024-08-21 22:10:34.369 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][30/77] Data 0.027 (0.031) Batch 0.065 (0.072) Remain 00:00:14 loss: 0.0679 data: -0.0056 Lr: 0.32630
2024-08-21 22:10:34.435 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][31/77] Data 0.027 (0.025) Batch 0.066 (0.072) Remain 00:00:14 loss: 0.0855 data: -0.0020 Lr: 0.32468
2024-08-21 22:10:34.435 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][31/77] Data 0.027 (0.031) Batch 0.066 (0.072) Remain 00:00:14 loss: 0.0855 data: -0.0062 Lr: 0.32468
2024-08-21 22:10:34.498 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][32/77] Data 0.027 (0.025) Batch 0.063 (0.072) Remain 00:00:14 loss: 0.0814 data: 0.0069 Lr: 0.32305
2024-08-21 22:10:34.498 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][32/77] Data 0.027 (0.031) Batch 0.063 (0.072) Remain 00:00:14 loss: 0.0814 data: -0.0094 Lr: 0.32305
2024-08-21 22:10:34.566 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][33/77] Data 0.022 (0.025) Batch 0.068 (0.072) Remain 00:00:14 loss: 0.1354 data: 0.0004 Lr: 0.32143
2024-08-21 22:10:34.567 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][33/77] Data 0.027 (0.031) Batch 0.068 (0.072) Remain 00:00:14 loss: 0.1354 data: -0.0007 Lr: 0.32143
2024-08-21 22:10:34.633 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][34/77] Data 0.027 (0.025) Batch 0.067 (0.072) Remain 00:00:14 loss: 0.0870 data: -0.0012 Lr: 0.31981
2024-08-21 22:10:34.633 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][34/77] Data 0.027 (0.031) Batch 0.067 (0.072) Remain 00:00:14 loss: 0.0870 data: 0.0023 Lr: 0.31981
2024-08-21 22:10:34.698 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][35/77] Data 0.026 (0.025) Batch 0.064 (0.071) Remain 00:00:14 loss: 0.1154 data: -0.0101 Lr: 0.31818
2024-08-21 22:10:34.698 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][35/77] Data 0.027 (0.030) Batch 0.064 (0.071) Remain 00:00:14 loss: 0.1154 data: 0.0042 Lr: 0.31818
2024-08-21 22:10:34.765 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][36/77] Data 0.023 (0.025) Batch 0.067 (0.071) Remain 00:00:13 loss: 0.0619 data: -0.0069 Lr: 0.31656
2024-08-21 22:10:34.766 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][36/77] Data 0.027 (0.030) Batch 0.068 (0.071) Remain 00:00:13 loss: 0.0619 data: -0.0051 Lr: 0.31656
2024-08-21 22:10:34.840 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][37/77] Data 0.038 (0.031) Batch 0.075 (0.071) Remain 00:00:13 loss: 0.2500 data: -0.0017 Lr: 0.31494
2024-08-21 22:10:34.840 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][37/77] Data 0.026 (0.025) Batch 0.075 (0.071) Remain 00:00:13 loss: 0.2500 data: -0.0185 Lr: 0.31494
2024-08-21 22:10:34.904 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][38/77] Data 0.023 (0.025) Batch 0.064 (0.071) Remain 00:00:13 loss: 0.0901 data: -0.0115 Lr: 0.31331
2024-08-21 22:10:34.904 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][38/77] Data 0.027 (0.030) Batch 0.064 (0.071) Remain 00:00:13 loss: 0.0901 data: -0.0026 Lr: 0.31331
2024-08-21 22:10:34.971 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][39/77] Data 0.024 (0.025) Batch 0.067 (0.071) Remain 00:00:13 loss: 0.0606 data: 0.0002 Lr: 0.31169
2024-08-21 22:10:34.971 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][39/77] Data 0.029 (0.030) Batch 0.067 (0.071) Remain 00:00:13 loss: 0.0606 data: 0.0017 Lr: 0.31169
2024-08-21 22:10:35.036 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][40/77] Data 0.023 (0.025) Batch 0.065 (0.071) Remain 00:00:13 loss: 0.0802 data: 0.0048 Lr: 0.31006
2024-08-21 22:10:35.036 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][40/77] Data 0.027 (0.030) Batch 0.065 (0.071) Remain 00:00:13 loss: 0.0802 data: 0.0082 Lr: 0.31006
2024-08-21 22:10:35.101 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][41/77] Data 0.022 (0.025) Batch 0.065 (0.071) Remain 00:00:13 loss: 0.1477 data: -0.0055 Lr: 0.30844
2024-08-21 22:10:35.101 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][41/77] Data 0.027 (0.030) Batch 0.065 (0.071) Remain 00:00:13 loss: 0.1477 data: 0.0003 Lr: 0.30844
2024-08-21 22:10:35.167 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][42/77] Data 0.022 (0.025) Batch 0.066 (0.071) Remain 00:00:13 loss: 0.0896 data: -0.0115 Lr: 0.30682
2024-08-21 22:10:35.167 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][42/77] Data 0.027 (0.030) Batch 0.065 (0.071) Remain 00:00:13 loss: 0.0896 data: -0.0019 Lr: 0.30682
2024-08-21 22:10:35.232 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][43/77] Data 0.022 (0.025) Batch 0.066 (0.070) Remain 00:00:13 loss: 0.1749 data: 0.0048 Lr: 0.30519
2024-08-21 22:10:35.232 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][43/77] Data 0.027 (0.030) Batch 0.066 (0.070) Remain 00:00:13 loss: 0.1749 data: 0.0038 Lr: 0.30519
2024-08-21 22:10:35.297 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][44/77] Data 0.021 (0.025) Batch 0.065 (0.070) Remain 00:00:13 loss: 0.0985 data: -0.0127 Lr: 0.30357
2024-08-21 22:10:35.298 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][44/77] Data 0.027 (0.030) Batch 0.065 (0.070) Remain 00:00:13 loss: 0.0985 data: -0.0067 Lr: 0.30357
2024-08-21 22:10:35.362 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][45/77] Data 0.022 (0.025) Batch 0.065 (0.070) Remain 00:00:13 loss: 0.1229 data: 0.0013 Lr: 0.30195
2024-08-21 22:10:35.362 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][45/77] Data 0.027 (0.030) Batch 0.065 (0.070) Remain 00:00:13 loss: 0.1229 data: 0.0039 Lr: 0.30195
2024-08-21 22:10:35.427 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][46/77] Data 0.022 (0.025) Batch 0.065 (0.070) Remain 00:00:13 loss: 0.1010 data: -0.0181 Lr: 0.30032
2024-08-21 22:10:35.427 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][46/77] Data 0.027 (0.030) Batch 0.065 (0.070) Remain 00:00:13 loss: 0.1010 data: 0.0130 Lr: 0.30032
2024-08-21 22:10:35.491 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][47/77] Data 0.022 (0.025) Batch 0.064 (0.070) Remain 00:00:12 loss: 0.0681 data: -0.0068 Lr: 0.29870
2024-08-21 22:10:35.491 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][47/77] Data 0.027 (0.030) Batch 0.064 (0.070) Remain 00:00:12 loss: 0.0681 data: -0.0153 Lr: 0.29870
2024-08-21 22:10:35.555 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][48/77] Data 0.022 (0.025) Batch 0.064 (0.070) Remain 00:00:12 loss: 0.0929 data: -0.0043 Lr: 0.29708
2024-08-21 22:10:35.556 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][48/77] Data 0.027 (0.030) Batch 0.064 (0.070) Remain 00:00:12 loss: 0.0929 data: 0.0115 Lr: 0.29708
2024-08-21 22:10:35.623 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][49/77] Data 0.023 (0.025) Batch 0.068 (0.070) Remain 00:00:12 loss: 0.1141 data: -0.0033 Lr: 0.29545
2024-08-21 22:10:35.623 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][49/77] Data 0.027 (0.030) Batch 0.068 (0.070) Remain 00:00:12 loss: 0.1141 data: 0.0087 Lr: 0.29545
2024-08-21 22:10:35.690 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][50/77] Data 0.027 (0.025) Batch 0.067 (0.070) Remain 00:00:12 loss: 0.0656 data: 0.0039 Lr: 0.29383
2024-08-21 22:10:35.690 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][50/77] Data 0.027 (0.030) Batch 0.066 (0.070) Remain 00:00:12 loss: 0.0656 data: 0.0019 Lr: 0.29383
2024-08-21 22:10:35.756 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][51/77] Data 0.028 (0.025) Batch 0.066 (0.070) Remain 00:00:12 loss: 0.1271 data: 0.0043 Lr: 0.29221
2024-08-21 22:10:35.756 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][51/77] Data 0.027 (0.030) Batch 0.066 (0.070) Remain 00:00:12 loss: 0.1271 data: -0.0014 Lr: 0.29221
2024-08-21 22:10:35.821 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][52/77] Data 0.027 (0.025) Batch 0.066 (0.070) Remain 00:00:12 loss: 0.0919 data: 0.0039 Lr: 0.29058
2024-08-21 22:10:35.822 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][52/77] Data 0.028 (0.030) Batch 0.066 (0.070) Remain 00:00:12 loss: 0.0919 data: 0.0043 Lr: 0.29058
2024-08-21 22:10:35.888 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][53/77] Data 0.028 (0.025) Batch 0.066 (0.070) Remain 00:00:12 loss: 0.1012 data: 0.0042 Lr: 0.28896
2024-08-21 22:10:35.888 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][53/77] Data 0.027 (0.030) Batch 0.066 (0.070) Remain 00:00:12 loss: 0.1012 data: 0.0098 Lr: 0.28896
2024-08-21 22:10:35.953 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][54/77] Data 0.027 (0.025) Batch 0.066 (0.069) Remain 00:00:12 loss: 0.0874 data: 0.0206 Lr: 0.28734
2024-08-21 22:10:35.954 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][54/77] Data 0.027 (0.029) Batch 0.066 (0.069) Remain 00:00:12 loss: 0.0874 data: -0.0146 Lr: 0.28734
2024-08-21 22:10:36.020 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][55/77] Data 0.027 (0.025) Batch 0.066 (0.069) Remain 00:00:12 loss: 0.1005 data: -0.0055 Lr: 0.28571
2024-08-21 22:10:36.020 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][55/77] Data 0.027 (0.029) Batch 0.066 (0.069) Remain 00:00:12 loss: 0.1005 data: 0.0108 Lr: 0.28571
2024-08-21 22:10:36.092 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][56/77] Data 0.027 (0.025) Batch 0.072 (0.069) Remain 00:00:12 loss: 0.1237 data: 0.0123 Lr: 0.28409
2024-08-21 22:10:36.092 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][56/77] Data 0.033 (0.029) Batch 0.072 (0.069) Remain 00:00:12 loss: 0.1237 data: -0.0007 Lr: 0.28409
2024-08-21 22:10:36.162 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][57/77] Data 0.027 (0.025) Batch 0.070 (0.069) Remain 00:00:12 loss: 0.1760 data: 0.0129 Lr: 0.28247
2024-08-21 22:10:36.162 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][57/77] Data 0.027 (0.029) Batch 0.070 (0.069) Remain 00:00:12 loss: 0.1760 data: 0.0060 Lr: 0.28247
2024-08-21 22:10:36.233 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][58/77] Data 0.027 (0.025) Batch 0.071 (0.069) Remain 00:00:12 loss: 0.0832 data: 0.0061 Lr: 0.28084
2024-08-21 22:10:36.233 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][58/77] Data 0.034 (0.030) Batch 0.071 (0.069) Remain 00:00:12 loss: 0.0832 data: -0.0133 Lr: 0.28084
2024-08-21 22:10:36.302 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][59/77] Data 0.027 (0.025) Batch 0.069 (0.069) Remain 00:00:12 loss: 0.1934 data: 0.0014 Lr: 0.27922
2024-08-21 22:10:36.302 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][59/77] Data 0.028 (0.029) Batch 0.069 (0.069) Remain 00:00:12 loss: 0.1934 data: 0.0114 Lr: 0.27922
2024-08-21 22:10:36.373 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][60/77] Data 0.029 (0.025) Batch 0.071 (0.070) Remain 00:00:11 loss: 0.0700 data: -0.0059 Lr: 0.27760
2024-08-21 22:10:36.374 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][60/77] Data 0.029 (0.029) Batch 0.071 (0.070) Remain 00:00:11 loss: 0.0700 data: -0.0132 Lr: 0.27760
2024-08-21 22:10:36.439 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][61/77] Data 0.027 (0.025) Batch 0.066 (0.069) Remain 00:00:11 loss: 0.0835 data: -0.0065 Lr: 0.27597
2024-08-21 22:10:36.440 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][61/77] Data 0.028 (0.029) Batch 0.066 (0.069) Remain 00:00:11 loss: 0.0835 data: 0.0008 Lr: 0.27597
2024-08-21 22:10:36.505 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][62/77] Data 0.027 (0.025) Batch 0.066 (0.069) Remain 00:00:11 loss: 0.0831 data: -0.0120 Lr: 0.27435
2024-08-21 22:10:36.506 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][62/77] Data 0.027 (0.029) Batch 0.066 (0.069) Remain 00:00:11 loss: 0.0831 data: 0.0184 Lr: 0.27435
2024-08-21 22:10:36.572 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][63/77] Data 0.027 (0.029) Batch 0.066 (0.069) Remain 00:00:11 loss: 0.0802 data: 0.0043 Lr: 0.27273
2024-08-21 22:10:36.572 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][63/77] Data 0.027 (0.025) Batch 0.067 (0.069) Remain 00:00:11 loss: 0.0802 data: 0.0034 Lr: 0.27273
2024-08-21 22:10:36.636 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][64/77] Data 0.023 (0.025) Batch 0.064 (0.069) Remain 00:00:11 loss: 0.1165 data: -0.0036 Lr: 0.27110
2024-08-21 22:10:36.636 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][64/77] Data 0.027 (0.029) Batch 0.064 (0.069) Remain 00:00:11 loss: 0.1165 data: 0.0031 Lr: 0.27110
2024-08-21 22:10:36.700 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][65/77] Data 0.023 (0.025) Batch 0.063 (0.069) Remain 00:00:11 loss: 0.0824 data: 0.0011 Lr: 0.26948
2024-08-21 22:10:36.700 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_450
2024-08-21 22:10:36.700 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][65/77] Data 0.027 (0.029) Batch 0.063 (0.069) Remain 00:00:11 loss: 0.0824 data: 0.0020 Lr: 0.26948
2024-08-21 22:10:36.700 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_450
2024-08-21 22:10:36.725 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -567.6279296875
2024-08-21 22:10:36.725 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -266.52777099609375
2024-08-21 22:10:36.725 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -567.6279296875
2024-08-21 22:10:36.725 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -301.10015869140625
2024-08-21 22:10:36.788 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][66/77] Data 0.046 (0.025) Batch 0.089 (0.069) Remain 00:00:11 loss: 0.1223 data: -0.0039 Lr: 0.26786
2024-08-21 22:10:36.788 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][66/77] Data 0.052 (0.030) Batch 0.089 (0.069) Remain 00:00:11 loss: 0.1223 data: 0.0113 Lr: 0.26786
2024-08-21 22:10:36.851 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][67/77] Data 0.023 (0.025) Batch 0.063 (0.069) Remain 00:00:11 loss: 0.0711 data: 0.0163 Lr: 0.26623
2024-08-21 22:10:36.851 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][67/77] Data 0.027 (0.030) Batch 0.063 (0.069) Remain 00:00:11 loss: 0.0711 data: -0.0025 Lr: 0.26623
2024-08-21 22:10:36.915 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][68/77] Data 0.022 (0.025) Batch 0.064 (0.069) Remain 00:00:11 loss: 0.0738 data: -0.0100 Lr: 0.26461
2024-08-21 22:10:36.915 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][68/77] Data 0.027 (0.030) Batch 0.064 (0.069) Remain 00:00:11 loss: 0.0738 data: 0.0062 Lr: 0.26461
2024-08-21 22:10:36.990 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][69/77] Data 0.021 (0.025) Batch 0.074 (0.069) Remain 00:00:11 loss: 0.1029 data: -0.0002 Lr: 0.26299
2024-08-21 22:10:36.990 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][69/77] Data 0.027 (0.030) Batch 0.074 (0.069) Remain 00:00:11 loss: 0.1029 data: -0.0260 Lr: 0.26299
2024-08-21 22:10:37.069 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][70/77] Data 0.021 (0.025) Batch 0.080 (0.070) Remain 00:00:11 loss: 0.1472 data: -0.0036 Lr: 0.26136
2024-08-21 22:10:37.069 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][70/77] Data 0.031 (0.030) Batch 0.080 (0.070) Remain 00:00:11 loss: 0.1472 data: -0.0002 Lr: 0.26136
2024-08-21 22:10:37.159 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][71/77] Data 0.021 (0.025) Batch 0.090 (0.070) Remain 00:00:11 loss: 0.0995 data: 0.0014 Lr: 0.25974
2024-08-21 22:10:37.159 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][71/77] Data 0.039 (0.030) Batch 0.090 (0.070) Remain 00:00:11 loss: 0.0995 data: 0.0028 Lr: 0.25974
2024-08-21 22:10:37.244 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][72/77] Data 0.021 (0.025) Batch 0.085 (0.070) Remain 00:00:11 loss: 0.1111 data: 0.0001 Lr: 0.25812
2024-08-21 22:10:37.244 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][72/77] Data 0.039 (0.030) Batch 0.085 (0.070) Remain 00:00:11 loss: 0.1111 data: 0.0056 Lr: 0.25812
2024-08-21 22:10:37.321 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][73/77] Data 0.021 (0.025) Batch 0.078 (0.070) Remain 00:00:11 loss: 0.1273 data: -0.0077 Lr: 0.25649
2024-08-21 22:10:37.321 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][73/77] Data 0.039 (0.030) Batch 0.078 (0.070) Remain 00:00:11 loss: 0.1273 data: -0.0081 Lr: 0.25649
2024-08-21 22:10:37.386 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][74/77] Data 0.022 (0.025) Batch 0.065 (0.070) Remain 00:00:11 loss: 0.1419 data: 0.0020 Lr: 0.25487
2024-08-21 22:10:37.386 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][74/77] Data 0.027 (0.030) Batch 0.065 (0.070) Remain 00:00:11 loss: 0.1419 data: -0.0110 Lr: 0.25487
2024-08-21 22:10:37.451 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][75/77] Data 0.021 (0.025) Batch 0.065 (0.070) Remain 00:00:10 loss: 0.1058 data: 0.0091 Lr: 0.25325
2024-08-21 22:10:37.451 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][75/77] Data 0.027 (0.030) Batch 0.065 (0.070) Remain 00:00:10 loss: 0.1058 data: 0.0016 Lr: 0.25325
2024-08-21 22:10:37.515 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][76/77] Data 0.022 (0.025) Batch 0.064 (0.070) Remain 00:00:10 loss: 0.0817 data: 0.0090 Lr: 0.25162
2024-08-21 22:10:37.515 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][76/77] Data 0.027 (0.030) Batch 0.064 (0.070) Remain 00:00:10 loss: 0.0817 data: -0.0059 Lr: 0.25162
2024-08-21 22:10:37.556 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][77/77] Data 0.023 (0.025) Batch 0.040 (0.070) Remain 00:00:10 loss: 0.1230 data: 0.0059 Lr: 0.25000
2024-08-21 22:10:37.556 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 22:10:37.556 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][77/77] Data 0.030 (0.030) Batch 0.040 (0.070) Remain 00:00:10 loss: 0.1230 data: -0.0222 Lr: 0.25000
2024-08-21 22:10:37.556 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 22:10:41.401 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0322, Accuracy: 0.9888
2024-08-21 22:10:41.401 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0322, Accuracy: 0.9888
2024-08-21 22:10:41.401 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 22:10:41.401 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 22:10:41.402 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 22:10:41.402 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 22:10:41.495 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][1/77] Data 0.043 (0.043) Batch 0.093 (0.093) Remain 00:00:14 loss: 0.0671 data: -0.0048 Lr: 0.24838
2024-08-21 22:10:41.495 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][1/77] Data 0.055 (0.055) Batch 0.093 (0.093) Remain 00:00:14 loss: 0.0671 data: 0.0207 Lr: 0.24838
2024-08-21 22:10:41.560 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][2/77] Data 0.021 (0.021) Batch 0.065 (0.065) Remain 00:00:09 loss: 0.0617 data: 0.0003 Lr: 0.24675
2024-08-21 22:10:41.560 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][2/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:09 loss: 0.0617 data: 0.0018 Lr: 0.24675
2024-08-21 22:10:41.626 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][3/77] Data 0.023 (0.022) Batch 0.066 (0.066) Remain 00:00:09 loss: 0.0929 data: -0.0084 Lr: 0.24513
2024-08-21 22:10:41.626 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][3/77] Data 0.027 (0.027) Batch 0.066 (0.066) Remain 00:00:09 loss: 0.0929 data: -0.0102 Lr: 0.24513
2024-08-21 22:10:41.691 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][4/77] Data 0.021 (0.022) Batch 0.065 (0.065) Remain 00:00:09 loss: 0.1021 data: 0.0040 Lr: 0.24351
2024-08-21 22:10:41.691 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][4/77] Data 0.028 (0.028) Batch 0.065 (0.065) Remain 00:00:09 loss: 0.1021 data: 0.0095 Lr: 0.24351
2024-08-21 22:10:41.755 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][5/77] Data 0.021 (0.022) Batch 0.065 (0.065) Remain 00:00:09 loss: 0.0788 data: 0.0098 Lr: 0.24188
2024-08-21 22:10:41.756 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][5/77] Data 0.027 (0.028) Batch 0.065 (0.065) Remain 00:00:09 loss: 0.0788 data: 0.0200 Lr: 0.24188
2024-08-21 22:10:41.819 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][6/77] Data 0.023 (0.022) Batch 0.063 (0.065) Remain 00:00:09 loss: 0.0970 data: -0.0010 Lr: 0.24026
2024-08-21 22:10:41.819 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][6/77] Data 0.027 (0.027) Batch 0.063 (0.065) Remain 00:00:09 loss: 0.0970 data: -0.0033 Lr: 0.24026
2024-08-21 22:10:41.883 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][7/77] Data 0.022 (0.022) Batch 0.064 (0.065) Remain 00:00:09 loss: 0.0864 data: -0.0009 Lr: 0.23864
2024-08-21 22:10:41.883 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][7/77] Data 0.027 (0.027) Batch 0.064 (0.065) Remain 00:00:09 loss: 0.0864 data: -0.0046 Lr: 0.23864
2024-08-21 22:10:41.949 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][8/77] Data 0.021 (0.022) Batch 0.066 (0.065) Remain 00:00:09 loss: 0.0575 data: 0.0052 Lr: 0.23701
2024-08-21 22:10:41.949 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][8/77] Data 0.027 (0.027) Batch 0.066 (0.065) Remain 00:00:09 loss: 0.0575 data: -0.0108 Lr: 0.23701
2024-08-21 22:10:42.015 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][9/77] Data 0.022 (0.022) Batch 0.066 (0.065) Remain 00:00:09 loss: 0.0707 data: -0.0215 Lr: 0.23539
2024-08-21 22:10:42.015 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][9/77] Data 0.027 (0.027) Batch 0.066 (0.065) Remain 00:00:09 loss: 0.0707 data: -0.0174 Lr: 0.23539
2024-08-21 22:10:42.081 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][10/77] Data 0.022 (0.022) Batch 0.066 (0.065) Remain 00:00:09 loss: 0.0865 data: -0.0005 Lr: 0.23377
2024-08-21 22:10:42.081 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][10/77] Data 0.027 (0.027) Batch 0.066 (0.065) Remain 00:00:09 loss: 0.0865 data: 0.0161 Lr: 0.23377
2024-08-21 22:10:42.147 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][11/77] Data 0.021 (0.022) Batch 0.066 (0.065) Remain 00:00:09 loss: 0.1266 data: 0.0047 Lr: 0.23214
2024-08-21 22:10:42.147 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][11/77] Data 0.027 (0.027) Batch 0.066 (0.065) Remain 00:00:09 loss: 0.1266 data: -0.0004 Lr: 0.23214
2024-08-21 22:10:42.210 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][12/77] Data 0.021 (0.022) Batch 0.063 (0.065) Remain 00:00:09 loss: 0.1653 data: 0.0077 Lr: 0.23052
2024-08-21 22:10:42.210 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][12/77] Data 0.027 (0.027) Batch 0.063 (0.065) Remain 00:00:09 loss: 0.1653 data: -0.0003 Lr: 0.23052
2024-08-21 22:10:42.275 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][13/77] Data 0.022 (0.022) Batch 0.065 (0.065) Remain 00:00:09 loss: 0.0923 data: -0.0017 Lr: 0.22890
2024-08-21 22:10:42.275 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][13/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:09 loss: 0.0923 data: -0.0061 Lr: 0.22890
2024-08-21 22:10:42.341 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][14/77] Data 0.022 (0.022) Batch 0.066 (0.065) Remain 00:00:09 loss: 0.0647 data: -0.0142 Lr: 0.22727
2024-08-21 22:10:42.341 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][14/77] Data 0.027 (0.027) Batch 0.066 (0.065) Remain 00:00:09 loss: 0.0647 data: 0.0031 Lr: 0.22727
2024-08-21 22:10:42.408 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][15/77] Data 0.027 (0.022) Batch 0.066 (0.065) Remain 00:00:09 loss: 0.0517 data: 0.0090 Lr: 0.22565
2024-08-21 22:10:42.408 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][15/77] Data 0.027 (0.027) Batch 0.066 (0.065) Remain 00:00:09 loss: 0.0517 data: 0.0112 Lr: 0.22565
2024-08-21 22:10:42.474 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][16/77] Data 0.027 (0.022) Batch 0.066 (0.065) Remain 00:00:09 loss: 0.0970 data: -0.0122 Lr: 0.22403
2024-08-21 22:10:42.474 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][16/77] Data 0.027 (0.027) Batch 0.066 (0.065) Remain 00:00:09 loss: 0.0970 data: -0.0009 Lr: 0.22403
2024-08-21 22:10:42.539 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][17/77] Data 0.027 (0.023) Batch 0.066 (0.065) Remain 00:00:09 loss: 0.0735 data: 0.0150 Lr: 0.22240
2024-08-21 22:10:42.540 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][17/77] Data 0.027 (0.027) Batch 0.066 (0.065) Remain 00:00:09 loss: 0.0735 data: 0.0024 Lr: 0.22240
2024-08-21 22:10:42.604 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][18/77] Data 0.027 (0.023) Batch 0.065 (0.065) Remain 00:00:08 loss: 0.1413 data: -0.0023 Lr: 0.22078
2024-08-21 22:10:42.605 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][18/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:08 loss: 0.1413 data: 0.0074 Lr: 0.22078
2024-08-21 22:10:42.671 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][19/77] Data 0.027 (0.023) Batch 0.067 (0.065) Remain 00:00:08 loss: 0.1496 data: -0.0049 Lr: 0.21916
2024-08-21 22:10:42.672 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][19/77] Data 0.027 (0.027) Batch 0.067 (0.065) Remain 00:00:08 loss: 0.1496 data: 0.0019 Lr: 0.21916
2024-08-21 22:10:42.742 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][20/77] Data 0.028 (0.023) Batch 0.070 (0.066) Remain 00:00:08 loss: 0.0856 data: 0.0180 Lr: 0.21753
2024-08-21 22:10:42.742 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][20/77] Data 0.028 (0.027) Batch 0.071 (0.066) Remain 00:00:08 loss: 0.0856 data: 0.0016 Lr: 0.21753
2024-08-21 22:10:42.807 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][21/77] Data 0.027 (0.024) Batch 0.065 (0.066) Remain 00:00:08 loss: 0.0617 data: 0.0004 Lr: 0.21591
2024-08-21 22:10:42.807 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][21/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:08 loss: 0.0617 data: -0.0171 Lr: 0.21591
2024-08-21 22:10:42.873 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][22/77] Data 0.027 (0.024) Batch 0.065 (0.066) Remain 00:00:08 loss: 0.0636 data: 0.0093 Lr: 0.21429
2024-08-21 22:10:42.873 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][22/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:08 loss: 0.0636 data: 0.0281 Lr: 0.21429
2024-08-21 22:10:42.938 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][23/77] Data 0.027 (0.024) Batch 0.066 (0.066) Remain 00:00:08 loss: 0.1558 data: -0.0026 Lr: 0.21266
2024-08-21 22:10:42.938 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][23/77] Data 0.027 (0.027) Batch 0.066 (0.066) Remain 00:00:08 loss: 0.1558 data: -0.0006 Lr: 0.21266
2024-08-21 22:10:43.004 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][24/77] Data 0.027 (0.024) Batch 0.066 (0.066) Remain 00:00:08 loss: 0.1087 data: -0.0075 Lr: 0.21104
2024-08-21 22:10:43.004 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][24/77] Data 0.027 (0.027) Batch 0.066 (0.066) Remain 00:00:08 loss: 0.1087 data: -0.0042 Lr: 0.21104
2024-08-21 22:10:43.074 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][25/77] Data 0.027 (0.024) Batch 0.070 (0.066) Remain 00:00:08 loss: 0.0803 data: -0.0091 Lr: 0.20942
2024-08-21 22:10:43.074 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][25/77] Data 0.028 (0.027) Batch 0.070 (0.066) Remain 00:00:08 loss: 0.0803 data: -0.0039 Lr: 0.20942
2024-08-21 22:10:43.145 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][26/77] Data 0.029 (0.024) Batch 0.072 (0.066) Remain 00:00:08 loss: 0.1190 data: 0.0128 Lr: 0.20779
2024-08-21 22:10:43.145 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][26/77] Data 0.029 (0.027) Batch 0.071 (0.066) Remain 00:00:08 loss: 0.1190 data: 0.0125 Lr: 0.20779
2024-08-21 22:10:43.212 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][27/77] Data 0.027 (0.025) Batch 0.067 (0.066) Remain 00:00:08 loss: 0.0991 data: 0.0105 Lr: 0.20617
2024-08-21 22:10:43.213 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][27/77] Data 0.027 (0.027) Batch 0.067 (0.066) Remain 00:00:08 loss: 0.0991 data: -0.0005 Lr: 0.20617
2024-08-21 22:10:43.278 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][28/77] Data 0.027 (0.025) Batch 0.065 (0.066) Remain 00:00:08 loss: 0.0820 data: 0.0011 Lr: 0.20455
2024-08-21 22:10:43.278 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][28/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:08 loss: 0.0820 data: 0.0065 Lr: 0.20455
2024-08-21 22:10:43.343 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][29/77] Data 0.027 (0.025) Batch 0.066 (0.066) Remain 00:00:08 loss: 0.0582 data: 0.0052 Lr: 0.20292
2024-08-21 22:10:43.344 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][29/77] Data 0.027 (0.027) Batch 0.066 (0.066) Remain 00:00:08 loss: 0.0582 data: 0.0013 Lr: 0.20292
2024-08-21 22:10:43.409 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][30/77] Data 0.027 (0.025) Batch 0.065 (0.066) Remain 00:00:08 loss: 0.0454 data: -0.0017 Lr: 0.20130
2024-08-21 22:10:43.409 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][30/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:08 loss: 0.0454 data: 0.0014 Lr: 0.20130
2024-08-21 22:10:43.473 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][31/77] Data 0.027 (0.025) Batch 0.065 (0.066) Remain 00:00:08 loss: 0.0862 data: 0.0064 Lr: 0.19968
2024-08-21 22:10:43.474 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][31/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:08 loss: 0.0862 data: 0.0081 Lr: 0.19968
2024-08-21 22:10:43.540 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][32/77] Data 0.027 (0.025) Batch 0.066 (0.066) Remain 00:00:08 loss: 0.0778 data: 0.0188 Lr: 0.19805
2024-08-21 22:10:43.540 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][32/77] Data 0.028 (0.027) Batch 0.066 (0.066) Remain 00:00:08 loss: 0.0778 data: 0.0002 Lr: 0.19805
2024-08-21 22:10:43.606 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][33/77] Data 0.027 (0.025) Batch 0.066 (0.066) Remain 00:00:08 loss: 0.0621 data: 0.0007 Lr: 0.19643
2024-08-21 22:10:43.606 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][33/77] Data 0.027 (0.027) Batch 0.066 (0.066) Remain 00:00:08 loss: 0.0621 data: 0.0149 Lr: 0.19643
2024-08-21 22:10:43.673 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][34/77] Data 0.028 (0.025) Batch 0.068 (0.066) Remain 00:00:07 loss: 0.0700 data: -0.0118 Lr: 0.19481
2024-08-21 22:10:43.674 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][34/77] Data 0.028 (0.027) Batch 0.068 (0.066) Remain 00:00:07 loss: 0.0700 data: 0.0067 Lr: 0.19481
2024-08-21 22:10:43.739 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][35/77] Data 0.027 (0.025) Batch 0.065 (0.066) Remain 00:00:07 loss: 0.0942 data: 0.0114 Lr: 0.19318
2024-08-21 22:10:43.739 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][35/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:07 loss: 0.0942 data: -0.0016 Lr: 0.19318
2024-08-21 22:10:43.804 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][36/77] Data 0.027 (0.025) Batch 0.065 (0.066) Remain 00:00:07 loss: 0.0866 data: -0.0139 Lr: 0.19156
2024-08-21 22:10:43.804 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][36/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:07 loss: 0.0866 data: 0.0071 Lr: 0.19156
2024-08-21 22:10:43.874 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][37/77] Data 0.028 (0.025) Batch 0.070 (0.066) Remain 00:00:07 loss: 0.0610 data: 0.0010 Lr: 0.18994
2024-08-21 22:10:43.874 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][37/77] Data 0.028 (0.027) Batch 0.070 (0.066) Remain 00:00:07 loss: 0.0610 data: 0.0075 Lr: 0.18994
2024-08-21 22:10:43.947 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][38/77] Data 0.029 (0.025) Batch 0.073 (0.066) Remain 00:00:07 loss: 0.0760 data: -0.0011 Lr: 0.18831
2024-08-21 22:10:43.947 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_500
2024-08-21 22:10:43.947 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][38/77] Data 0.028 (0.027) Batch 0.073 (0.066) Remain 00:00:07 loss: 0.0760 data: -0.0155 Lr: 0.18831
2024-08-21 22:10:43.948 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_500
2024-08-21 22:10:43.982 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -573.3421020507812
2024-08-21 22:10:43.982 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -269.5319519042969
2024-08-21 22:10:43.983 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -573.3421020507812
2024-08-21 22:10:43.984 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -303.8101806640625
2024-08-21 22:10:44.053 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][39/77] Data 0.064 (0.026) Batch 0.106 (0.067) Remain 00:00:07 loss: 0.1307 data: -0.0121 Lr: 0.18669
2024-08-21 22:10:44.053 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][39/77] Data 0.067 (0.028) Batch 0.106 (0.067) Remain 00:00:07 loss: 0.1307 data: 0.0119 Lr: 0.18669
2024-08-21 22:10:44.115 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][40/77] Data 0.027 (0.026) Batch 0.063 (0.067) Remain 00:00:07 loss: 0.0784 data: -0.0064 Lr: 0.18506
2024-08-21 22:10:44.115 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][40/77] Data 0.027 (0.028) Batch 0.063 (0.067) Remain 00:00:07 loss: 0.0784 data: 0.0154 Lr: 0.18506
2024-08-21 22:10:44.179 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][41/77] Data 0.027 (0.026) Batch 0.064 (0.067) Remain 00:00:07 loss: 0.1162 data: 0.0134 Lr: 0.18344
2024-08-21 22:10:44.179 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][41/77] Data 0.027 (0.028) Batch 0.064 (0.067) Remain 00:00:07 loss: 0.1162 data: -0.0015 Lr: 0.18344
2024-08-21 22:10:44.255 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][42/77] Data 0.034 (0.027) Batch 0.076 (0.067) Remain 00:00:07 loss: 0.0678 data: 0.0074 Lr: 0.18182
2024-08-21 22:10:44.255 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][42/77] Data 0.027 (0.028) Batch 0.076 (0.067) Remain 00:00:07 loss: 0.0678 data: 0.0047 Lr: 0.18182
2024-08-21 22:10:44.327 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][43/77] Data 0.034 (0.027) Batch 0.072 (0.067) Remain 00:00:07 loss: 0.0914 data: 0.0085 Lr: 0.18019
2024-08-21 22:10:44.328 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][43/77] Data 0.027 (0.028) Batch 0.072 (0.067) Remain 00:00:07 loss: 0.0914 data: -0.0006 Lr: 0.18019
2024-08-21 22:10:44.394 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][44/77] Data 0.027 (0.027) Batch 0.067 (0.067) Remain 00:00:07 loss: 0.1454 data: 0.0050 Lr: 0.17857
2024-08-21 22:10:44.395 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][44/77] Data 0.031 (0.028) Batch 0.067 (0.067) Remain 00:00:07 loss: 0.1454 data: 0.0176 Lr: 0.17857
2024-08-21 22:10:44.464 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][45/77] Data 0.027 (0.027) Batch 0.069 (0.067) Remain 00:00:07 loss: 0.0478 data: 0.0038 Lr: 0.17695
2024-08-21 22:10:44.464 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][45/77] Data 0.027 (0.028) Batch 0.069 (0.067) Remain 00:00:07 loss: 0.0478 data: 0.0177 Lr: 0.17695
2024-08-21 22:10:44.528 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][46/77] Data 0.027 (0.027) Batch 0.064 (0.067) Remain 00:00:07 loss: 0.1377 data: -0.0016 Lr: 0.17532
2024-08-21 22:10:44.528 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][46/77] Data 0.027 (0.028) Batch 0.064 (0.067) Remain 00:00:07 loss: 0.1377 data: -0.0026 Lr: 0.17532
2024-08-21 22:10:44.593 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][47/77] Data 0.027 (0.027) Batch 0.065 (0.067) Remain 00:00:07 loss: 0.1256 data: -0.0064 Lr: 0.17370
2024-08-21 22:10:44.593 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][47/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:07 loss: 0.1256 data: 0.0036 Lr: 0.17370
2024-08-21 22:10:44.665 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][48/77] Data 0.027 (0.027) Batch 0.072 (0.067) Remain 00:00:07 loss: 0.0455 data: -0.0077 Lr: 0.17208
2024-08-21 22:10:44.665 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][48/77] Data 0.031 (0.028) Batch 0.072 (0.067) Remain 00:00:07 loss: 0.0455 data: -0.0030 Lr: 0.17208
2024-08-21 22:10:44.729 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][49/77] Data 0.027 (0.027) Batch 0.064 (0.067) Remain 00:00:07 loss: 0.1075 data: 0.0010 Lr: 0.17045
2024-08-21 22:10:44.729 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][49/77] Data 0.027 (0.028) Batch 0.064 (0.067) Remain 00:00:07 loss: 0.1075 data: -0.0064 Lr: 0.17045
2024-08-21 22:10:44.793 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][50/77] Data 0.027 (0.027) Batch 0.064 (0.067) Remain 00:00:07 loss: 0.0718 data: 0.0031 Lr: 0.16883
2024-08-21 22:10:44.793 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][50/77] Data 0.027 (0.028) Batch 0.064 (0.067) Remain 00:00:07 loss: 0.0718 data: -0.0073 Lr: 0.16883
2024-08-21 22:10:44.857 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][51/77] Data 0.027 (0.027) Batch 0.064 (0.067) Remain 00:00:06 loss: 0.0790 data: 0.0146 Lr: 0.16721
2024-08-21 22:10:44.857 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][51/77] Data 0.027 (0.028) Batch 0.064 (0.067) Remain 00:00:06 loss: 0.0790 data: 0.0001 Lr: 0.16721
2024-08-21 22:10:44.921 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][52/77] Data 0.027 (0.027) Batch 0.064 (0.067) Remain 00:00:06 loss: 0.0832 data: -0.0058 Lr: 0.16558
2024-08-21 22:10:44.921 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][52/77] Data 0.027 (0.028) Batch 0.064 (0.067) Remain 00:00:06 loss: 0.0832 data: -0.0161 Lr: 0.16558
2024-08-21 22:10:44.985 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][53/77] Data 0.027 (0.027) Batch 0.064 (0.067) Remain 00:00:06 loss: 0.1584 data: -0.0125 Lr: 0.16396
2024-08-21 22:10:44.985 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][53/77] Data 0.027 (0.028) Batch 0.064 (0.067) Remain 00:00:06 loss: 0.1584 data: 0.0044 Lr: 0.16396
2024-08-21 22:10:45.049 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][54/77] Data 0.027 (0.027) Batch 0.064 (0.067) Remain 00:00:06 loss: 0.0491 data: -0.0028 Lr: 0.16234
2024-08-21 22:10:45.049 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][54/77] Data 0.027 (0.028) Batch 0.064 (0.067) Remain 00:00:06 loss: 0.0491 data: 0.0090 Lr: 0.16234
2024-08-21 22:10:45.113 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][55/77] Data 0.027 (0.027) Batch 0.064 (0.067) Remain 00:00:06 loss: 0.0712 data: -0.0214 Lr: 0.16071
2024-08-21 22:10:45.113 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][55/77] Data 0.027 (0.028) Batch 0.064 (0.067) Remain 00:00:06 loss: 0.0712 data: -0.0014 Lr: 0.16071
2024-08-21 22:10:45.184 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][56/77] Data 0.027 (0.027) Batch 0.071 (0.067) Remain 00:00:06 loss: 0.0570 data: -0.0066 Lr: 0.15909
2024-08-21 22:10:45.184 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][56/77] Data 0.027 (0.028) Batch 0.071 (0.067) Remain 00:00:06 loss: 0.0570 data: -0.0109 Lr: 0.15909
2024-08-21 22:10:45.248 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][57/77] Data 0.027 (0.027) Batch 0.064 (0.067) Remain 00:00:06 loss: 0.0584 data: -0.0083 Lr: 0.15747
2024-08-21 22:10:45.249 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][57/77] Data 0.027 (0.028) Batch 0.064 (0.067) Remain 00:00:06 loss: 0.0584 data: -0.0063 Lr: 0.15747
2024-08-21 22:10:45.316 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][58/77] Data 0.027 (0.027) Batch 0.068 (0.067) Remain 00:00:06 loss: 0.0937 data: -0.0136 Lr: 0.15584
2024-08-21 22:10:45.316 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][58/77] Data 0.027 (0.028) Batch 0.068 (0.067) Remain 00:00:06 loss: 0.0937 data: -0.0010 Lr: 0.15584
2024-08-21 22:10:45.393 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][59/77] Data 0.027 (0.027) Batch 0.077 (0.067) Remain 00:00:06 loss: 0.0985 data: -0.0025 Lr: 0.15422
2024-08-21 22:10:45.394 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][59/77] Data 0.034 (0.028) Batch 0.077 (0.067) Remain 00:00:06 loss: 0.0985 data: 0.0090 Lr: 0.15422
2024-08-21 22:10:45.465 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][60/77] Data 0.027 (0.027) Batch 0.072 (0.067) Remain 00:00:06 loss: 0.0830 data: -0.0015 Lr: 0.15260
2024-08-21 22:10:45.465 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][60/77] Data 0.033 (0.028) Batch 0.072 (0.067) Remain 00:00:06 loss: 0.0830 data: -0.0037 Lr: 0.15260
2024-08-21 22:10:45.534 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][61/77] Data 0.027 (0.027) Batch 0.069 (0.067) Remain 00:00:06 loss: 0.0509 data: 0.0039 Lr: 0.15097
2024-08-21 22:10:45.534 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][61/77] Data 0.027 (0.028) Batch 0.069 (0.067) Remain 00:00:06 loss: 0.0509 data: -0.0114 Lr: 0.15097
2024-08-21 22:10:45.598 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][62/77] Data 0.027 (0.027) Batch 0.064 (0.067) Remain 00:00:06 loss: 0.0925 data: -0.0047 Lr: 0.14935
2024-08-21 22:10:45.598 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][62/77] Data 0.027 (0.028) Batch 0.064 (0.067) Remain 00:00:06 loss: 0.0925 data: -0.0136 Lr: 0.14935
2024-08-21 22:10:45.670 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][63/77] Data 0.027 (0.027) Batch 0.072 (0.067) Remain 00:00:06 loss: 0.0885 data: 0.0097 Lr: 0.14773
2024-08-21 22:10:45.670 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][63/77] Data 0.034 (0.028) Batch 0.072 (0.067) Remain 00:00:06 loss: 0.0885 data: -0.0039 Lr: 0.14773
2024-08-21 22:10:45.735 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][64/77] Data 0.027 (0.027) Batch 0.065 (0.067) Remain 00:00:06 loss: 0.1127 data: 0.0087 Lr: 0.14610
2024-08-21 22:10:45.735 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][64/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:06 loss: 0.1127 data: -0.0002 Lr: 0.14610
2024-08-21 22:10:45.819 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][65/77] Data 0.037 (0.027) Batch 0.084 (0.068) Remain 00:00:06 loss: 0.2040 data: 0.0106 Lr: 0.14448
2024-08-21 22:10:45.819 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][65/77] Data 0.028 (0.028) Batch 0.084 (0.068) Remain 00:00:06 loss: 0.2040 data: 0.0039 Lr: 0.14448
2024-08-21 22:10:45.899 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][66/77] Data 0.031 (0.027) Batch 0.080 (0.068) Remain 00:00:06 loss: 0.0642 data: -0.0167 Lr: 0.14286
2024-08-21 22:10:45.899 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][66/77] Data 0.027 (0.028) Batch 0.080 (0.068) Remain 00:00:06 loss: 0.0642 data: -0.0019 Lr: 0.14286
2024-08-21 22:10:45.964 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][67/77] Data 0.027 (0.027) Batch 0.065 (0.068) Remain 00:00:05 loss: 0.0319 data: -0.0091 Lr: 0.14123
2024-08-21 22:10:45.964 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][67/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:05 loss: 0.0319 data: -0.0088 Lr: 0.14123
2024-08-21 22:10:46.031 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][68/77] Data 0.027 (0.027) Batch 0.067 (0.068) Remain 00:00:05 loss: 0.0986 data: 0.0082 Lr: 0.13961
2024-08-21 22:10:46.031 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][68/77] Data 0.027 (0.028) Batch 0.067 (0.068) Remain 00:00:05 loss: 0.0986 data: -0.0022 Lr: 0.13961
2024-08-21 22:10:46.098 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][69/77] Data 0.028 (0.027) Batch 0.067 (0.068) Remain 00:00:05 loss: 0.0470 data: -0.0219 Lr: 0.13799
2024-08-21 22:10:46.098 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][69/77] Data 0.029 (0.028) Batch 0.067 (0.068) Remain 00:00:05 loss: 0.0470 data: 0.0064 Lr: 0.13799
2024-08-21 22:10:46.167 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][70/77] Data 0.027 (0.027) Batch 0.070 (0.068) Remain 00:00:05 loss: 0.0670 data: -0.0131 Lr: 0.13636
2024-08-21 22:10:46.168 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][70/77] Data 0.027 (0.028) Batch 0.070 (0.068) Remain 00:00:05 loss: 0.0670 data: -0.0016 Lr: 0.13636
2024-08-21 22:10:46.245 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][71/77] Data 0.028 (0.027) Batch 0.078 (0.068) Remain 00:00:05 loss: 0.0962 data: -0.0031 Lr: 0.13474
2024-08-21 22:10:46.245 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][71/77] Data 0.036 (0.028) Batch 0.078 (0.068) Remain 00:00:05 loss: 0.0962 data: 0.0018 Lr: 0.13474
2024-08-21 22:10:46.326 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][72/77] Data 0.028 (0.027) Batch 0.081 (0.068) Remain 00:00:05 loss: 0.0792 data: 0.0094 Lr: 0.13312
2024-08-21 22:10:46.326 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][72/77] Data 0.039 (0.028) Batch 0.081 (0.068) Remain 00:00:05 loss: 0.0792 data: 0.0098 Lr: 0.13312
2024-08-21 22:10:46.391 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][73/77] Data 0.027 (0.027) Batch 0.065 (0.068) Remain 00:00:05 loss: 0.1175 data: 0.0064 Lr: 0.13149
2024-08-21 22:10:46.391 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][73/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:05 loss: 0.1175 data: 0.0076 Lr: 0.13149
2024-08-21 22:10:46.456 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][74/77] Data 0.027 (0.027) Batch 0.065 (0.068) Remain 00:00:05 loss: 0.0545 data: 0.0072 Lr: 0.12987
2024-08-21 22:10:46.456 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][74/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:05 loss: 0.0545 data: -0.0042 Lr: 0.12987
2024-08-21 22:10:46.524 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][75/77] Data 0.027 (0.027) Batch 0.068 (0.068) Remain 00:00:05 loss: 0.0884 data: -0.0094 Lr: 0.12825
2024-08-21 22:10:46.524 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][75/77] Data 0.027 (0.028) Batch 0.068 (0.068) Remain 00:00:05 loss: 0.0884 data: 0.0026 Lr: 0.12825
2024-08-21 22:10:46.594 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][76/77] Data 0.028 (0.027) Batch 0.070 (0.068) Remain 00:00:05 loss: 0.0836 data: 0.0090 Lr: 0.12662
2024-08-21 22:10:46.594 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][76/77] Data 0.028 (0.028) Batch 0.070 (0.068) Remain 00:00:05 loss: 0.0836 data: -0.0027 Lr: 0.12662
2024-08-21 22:10:46.639 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][77/77] Data 0.031 (0.027) Batch 0.045 (0.068) Remain 00:00:05 loss: 0.0979 data: 0.0192 Lr: 0.12500
2024-08-21 22:10:46.640 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 22:10:46.639 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][77/77] Data 0.031 (0.028) Batch 0.045 (0.068) Remain 00:00:05 loss: 0.0979 data: -0.0109 Lr: 0.12500
2024-08-21 22:10:46.640 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 22:10:51.276 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0307, Accuracy: 0.9893
2024-08-21 22:10:51.276 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0307, Accuracy: 0.9893
2024-08-21 22:10:51.276 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 22:10:51.276 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 22:10:51.276 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 22:10:51.276 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 22:10:51.371 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][1/77] Data 0.043 (0.043) Batch 0.094 (0.094) Remain 00:00:07 loss: 0.0906 data: -0.0011 Lr: 0.12338
2024-08-21 22:10:51.371 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][1/77] Data 0.055 (0.055) Batch 0.094 (0.094) Remain 00:00:07 loss: 0.0906 data: 0.0001 Lr: 0.12338
2024-08-21 22:10:51.434 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][2/77] Data 0.021 (0.021) Batch 0.063 (0.063) Remain 00:00:04 loss: 0.0946 data: 0.0130 Lr: 0.12175
2024-08-21 22:10:51.434 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][2/77] Data 0.027 (0.027) Batch 0.063 (0.063) Remain 00:00:04 loss: 0.0946 data: 0.0024 Lr: 0.12175
2024-08-21 22:10:51.497 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][3/77] Data 0.021 (0.021) Batch 0.063 (0.063) Remain 00:00:04 loss: 0.0555 data: -0.0149 Lr: 0.12013
2024-08-21 22:10:51.497 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][3/77] Data 0.027 (0.027) Batch 0.063 (0.063) Remain 00:00:04 loss: 0.0555 data: -0.0094 Lr: 0.12013
2024-08-21 22:10:51.560 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][4/77] Data 0.021 (0.021) Batch 0.063 (0.063) Remain 00:00:04 loss: 0.1143 data: 0.0154 Lr: 0.11851
2024-08-21 22:10:51.560 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][4/77] Data 0.027 (0.027) Batch 0.063 (0.063) Remain 00:00:04 loss: 0.1143 data: 0.0173 Lr: 0.11851
2024-08-21 22:10:51.624 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][5/77] Data 0.022 (0.021) Batch 0.064 (0.063) Remain 00:00:04 loss: 0.0616 data: 0.0002 Lr: 0.11688
2024-08-21 22:10:51.624 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][5/77] Data 0.027 (0.027) Batch 0.064 (0.063) Remain 00:00:04 loss: 0.0616 data: 0.0112 Lr: 0.11688
2024-08-21 22:10:51.687 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][6/77] Data 0.022 (0.021) Batch 0.063 (0.063) Remain 00:00:04 loss: 0.0685 data: -0.0313 Lr: 0.11526
2024-08-21 22:10:51.687 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][6/77] Data 0.027 (0.027) Batch 0.063 (0.063) Remain 00:00:04 loss: 0.0685 data: -0.0011 Lr: 0.11526
2024-08-21 22:10:51.750 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][7/77] Data 0.022 (0.022) Batch 0.063 (0.063) Remain 00:00:04 loss: 0.1434 data: -0.0014 Lr: 0.11364
2024-08-21 22:10:51.750 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][7/77] Data 0.027 (0.027) Batch 0.063 (0.063) Remain 00:00:04 loss: 0.1434 data: 0.0004 Lr: 0.11364
2024-08-21 22:10:51.813 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][8/77] Data 0.021 (0.022) Batch 0.063 (0.063) Remain 00:00:04 loss: 0.0777 data: 0.0011 Lr: 0.11201
2024-08-21 22:10:51.813 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][8/77] Data 0.027 (0.027) Batch 0.063 (0.063) Remain 00:00:04 loss: 0.0777 data: -0.0148 Lr: 0.11201
2024-08-21 22:10:51.875 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][9/77] Data 0.021 (0.021) Batch 0.063 (0.063) Remain 00:00:04 loss: 0.0951 data: 0.0141 Lr: 0.11039
2024-08-21 22:10:51.875 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][9/77] Data 0.027 (0.027) Batch 0.063 (0.063) Remain 00:00:04 loss: 0.0951 data: 0.0075 Lr: 0.11039
2024-08-21 22:10:51.940 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][10/77] Data 0.022 (0.021) Batch 0.064 (0.063) Remain 00:00:04 loss: 0.1325 data: 0.0063 Lr: 0.10877
2024-08-21 22:10:51.940 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][10/77] Data 0.027 (0.027) Batch 0.064 (0.063) Remain 00:00:04 loss: 0.1325 data: -0.0173 Lr: 0.10877
2024-08-21 22:10:52.003 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][11/77] Data 0.021 (0.021) Batch 0.063 (0.063) Remain 00:00:04 loss: 0.0573 data: 0.0060 Lr: 0.10714
2024-08-21 22:10:52.003 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_550
2024-08-21 22:10:52.003 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][11/77] Data 0.027 (0.027) Batch 0.063 (0.063) Remain 00:00:04 loss: 0.0573 data: 0.0108 Lr: 0.10714
2024-08-21 22:10:52.003 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_550
2024-08-21 22:10:52.026 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -575.1599731445312
2024-08-21 22:10:52.026 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -275.3646545410156
2024-08-21 22:10:52.026 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -575.1599731445312
2024-08-21 22:10:52.026 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -299.7952880859375
2024-08-21 22:10:52.089 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][12/77] Data 0.044 (0.023) Batch 0.086 (0.065) Remain 00:00:04 loss: 0.0684 data: -0.0053 Lr: 0.10552
2024-08-21 22:10:52.089 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][12/77] Data 0.051 (0.029) Batch 0.086 (0.065) Remain 00:00:04 loss: 0.0684 data: 0.0054 Lr: 0.10552
2024-08-21 22:10:52.152 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][13/77] Data 0.021 (0.023) Batch 0.064 (0.065) Remain 00:00:04 loss: 0.0912 data: -0.0085 Lr: 0.10390
2024-08-21 22:10:52.153 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][13/77] Data 0.027 (0.029) Batch 0.064 (0.065) Remain 00:00:04 loss: 0.0912 data: 0.0079 Lr: 0.10390
2024-08-21 22:10:52.216 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][14/77] Data 0.021 (0.023) Batch 0.063 (0.065) Remain 00:00:04 loss: 0.0945 data: 0.0011 Lr: 0.10227
2024-08-21 22:10:52.216 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][14/77] Data 0.027 (0.029) Batch 0.063 (0.065) Remain 00:00:04 loss: 0.0945 data: 0.0143 Lr: 0.10227
2024-08-21 22:10:52.280 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][15/77] Data 0.022 (0.023) Batch 0.064 (0.065) Remain 00:00:04 loss: 0.0676 data: -0.0009 Lr: 0.10065
2024-08-21 22:10:52.280 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][15/77] Data 0.027 (0.029) Batch 0.064 (0.065) Remain 00:00:04 loss: 0.0676 data: 0.0073 Lr: 0.10065
2024-08-21 22:10:52.344 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][16/77] Data 0.022 (0.023) Batch 0.064 (0.065) Remain 00:00:04 loss: 0.1014 data: 0.0100 Lr: 0.09903
2024-08-21 22:10:52.344 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][16/77] Data 0.027 (0.028) Batch 0.064 (0.065) Remain 00:00:04 loss: 0.1014 data: -0.0112 Lr: 0.09903
2024-08-21 22:10:52.408 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][17/77] Data 0.021 (0.023) Batch 0.064 (0.065) Remain 00:00:03 loss: 0.0734 data: -0.0082 Lr: 0.09740
2024-08-21 22:10:52.408 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][17/77] Data 0.027 (0.028) Batch 0.064 (0.065) Remain 00:00:03 loss: 0.0734 data: 0.0017 Lr: 0.09740
2024-08-21 22:10:52.471 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][18/77] Data 0.023 (0.023) Batch 0.063 (0.065) Remain 00:00:03 loss: 0.1426 data: 0.0112 Lr: 0.09578
2024-08-21 22:10:52.472 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][18/77] Data 0.027 (0.028) Batch 0.063 (0.065) Remain 00:00:03 loss: 0.1426 data: -0.0106 Lr: 0.09578
2024-08-21 22:10:52.535 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][19/77] Data 0.023 (0.023) Batch 0.064 (0.065) Remain 00:00:03 loss: 0.0529 data: 0.0047 Lr: 0.09416
2024-08-21 22:10:52.535 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][19/77] Data 0.027 (0.028) Batch 0.064 (0.065) Remain 00:00:03 loss: 0.0529 data: -0.0053 Lr: 0.09416
2024-08-21 22:10:52.599 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][20/77] Data 0.022 (0.023) Batch 0.063 (0.065) Remain 00:00:03 loss: 0.1903 data: 0.0030 Lr: 0.09253
2024-08-21 22:10:52.599 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][20/77] Data 0.027 (0.028) Batch 0.063 (0.065) Remain 00:00:03 loss: 0.1903 data: -0.0103 Lr: 0.09253
2024-08-21 22:10:52.661 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][21/77] Data 0.022 (0.023) Batch 0.063 (0.065) Remain 00:00:03 loss: 0.1352 data: -0.0080 Lr: 0.09091
2024-08-21 22:10:52.662 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][21/77] Data 0.027 (0.028) Batch 0.063 (0.065) Remain 00:00:03 loss: 0.1352 data: 0.0014 Lr: 0.09091
2024-08-21 22:10:52.724 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][22/77] Data 0.021 (0.023) Batch 0.062 (0.064) Remain 00:00:03 loss: 0.1050 data: 0.0072 Lr: 0.08929
2024-08-21 22:10:52.724 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][22/77] Data 0.027 (0.028) Batch 0.062 (0.064) Remain 00:00:03 loss: 0.1050 data: 0.0074 Lr: 0.08929
2024-08-21 22:10:52.788 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][23/77] Data 0.021 (0.023) Batch 0.064 (0.064) Remain 00:00:03 loss: 0.1044 data: -0.0051 Lr: 0.08766
2024-08-21 22:10:52.788 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][23/77] Data 0.027 (0.028) Batch 0.064 (0.064) Remain 00:00:03 loss: 0.1044 data: 0.0091 Lr: 0.08766
2024-08-21 22:10:52.851 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][24/77] Data 0.022 (0.023) Batch 0.063 (0.064) Remain 00:00:03 loss: 0.0748 data: 0.0033 Lr: 0.08604
2024-08-21 22:10:52.851 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][24/77] Data 0.027 (0.028) Batch 0.063 (0.064) Remain 00:00:03 loss: 0.0748 data: 0.0050 Lr: 0.08604
2024-08-21 22:10:52.913 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][25/77] Data 0.021 (0.022) Batch 0.062 (0.064) Remain 00:00:03 loss: 0.0584 data: 0.0152 Lr: 0.08442
2024-08-21 22:10:52.913 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][25/77] Data 0.027 (0.028) Batch 0.062 (0.064) Remain 00:00:03 loss: 0.0584 data: 0.0067 Lr: 0.08442
2024-08-21 22:10:52.977 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][26/77] Data 0.021 (0.022) Batch 0.065 (0.064) Remain 00:00:03 loss: 0.0929 data: -0.0023 Lr: 0.08279
2024-08-21 22:10:52.977 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][26/77] Data 0.027 (0.028) Batch 0.064 (0.064) Remain 00:00:03 loss: 0.0929 data: 0.0008 Lr: 0.08279
2024-08-21 22:10:53.041 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][27/77] Data 0.022 (0.022) Batch 0.063 (0.064) Remain 00:00:03 loss: 0.1141 data: 0.0118 Lr: 0.08117
2024-08-21 22:10:53.041 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][27/77] Data 0.027 (0.028) Batch 0.063 (0.064) Remain 00:00:03 loss: 0.1141 data: 0.0005 Lr: 0.08117
2024-08-21 22:10:53.104 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][28/77] Data 0.021 (0.022) Batch 0.063 (0.064) Remain 00:00:03 loss: 0.0721 data: -0.0045 Lr: 0.07955
2024-08-21 22:10:53.104 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][28/77] Data 0.027 (0.028) Batch 0.063 (0.064) Remain 00:00:03 loss: 0.0721 data: 0.0051 Lr: 0.07955
2024-08-21 22:10:53.166 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][29/77] Data 0.022 (0.022) Batch 0.062 (0.064) Remain 00:00:03 loss: 0.0944 data: -0.0048 Lr: 0.07792
2024-08-21 22:10:53.166 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][29/77] Data 0.027 (0.028) Batch 0.062 (0.064) Remain 00:00:03 loss: 0.0944 data: 0.0000 Lr: 0.07792
2024-08-21 22:10:53.228 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][30/77] Data 0.021 (0.022) Batch 0.062 (0.064) Remain 00:00:03 loss: 0.1085 data: 0.0166 Lr: 0.07630
2024-08-21 22:10:53.229 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][30/77] Data 0.027 (0.028) Batch 0.062 (0.064) Remain 00:00:03 loss: 0.1085 data: 0.0034 Lr: 0.07630
2024-08-21 22:10:53.291 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][31/77] Data 0.022 (0.022) Batch 0.063 (0.064) Remain 00:00:03 loss: 0.1180 data: 0.0001 Lr: 0.07468
2024-08-21 22:10:53.291 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][31/77] Data 0.027 (0.028) Batch 0.063 (0.064) Remain 00:00:03 loss: 0.1180 data: -0.0229 Lr: 0.07468
2024-08-21 22:10:53.355 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][32/77] Data 0.022 (0.022) Batch 0.064 (0.064) Remain 00:00:02 loss: 0.0626 data: 0.0063 Lr: 0.07305
2024-08-21 22:10:53.355 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][32/77] Data 0.027 (0.028) Batch 0.064 (0.064) Remain 00:00:02 loss: 0.0626 data: 0.0009 Lr: 0.07305
2024-08-21 22:10:53.419 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][33/77] Data 0.027 (0.022) Batch 0.063 (0.064) Remain 00:00:02 loss: 0.1022 data: -0.0038 Lr: 0.07143
2024-08-21 22:10:53.419 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][33/77] Data 0.027 (0.028) Batch 0.063 (0.064) Remain 00:00:02 loss: 0.1022 data: -0.0264 Lr: 0.07143
2024-08-21 22:10:53.484 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][34/77] Data 0.027 (0.023) Batch 0.065 (0.064) Remain 00:00:02 loss: 0.1016 data: -0.0071 Lr: 0.06981
2024-08-21 22:10:53.484 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][34/77] Data 0.027 (0.028) Batch 0.065 (0.064) Remain 00:00:02 loss: 0.1016 data: -0.0025 Lr: 0.06981
2024-08-21 22:10:53.554 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][35/77] Data 0.028 (0.023) Batch 0.070 (0.064) Remain 00:00:02 loss: 0.1174 data: -0.0320 Lr: 0.06818
2024-08-21 22:10:53.554 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][35/77] Data 0.028 (0.028) Batch 0.070 (0.064) Remain 00:00:02 loss: 0.1174 data: 0.0015 Lr: 0.06818
2024-08-21 22:10:53.621 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][36/77] Data 0.028 (0.023) Batch 0.067 (0.064) Remain 00:00:02 loss: 0.1199 data: 0.0079 Lr: 0.06656
2024-08-21 22:10:53.621 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][36/77] Data 0.029 (0.028) Batch 0.067 (0.064) Remain 00:00:02 loss: 0.1199 data: -0.0123 Lr: 0.06656
2024-08-21 22:10:53.687 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][37/77] Data 0.027 (0.023) Batch 0.067 (0.064) Remain 00:00:02 loss: 0.1229 data: 0.0035 Lr: 0.06494
2024-08-21 22:10:53.688 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][37/77] Data 0.027 (0.028) Batch 0.067 (0.064) Remain 00:00:02 loss: 0.1229 data: 0.0167 Lr: 0.06494
2024-08-21 22:10:53.752 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][38/77] Data 0.027 (0.023) Batch 0.065 (0.064) Remain 00:00:02 loss: 0.0782 data: 0.0007 Lr: 0.06331
2024-08-21 22:10:53.753 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][38/77] Data 0.027 (0.028) Batch 0.065 (0.064) Remain 00:00:02 loss: 0.0782 data: -0.0054 Lr: 0.06331
2024-08-21 22:10:53.818 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][39/77] Data 0.027 (0.023) Batch 0.066 (0.064) Remain 00:00:02 loss: 0.0807 data: 0.0020 Lr: 0.06169
2024-08-21 22:10:53.819 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][39/77] Data 0.027 (0.028) Batch 0.066 (0.064) Remain 00:00:02 loss: 0.0807 data: -0.0102 Lr: 0.06169
2024-08-21 22:10:53.884 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][40/77] Data 0.027 (0.023) Batch 0.066 (0.064) Remain 00:00:02 loss: 0.1232 data: -0.0045 Lr: 0.06006
2024-08-21 22:10:53.884 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][40/77] Data 0.027 (0.028) Batch 0.066 (0.064) Remain 00:00:02 loss: 0.1232 data: -0.0030 Lr: 0.06006
2024-08-21 22:10:53.949 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][41/77] Data 0.027 (0.023) Batch 0.065 (0.064) Remain 00:00:02 loss: 0.0684 data: 0.0053 Lr: 0.05844
2024-08-21 22:10:53.949 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][41/77] Data 0.027 (0.028) Batch 0.065 (0.064) Remain 00:00:02 loss: 0.0684 data: -0.0008 Lr: 0.05844
2024-08-21 22:10:54.015 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][42/77] Data 0.027 (0.023) Batch 0.066 (0.064) Remain 00:00:02 loss: 0.0531 data: -0.0071 Lr: 0.05682
2024-08-21 22:10:54.015 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][42/77] Data 0.027 (0.028) Batch 0.066 (0.064) Remain 00:00:02 loss: 0.0531 data: 0.0082 Lr: 0.05682
2024-08-21 22:10:54.081 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][43/77] Data 0.027 (0.024) Batch 0.066 (0.065) Remain 00:00:02 loss: 0.0690 data: -0.0066 Lr: 0.05519
2024-08-21 22:10:54.081 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][43/77] Data 0.027 (0.028) Batch 0.066 (0.065) Remain 00:00:02 loss: 0.0690 data: -0.0112 Lr: 0.05519
2024-08-21 22:10:54.148 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][44/77] Data 0.027 (0.024) Batch 0.067 (0.065) Remain 00:00:02 loss: 0.0501 data: 0.0052 Lr: 0.05357
2024-08-21 22:10:54.148 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][44/77] Data 0.027 (0.028) Batch 0.067 (0.065) Remain 00:00:02 loss: 0.0501 data: -0.0066 Lr: 0.05357
2024-08-21 22:10:54.218 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][45/77] Data 0.028 (0.024) Batch 0.070 (0.065) Remain 00:00:02 loss: 0.1018 data: 0.0016 Lr: 0.05195
2024-08-21 22:10:54.218 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][45/77] Data 0.028 (0.028) Batch 0.070 (0.065) Remain 00:00:02 loss: 0.1018 data: 0.0145 Lr: 0.05195
2024-08-21 22:10:54.290 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][46/77] Data 0.029 (0.024) Batch 0.072 (0.065) Remain 00:00:02 loss: 0.0192 data: -0.0031 Lr: 0.05032
2024-08-21 22:10:54.290 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][46/77] Data 0.029 (0.028) Batch 0.072 (0.065) Remain 00:00:02 loss: 0.0192 data: -0.0037 Lr: 0.05032
2024-08-21 22:10:54.360 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][47/77] Data 0.028 (0.024) Batch 0.071 (0.065) Remain 00:00:02 loss: 0.0763 data: 0.0088 Lr: 0.04870
2024-08-21 22:10:54.361 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][47/77] Data 0.028 (0.028) Batch 0.071 (0.065) Remain 00:00:02 loss: 0.0763 data: 0.0097 Lr: 0.04870
2024-08-21 22:10:54.428 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][48/77] Data 0.028 (0.024) Batch 0.067 (0.065) Remain 00:00:01 loss: 0.0816 data: -0.0063 Lr: 0.04708
2024-08-21 22:10:54.428 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][48/77] Data 0.028 (0.028) Batch 0.067 (0.065) Remain 00:00:01 loss: 0.0816 data: 0.0044 Lr: 0.04708
2024-08-21 22:10:54.493 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][49/77] Data 0.027 (0.024) Batch 0.066 (0.065) Remain 00:00:01 loss: 0.0891 data: 0.0184 Lr: 0.04545
2024-08-21 22:10:54.494 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][49/77] Data 0.027 (0.028) Batch 0.066 (0.065) Remain 00:00:01 loss: 0.0891 data: 0.0028 Lr: 0.04545
2024-08-21 22:10:54.559 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][50/77] Data 0.027 (0.024) Batch 0.065 (0.065) Remain 00:00:01 loss: 0.0717 data: 0.0187 Lr: 0.04383
2024-08-21 22:10:54.559 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][50/77] Data 0.027 (0.028) Batch 0.065 (0.065) Remain 00:00:01 loss: 0.0717 data: -0.0056 Lr: 0.04383
2024-08-21 22:10:54.625 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][51/77] Data 0.027 (0.024) Batch 0.066 (0.065) Remain 00:00:01 loss: 0.1434 data: -0.0055 Lr: 0.04221
2024-08-21 22:10:54.625 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][51/77] Data 0.027 (0.028) Batch 0.066 (0.065) Remain 00:00:01 loss: 0.1434 data: 0.0066 Lr: 0.04221
2024-08-21 22:10:54.691 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][52/77] Data 0.027 (0.024) Batch 0.066 (0.065) Remain 00:00:01 loss: 0.1513 data: -0.0154 Lr: 0.04058
2024-08-21 22:10:54.691 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][52/77] Data 0.027 (0.028) Batch 0.066 (0.065) Remain 00:00:01 loss: 0.1513 data: 0.0024 Lr: 0.04058
2024-08-21 22:10:54.757 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][53/77] Data 0.027 (0.024) Batch 0.066 (0.065) Remain 00:00:01 loss: 0.0986 data: 0.0010 Lr: 0.03896
2024-08-21 22:10:54.757 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][53/77] Data 0.027 (0.028) Batch 0.066 (0.065) Remain 00:00:01 loss: 0.0986 data: 0.0070 Lr: 0.03896
2024-08-21 22:10:54.822 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][54/77] Data 0.027 (0.024) Batch 0.065 (0.065) Remain 00:00:01 loss: 0.1023 data: -0.0035 Lr: 0.03734
2024-08-21 22:10:54.822 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][54/77] Data 0.027 (0.028) Batch 0.065 (0.065) Remain 00:00:01 loss: 0.1023 data: -0.0023 Lr: 0.03734
2024-08-21 22:10:54.885 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][55/77] Data 0.027 (0.024) Batch 0.063 (0.065) Remain 00:00:01 loss: 0.1139 data: 0.0044 Lr: 0.03571
2024-08-21 22:10:54.885 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][55/77] Data 0.027 (0.028) Batch 0.063 (0.065) Remain 00:00:01 loss: 0.1139 data: 0.0008 Lr: 0.03571
2024-08-21 22:10:54.949 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][56/77] Data 0.021 (0.024) Batch 0.064 (0.065) Remain 00:00:01 loss: 0.0927 data: -0.0034 Lr: 0.03409
2024-08-21 22:10:54.949 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][56/77] Data 0.027 (0.028) Batch 0.064 (0.065) Remain 00:00:01 loss: 0.0927 data: -0.0045 Lr: 0.03409
2024-08-21 22:10:55.013 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][57/77] Data 0.022 (0.024) Batch 0.065 (0.065) Remain 00:00:01 loss: 0.0896 data: 0.0192 Lr: 0.03247
2024-08-21 22:10:55.014 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][57/77] Data 0.027 (0.028) Batch 0.065 (0.065) Remain 00:00:01 loss: 0.0896 data: -0.0022 Lr: 0.03247
2024-08-21 22:10:55.078 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][58/77] Data 0.021 (0.024) Batch 0.065 (0.065) Remain 00:00:01 loss: 0.1056 data: -0.0038 Lr: 0.03084
2024-08-21 22:10:55.078 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][58/77] Data 0.027 (0.028) Batch 0.065 (0.065) Remain 00:00:01 loss: 0.1056 data: 0.0035 Lr: 0.03084
2024-08-21 22:10:55.143 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][59/77] Data 0.022 (0.024) Batch 0.065 (0.065) Remain 00:00:01 loss: 0.0720 data: -0.0079 Lr: 0.02922
2024-08-21 22:10:55.143 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][59/77] Data 0.027 (0.028) Batch 0.065 (0.065) Remain 00:00:01 loss: 0.0720 data: -0.0107 Lr: 0.02922
2024-08-21 22:10:55.208 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][60/77] Data 0.023 (0.024) Batch 0.065 (0.065) Remain 00:00:01 loss: 0.0607 data: -0.0010 Lr: 0.02760
2024-08-21 22:10:55.208 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][60/77] Data 0.027 (0.028) Batch 0.065 (0.065) Remain 00:00:01 loss: 0.0607 data: -0.0031 Lr: 0.02760
2024-08-21 22:10:55.274 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][61/77] Data 0.021 (0.024) Batch 0.066 (0.065) Remain 00:00:01 loss: 0.0762 data: -0.0009 Lr: 0.02597
2024-08-21 22:10:55.274 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_600
2024-08-21 22:10:55.274 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][61/77] Data 0.027 (0.028) Batch 0.066 (0.065) Remain 00:00:01 loss: 0.0762 data: -0.0113 Lr: 0.02597
2024-08-21 22:10:55.274 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_600
2024-08-21 22:10:55.298 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -580.1256103515625
2024-08-21 22:10:55.299 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -279.2394104003906
2024-08-21 22:10:55.299 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -580.1256103515625
2024-08-21 22:10:55.299 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -300.88629150390625
2024-08-21 22:10:55.365 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][62/77] Data 0.046 (0.025) Batch 0.091 (0.065) Remain 00:00:01 loss: 0.0699 data: -0.0007 Lr: 0.02435
2024-08-21 22:10:55.365 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][62/77] Data 0.052 (0.028) Batch 0.091 (0.065) Remain 00:00:01 loss: 0.0699 data: -0.0043 Lr: 0.02435
2024-08-21 22:10:55.430 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][63/77] Data 0.021 (0.024) Batch 0.066 (0.065) Remain 00:00:00 loss: 0.0694 data: -0.0075 Lr: 0.02273
2024-08-21 22:10:55.431 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][63/77] Data 0.027 (0.028) Batch 0.066 (0.065) Remain 00:00:00 loss: 0.0694 data: 0.0052 Lr: 0.02273
2024-08-21 22:10:55.497 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][64/77] Data 0.021 (0.024) Batch 0.066 (0.065) Remain 00:00:00 loss: 0.1336 data: 0.0079 Lr: 0.02110
2024-08-21 22:10:55.497 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][64/77] Data 0.024 (0.028) Batch 0.066 (0.065) Remain 00:00:00 loss: 0.1336 data: 0.0068 Lr: 0.02110
2024-08-21 22:10:55.562 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][65/77] Data 0.027 (0.024) Batch 0.066 (0.065) Remain 00:00:00 loss: 0.0690 data: -0.0062 Lr: 0.01948
2024-08-21 22:10:55.562 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][65/77] Data 0.022 (0.028) Batch 0.066 (0.065) Remain 00:00:00 loss: 0.0690 data: 0.0012 Lr: 0.01948
2024-08-21 22:10:55.627 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][66/77] Data 0.027 (0.024) Batch 0.065 (0.065) Remain 00:00:00 loss: 0.0547 data: 0.0081 Lr: 0.01786
2024-08-21 22:10:55.627 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][66/77] Data 0.022 (0.028) Batch 0.065 (0.065) Remain 00:00:00 loss: 0.0547 data: -0.0030 Lr: 0.01786
2024-08-21 22:10:55.691 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][67/77] Data 0.027 (0.025) Batch 0.064 (0.065) Remain 00:00:00 loss: 0.0716 data: -0.0052 Lr: 0.01623
2024-08-21 22:10:55.691 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][67/77] Data 0.021 (0.028) Batch 0.064 (0.065) Remain 00:00:00 loss: 0.0716 data: -0.0037 Lr: 0.01623
2024-08-21 22:10:55.755 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][68/77] Data 0.027 (0.025) Batch 0.064 (0.065) Remain 00:00:00 loss: 0.0907 data: -0.0219 Lr: 0.01461
2024-08-21 22:10:55.755 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][68/77] Data 0.022 (0.027) Batch 0.064 (0.065) Remain 00:00:00 loss: 0.0907 data: 0.0103 Lr: 0.01461
2024-08-21 22:10:55.818 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][69/77] Data 0.027 (0.025) Batch 0.064 (0.065) Remain 00:00:00 loss: 0.0541 data: -0.0128 Lr: 0.01299
2024-08-21 22:10:55.818 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][69/77] Data 0.022 (0.027) Batch 0.064 (0.065) Remain 00:00:00 loss: 0.0541 data: -0.0022 Lr: 0.01299
2024-08-21 22:10:55.883 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][70/77] Data 0.027 (0.025) Batch 0.065 (0.065) Remain 00:00:00 loss: 0.1305 data: -0.0059 Lr: 0.01136
2024-08-21 22:10:55.883 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][70/77] Data 0.022 (0.027) Batch 0.065 (0.065) Remain 00:00:00 loss: 0.1305 data: -0.0035 Lr: 0.01136
2024-08-21 22:10:55.948 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][71/77] Data 0.027 (0.025) Batch 0.065 (0.065) Remain 00:00:00 loss: 0.0479 data: -0.0177 Lr: 0.00974
2024-08-21 22:10:55.948 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][71/77] Data 0.021 (0.027) Batch 0.065 (0.065) Remain 00:00:00 loss: 0.0479 data: 0.0189 Lr: 0.00974
2024-08-21 22:10:56.013 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][72/77] Data 0.027 (0.025) Batch 0.065 (0.065) Remain 00:00:00 loss: 0.0836 data: 0.0072 Lr: 0.00812
2024-08-21 22:10:56.013 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][72/77] Data 0.022 (0.027) Batch 0.065 (0.065) Remain 00:00:00 loss: 0.0836 data: -0.0186 Lr: 0.00812
2024-08-21 22:10:56.078 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][73/77] Data 0.027 (0.025) Batch 0.065 (0.065) Remain 00:00:00 loss: 0.1636 data: -0.0020 Lr: 0.00649
2024-08-21 22:10:56.078 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][73/77] Data 0.022 (0.027) Batch 0.065 (0.065) Remain 00:00:00 loss: 0.1636 data: -0.0192 Lr: 0.00649
2024-08-21 22:10:56.142 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][74/77] Data 0.027 (0.025) Batch 0.064 (0.065) Remain 00:00:00 loss: 0.1586 data: 0.0071 Lr: 0.00487
2024-08-21 22:10:56.142 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][74/77] Data 0.021 (0.027) Batch 0.064 (0.065) Remain 00:00:00 loss: 0.1586 data: -0.0122 Lr: 0.00487
2024-08-21 22:10:56.205 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][75/77] Data 0.027 (0.025) Batch 0.063 (0.065) Remain 00:00:00 loss: 0.1333 data: 0.0052 Lr: 0.00325
2024-08-21 22:10:56.206 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][75/77] Data 0.021 (0.027) Batch 0.063 (0.065) Remain 00:00:00 loss: 0.1333 data: 0.0056 Lr: 0.00325
2024-08-21 22:10:56.270 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][76/77] Data 0.027 (0.025) Batch 0.064 (0.065) Remain 00:00:00 loss: 0.1181 data: -0.0066 Lr: 0.00162
2024-08-21 22:10:56.270 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][76/77] Data 0.021 (0.027) Batch 0.064 (0.065) Remain 00:00:00 loss: 0.1181 data: 0.0208 Lr: 0.00162
2024-08-21 22:10:56.309 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][77/77] Data 0.029 (0.025) Batch 0.039 (0.065) Remain 00:00:00 loss: 0.0676 data: 0.0074 Lr: 0.00000
2024-08-21 22:10:56.309 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][77/77] Data 0.023 (0.027) Batch 0.039 (0.065) Remain 00:00:00 loss: 0.0676 data: 0.0015 Lr: 0.00000
2024-08-21 22:10:56.309 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 22:10:56.309 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 22:10:59.750 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0296, Accuracy: 0.9896
2024-08-21 22:10:59.750 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0296, Accuracy: 0.9896
2024-08-21 22:10:59.750 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 22:10:59.750 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 22:10:59.750 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 22:10:59.750 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 22:10:59.750 | INFO     | trim.callbacks.evaluator:on_training_phase_end:49 - Best mIoU: 0.9896, epoch at  7
2024-08-21 22:10:59.751 | INFO     | trim.callbacks.evaluator:on_training_phase_end:49 - Best mIoU: 0.9896, epoch at  7
