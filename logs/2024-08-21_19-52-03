2024-08-21 19:52:03.294 | INFO     | __main__:configure_model:71 - => creating model ...
2024-08-21 19:52:03.331 | INFO     | __main__:configure_model:74 - Number of parameters: 1199882
2024-08-21 19:52:03.762 | INFO     | __main__:configure_model:71 - => creating model ...
2024-08-21 19:52:03.800 | INFO     | __main__:configure_model:74 - Number of parameters: 1199882
2024-08-21 19:52:05.772 | INFO     | trim.callbacks.misc:resume:179 - => Resuming from checkpoint: output/step_50
2024-08-21 19:52:05.806 | INFO     | trim.callbacks.misc:resume:184 - ### Model weight: -223.38320922851562
2024-08-21 19:52:05.807 | INFO     | trim.callbacks.misc:resume:185 - ### Optimizer weight: -115.26531982421875
2024-08-21 19:52:05.807 | INFO     | trim.callbacks.misc:on_training_phase_start:70 - Namespace(block_size=None, checkpointing_steps='300', gamma=0.7, load_best_model=False, log_project='accl_test', log_tag='init_1', lr=1.0, lr_scheduler_type='linear', num_train_epochs=8, num_warmup_steps=0, output_dir='./output', per_device_eval_batch_size=1, per_device_train_batch_size=196, preprocessing_num_workers=None, report_to='all', resume_from_checkpoint='output/step_300', save_path='output/', seed=1, weight_decay=0.0, with_tracking=False)
2024-08-21 19:52:05.807 | INFO     | trim.engine.trainer:fit:125 - >>>>>>>>>>>>>>>> Start Training >>>>>>>>>>>>>>>>
2024-08-21 19:52:06.416 | INFO     | trim.callbacks.misc:resume:179 - => Resuming from checkpoint: output/step_50
2024-08-21 19:52:06.455 | INFO     | trim.callbacks.misc:resume:184 - ### Model weight: -223.38320922851562
2024-08-21 19:52:06.455 | INFO     | trim.callbacks.misc:resume:185 - ### Optimizer weight: -108.11787414550781
2024-08-21 19:52:06.456 | INFO     | trim.callbacks.misc:on_training_phase_start:70 - Namespace(block_size=None, checkpointing_steps='300', gamma=0.7, load_best_model=False, log_project='accl_test', log_tag='init_1', lr=1.0, lr_scheduler_type='linear', num_train_epochs=8, num_warmup_steps=0, output_dir='./output', per_device_eval_batch_size=1, per_device_train_batch_size=196, preprocessing_num_workers=None, report_to='all', resume_from_checkpoint='output/step_300', save_path='output/', seed=1, weight_decay=0.0, with_tracking=False)
2024-08-21 19:52:06.456 | INFO     | trim.engine.trainer:fit:125 - >>>>>>>>>>>>>>>> Start Training >>>>>>>>>>>>>>>>
2024-08-21 19:52:08.136 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][51/77] Data 0.067 (0.067) Batch 1.679 (1.679) Remain 00:15:50 loss: 0.4194 data: 0.0113 Lr: 0.91721
2024-08-21 19:52:08.136 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][51/77] Data 0.715 (0.715) Batch 2.328 (2.328) Remain 00:21:57 loss: 0.4194 data: 0.0059 Lr: 0.91721
2024-08-21 19:52:08.213 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][52/77] Data 0.030 (0.030) Batch 0.077 (0.077) Remain 00:00:43 loss: 0.4249 data: -0.0018 Lr: 0.91558
2024-08-21 19:52:08.214 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][52/77] Data 0.032 (0.032) Batch 0.078 (0.078) Remain 00:00:43 loss: 0.4249 data: 0.0255 Lr: 0.91558
2024-08-21 19:52:08.299 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][53/77] Data 0.038 (0.035) Batch 0.085 (0.081) Remain 00:00:45 loss: 0.4813 data: -0.0151 Lr: 0.91396
2024-08-21 19:52:08.299 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][53/77] Data 0.031 (0.031) Batch 0.087 (0.082) Remain 00:00:46 loss: 0.4813 data: 0.0226 Lr: 0.91396
2024-08-21 19:52:08.381 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][54/77] Data 0.029 (0.033) Batch 0.082 (0.082) Remain 00:00:45 loss: 0.3326 data: -0.0040 Lr: 0.91234
2024-08-21 19:52:08.381 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][54/77] Data 0.029 (0.030) Batch 0.082 (0.082) Remain 00:00:46 loss: 0.3326 data: -0.0010 Lr: 0.91234
2024-08-21 19:52:08.451 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][55/77] Data 0.028 (0.032) Batch 0.071 (0.079) Remain 00:00:44 loss: 0.3743 data: 0.0194 Lr: 0.91071
2024-08-21 19:52:08.452 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][55/77] Data 0.029 (0.030) Batch 0.071 (0.079) Remain 00:00:44 loss: 0.3743 data: 0.0157 Lr: 0.91071
2024-08-21 19:52:08.540 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][56/77] Data 0.038 (0.033) Batch 0.089 (0.081) Remain 00:00:45 loss: 0.3336 data: -0.0027 Lr: 0.90909
2024-08-21 19:52:08.540 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][56/77] Data 0.029 (0.030) Batch 0.089 (0.081) Remain 00:00:45 loss: 0.3336 data: -0.0060 Lr: 0.90909
2024-08-21 19:52:08.625 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][57/77] Data 0.029 (0.029) Batch 0.085 (0.082) Remain 00:00:45 loss: 0.3566 data: -0.0121 Lr: 0.90747
2024-08-21 19:52:08.625 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][57/77] Data 0.039 (0.034) Batch 0.085 (0.082) Remain 00:00:45 loss: 0.3566 data: -0.0072 Lr: 0.90747
2024-08-21 19:52:08.697 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][58/77] Data 0.028 (0.033) Batch 0.072 (0.080) Remain 00:00:44 loss: 0.3554 data: 0.0202 Lr: 0.90584
2024-08-21 19:52:08.697 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][58/77] Data 0.029 (0.029) Batch 0.072 (0.080) Remain 00:00:44 loss: 0.3554 data: -0.0011 Lr: 0.90584
2024-08-21 19:52:08.768 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][59/77] Data 0.029 (0.033) Batch 0.071 (0.079) Remain 00:00:44 loss: 0.3607 data: -0.0228 Lr: 0.90422
2024-08-21 19:52:08.768 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][59/77] Data 0.029 (0.029) Batch 0.071 (0.079) Remain 00:00:44 loss: 0.3607 data: -0.0205 Lr: 0.90422
2024-08-21 19:52:08.835 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][60/77] Data 0.027 (0.032) Batch 0.067 (0.078) Remain 00:00:43 loss: 0.3259 data: 0.0153 Lr: 0.90260
2024-08-21 19:52:08.836 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][60/77] Data 0.027 (0.029) Batch 0.067 (0.078) Remain 00:00:43 loss: 0.3259 data: 0.0115 Lr: 0.90260
2024-08-21 19:52:08.903 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][61/77] Data 0.027 (0.032) Batch 0.067 (0.077) Remain 00:00:42 loss: 0.5034 data: 0.0110 Lr: 0.90097
2024-08-21 19:52:08.903 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][61/77] Data 0.027 (0.029) Batch 0.067 (0.077) Remain 00:00:42 loss: 0.5034 data: -0.0004 Lr: 0.90097
2024-08-21 19:52:08.973 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][62/77] Data 0.028 (0.031) Batch 0.070 (0.076) Remain 00:00:42 loss: 0.3727 data: 0.0010 Lr: 0.89935
2024-08-21 19:52:08.973 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][62/77] Data 0.029 (0.029) Batch 0.070 (0.076) Remain 00:00:42 loss: 0.3727 data: -0.0151 Lr: 0.89935
2024-08-21 19:52:09.048 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][63/77] Data 0.028 (0.031) Batch 0.075 (0.076) Remain 00:00:42 loss: 0.4684 data: 0.0141 Lr: 0.89773
2024-08-21 19:52:09.048 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][63/77] Data 0.029 (0.029) Batch 0.075 (0.076) Remain 00:00:42 loss: 0.4684 data: 0.0257 Lr: 0.89773
2024-08-21 19:52:09.120 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][64/77] Data 0.029 (0.031) Batch 0.072 (0.076) Remain 00:00:41 loss: 0.4913 data: 0.0234 Lr: 0.89610
2024-08-21 19:52:09.120 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][64/77] Data 0.029 (0.029) Batch 0.072 (0.076) Remain 00:00:41 loss: 0.4913 data: -0.0081 Lr: 0.89610
2024-08-21 19:52:09.195 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][65/77] Data 0.028 (0.031) Batch 0.075 (0.076) Remain 00:00:41 loss: 0.6195 data: 0.0046 Lr: 0.89448
2024-08-21 19:52:09.195 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][65/77] Data 0.029 (0.029) Batch 0.075 (0.076) Remain 00:00:41 loss: 0.6195 data: -0.0014 Lr: 0.89448
2024-08-21 19:52:09.265 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][66/77] Data 0.028 (0.030) Batch 0.071 (0.075) Remain 00:00:41 loss: 0.3913 data: -0.0121 Lr: 0.89286
2024-08-21 19:52:09.265 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][66/77] Data 0.029 (0.029) Batch 0.071 (0.075) Remain 00:00:41 loss: 0.3913 data: -0.0012 Lr: 0.89286
2024-08-21 19:52:09.336 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][67/77] Data 0.028 (0.030) Batch 0.071 (0.075) Remain 00:00:41 loss: 0.3400 data: -0.0150 Lr: 0.89123
2024-08-21 19:52:09.336 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][67/77] Data 0.029 (0.029) Batch 0.071 (0.075) Remain 00:00:41 loss: 0.3400 data: 0.0110 Lr: 0.89123
2024-08-21 19:52:09.407 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][68/77] Data 0.028 (0.030) Batch 0.071 (0.075) Remain 00:00:41 loss: 0.3087 data: -0.0077 Lr: 0.88961
2024-08-21 19:52:09.407 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][68/77] Data 0.028 (0.029) Batch 0.071 (0.075) Remain 00:00:41 loss: 0.3087 data: -0.0059 Lr: 0.88961
2024-08-21 19:52:09.476 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][69/77] Data 0.028 (0.030) Batch 0.070 (0.074) Remain 00:00:40 loss: 0.3069 data: 0.0118 Lr: 0.88799
2024-08-21 19:52:09.477 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][69/77] Data 0.028 (0.029) Batch 0.070 (0.075) Remain 00:00:40 loss: 0.3069 data: -0.0006 Lr: 0.88799
2024-08-21 19:52:09.550 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][70/77] Data 0.028 (0.030) Batch 0.074 (0.074) Remain 00:00:40 loss: 0.3505 data: -0.0061 Lr: 0.88636
2024-08-21 19:52:09.550 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][70/77] Data 0.029 (0.029) Batch 0.074 (0.075) Remain 00:00:40 loss: 0.3505 data: -0.0068 Lr: 0.88636
2024-08-21 19:52:09.622 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][71/77] Data 0.029 (0.030) Batch 0.071 (0.074) Remain 00:00:40 loss: 0.3158 data: 0.0023 Lr: 0.88474
2024-08-21 19:52:09.622 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][71/77] Data 0.028 (0.029) Batch 0.071 (0.074) Remain 00:00:40 loss: 0.3158 data: 0.0071 Lr: 0.88474
2024-08-21 19:52:09.692 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][72/77] Data 0.028 (0.030) Batch 0.070 (0.074) Remain 00:00:40 loss: 0.3641 data: -0.0090 Lr: 0.88312
2024-08-21 19:52:09.692 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][72/77] Data 0.028 (0.029) Batch 0.070 (0.074) Remain 00:00:40 loss: 0.3641 data: -0.0034 Lr: 0.88312
2024-08-21 19:52:09.762 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][73/77] Data 0.028 (0.030) Batch 0.070 (0.074) Remain 00:00:40 loss: 0.3541 data: 0.0081 Lr: 0.88149
2024-08-21 19:52:09.762 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][73/77] Data 0.028 (0.029) Batch 0.070 (0.074) Remain 00:00:40 loss: 0.3541 data: -0.0024 Lr: 0.88149
2024-08-21 19:52:09.842 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][74/77] Data 0.028 (0.030) Batch 0.080 (0.074) Remain 00:00:40 loss: 0.2666 data: -0.0020 Lr: 0.87987
2024-08-21 19:52:09.842 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][74/77] Data 0.028 (0.029) Batch 0.080 (0.074) Remain 00:00:40 loss: 0.2666 data: 0.0154 Lr: 0.87987
2024-08-21 19:52:09.926 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][75/77] Data 0.035 (0.030) Batch 0.084 (0.075) Remain 00:00:40 loss: 0.2669 data: 0.0045 Lr: 0.87825
2024-08-21 19:52:09.926 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][75/77] Data 0.035 (0.029) Batch 0.084 (0.075) Remain 00:00:40 loss: 0.2669 data: -0.0077 Lr: 0.87825
2024-08-21 19:52:09.997 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][76/77] Data 0.027 (0.030) Batch 0.071 (0.074) Remain 00:00:40 loss: 0.4222 data: 0.0079 Lr: 0.87662
2024-08-21 19:52:09.997 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][76/77] Data 0.032 (0.029) Batch 0.071 (0.075) Remain 00:00:40 loss: 0.4222 data: 0.0015 Lr: 0.87662
2024-08-21 19:52:10.050 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][77/77] Data 0.031 (0.030) Batch 0.052 (0.074) Remain 00:00:39 loss: 0.3256 data: -0.0059 Lr: 0.87500
2024-08-21 19:52:10.050 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 19:52:10.050 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [1/8][77/77] Data 0.037 (0.029) Batch 0.053 (0.074) Remain 00:00:39 loss: 0.3256 data: -0.0010 Lr: 0.87500
2024-08-21 19:52:10.051 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 19:52:14.964 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0951, Accuracy: 0.9731
2024-08-21 19:52:14.965 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 19:52:14.965 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0951, Accuracy: 0.9731
2024-08-21 19:52:14.965 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 19:52:14.968 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 19:52:14.970 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 19:52:15.068 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][1/77] Data 0.057 (0.057) Batch 0.098 (0.098) Remain 00:00:52 loss: 0.3104 data: 0.0100 Lr: 0.87338
2024-08-21 19:52:15.068 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][1/77] Data 0.043 (0.043) Batch 0.099 (0.099) Remain 00:00:53 loss: 0.3104 data: 0.0028 Lr: 0.87338
2024-08-21 19:52:15.132 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][2/77] Data 0.028 (0.028) Batch 0.064 (0.064) Remain 00:00:34 loss: 0.3229 data: -0.0113 Lr: 0.87175
2024-08-21 19:52:15.132 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][2/77] Data 0.021 (0.021) Batch 0.064 (0.064) Remain 00:00:34 loss: 0.3229 data: 0.0047 Lr: 0.87175
2024-08-21 19:52:15.197 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][3/77] Data 0.028 (0.028) Batch 0.065 (0.064) Remain 00:00:34 loss: 0.3184 data: -0.0137 Lr: 0.87013
2024-08-21 19:52:15.197 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][3/77] Data 0.021 (0.021) Batch 0.065 (0.064) Remain 00:00:34 loss: 0.3184 data: 0.0043 Lr: 0.87013
2024-08-21 19:52:15.262 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][4/77] Data 0.028 (0.028) Batch 0.066 (0.065) Remain 00:00:34 loss: 0.3552 data: 0.0291 Lr: 0.86851
2024-08-21 19:52:15.262 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][4/77] Data 0.021 (0.021) Batch 0.066 (0.065) Remain 00:00:34 loss: 0.3552 data: -0.0114 Lr: 0.86851
2024-08-21 19:52:15.327 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][5/77] Data 0.028 (0.028) Batch 0.064 (0.065) Remain 00:00:34 loss: 0.3922 data: 0.0033 Lr: 0.86688
2024-08-21 19:52:15.327 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][5/77] Data 0.021 (0.021) Batch 0.064 (0.065) Remain 00:00:34 loss: 0.3922 data: 0.0068 Lr: 0.86688
2024-08-21 19:52:15.390 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][6/77] Data 0.028 (0.028) Batch 0.063 (0.064) Remain 00:00:34 loss: 0.3161 data: 0.0031 Lr: 0.86526
2024-08-21 19:52:15.390 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][6/77] Data 0.021 (0.021) Batch 0.063 (0.064) Remain 00:00:34 loss: 0.3161 data: -0.0023 Lr: 0.86526
2024-08-21 19:52:15.456 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][7/77] Data 0.027 (0.028) Batch 0.066 (0.065) Remain 00:00:34 loss: 0.3084 data: 0.0027 Lr: 0.86364
2024-08-21 19:52:15.456 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][7/77] Data 0.029 (0.023) Batch 0.066 (0.065) Remain 00:00:34 loss: 0.3084 data: -0.0093 Lr: 0.86364
2024-08-21 19:52:15.522 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][8/77] Data 0.028 (0.028) Batch 0.066 (0.065) Remain 00:00:34 loss: 0.2192 data: -0.0128 Lr: 0.86201
2024-08-21 19:52:15.522 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][8/77] Data 0.027 (0.023) Batch 0.066 (0.065) Remain 00:00:34 loss: 0.2192 data: -0.0005 Lr: 0.86201
2024-08-21 19:52:15.590 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][9/77] Data 0.028 (0.028) Batch 0.068 (0.065) Remain 00:00:34 loss: 0.2662 data: -0.0067 Lr: 0.86039
2024-08-21 19:52:15.590 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][9/77] Data 0.027 (0.024) Batch 0.069 (0.065) Remain 00:00:34 loss: 0.2662 data: -0.0131 Lr: 0.86039
2024-08-21 19:52:15.666 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][10/77] Data 0.029 (0.028) Batch 0.076 (0.066) Remain 00:00:35 loss: 0.3167 data: -0.0018 Lr: 0.85877
2024-08-21 19:52:15.666 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][10/77] Data 0.028 (0.024) Batch 0.076 (0.066) Remain 00:00:35 loss: 0.3167 data: 0.0008 Lr: 0.85877
2024-08-21 19:52:15.741 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][11/77] Data 0.029 (0.028) Batch 0.075 (0.067) Remain 00:00:35 loss: 0.3123 data: -0.0030 Lr: 0.85714
2024-08-21 19:52:15.741 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][11/77] Data 0.028 (0.025) Batch 0.075 (0.067) Remain 00:00:35 loss: 0.3123 data: -0.0072 Lr: 0.85714
2024-08-21 19:52:15.820 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][12/77] Data 0.030 (0.028) Batch 0.079 (0.068) Remain 00:00:36 loss: 0.2207 data: 0.0053 Lr: 0.85552
2024-08-21 19:52:15.820 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][12/77] Data 0.028 (0.025) Batch 0.080 (0.068) Remain 00:00:36 loss: 0.2207 data: 0.0176 Lr: 0.85552
2024-08-21 19:52:15.886 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][13/77] Data 0.025 (0.028) Batch 0.066 (0.068) Remain 00:00:35 loss: 0.2682 data: 0.0083 Lr: 0.85390
2024-08-21 19:52:15.886 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][13/77] Data 0.028 (0.025) Batch 0.066 (0.068) Remain 00:00:35 loss: 0.2682 data: 0.0099 Lr: 0.85390
2024-08-21 19:52:15.956 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][14/77] Data 0.023 (0.028) Batch 0.071 (0.068) Remain 00:00:35 loss: 0.2620 data: -0.0109 Lr: 0.85227
2024-08-21 19:52:15.956 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][14/77] Data 0.027 (0.025) Batch 0.071 (0.068) Remain 00:00:35 loss: 0.2620 data: -0.0031 Lr: 0.85227
2024-08-21 19:52:16.021 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][15/77] Data 0.021 (0.027) Batch 0.065 (0.068) Remain 00:00:35 loss: 0.2340 data: -0.0187 Lr: 0.85065
2024-08-21 19:52:16.021 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][15/77] Data 0.027 (0.025) Batch 0.065 (0.068) Remain 00:00:35 loss: 0.2340 data: -0.0017 Lr: 0.85065
2024-08-21 19:52:16.087 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][16/77] Data 0.022 (0.027) Batch 0.066 (0.068) Remain 00:00:35 loss: 0.2758 data: -0.0092 Lr: 0.84903
2024-08-21 19:52:16.087 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][16/77] Data 0.029 (0.026) Batch 0.066 (0.068) Remain 00:00:35 loss: 0.2758 data: -0.0074 Lr: 0.84903
2024-08-21 19:52:16.152 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][17/77] Data 0.022 (0.026) Batch 0.064 (0.068) Remain 00:00:35 loss: 0.3242 data: 0.0063 Lr: 0.84740
2024-08-21 19:52:16.152 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][17/77] Data 0.027 (0.026) Batch 0.064 (0.068) Remain 00:00:35 loss: 0.3242 data: 0.0055 Lr: 0.84740
2024-08-21 19:52:16.216 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][18/77] Data 0.028 (0.027) Batch 0.064 (0.068) Remain 00:00:35 loss: 0.3490 data: -0.0094 Lr: 0.84578
2024-08-21 19:52:16.216 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][18/77] Data 0.027 (0.026) Batch 0.064 (0.068) Remain 00:00:35 loss: 0.3490 data: -0.0130 Lr: 0.84578
2024-08-21 19:52:16.280 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][19/77] Data 0.027 (0.027) Batch 0.064 (0.067) Remain 00:00:35 loss: 0.1948 data: -0.0153 Lr: 0.84416
2024-08-21 19:52:16.280 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][19/77] Data 0.027 (0.026) Batch 0.064 (0.067) Remain 00:00:35 loss: 0.1948 data: 0.0081 Lr: 0.84416
2024-08-21 19:52:16.345 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][20/77] Data 0.027 (0.027) Batch 0.065 (0.067) Remain 00:00:34 loss: 0.3743 data: -0.0177 Lr: 0.84253
2024-08-21 19:52:16.345 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][20/77] Data 0.027 (0.026) Batch 0.065 (0.067) Remain 00:00:34 loss: 0.3743 data: -0.0091 Lr: 0.84253
2024-08-21 19:52:16.410 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][21/77] Data 0.027 (0.027) Batch 0.066 (0.067) Remain 00:00:34 loss: 0.2980 data: -0.0022 Lr: 0.84091
2024-08-21 19:52:16.410 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][21/77] Data 0.027 (0.026) Batch 0.066 (0.067) Remain 00:00:34 loss: 0.2980 data: -0.0036 Lr: 0.84091
2024-08-21 19:52:16.476 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][22/77] Data 0.027 (0.027) Batch 0.066 (0.067) Remain 00:00:34 loss: 0.2895 data: -0.0025 Lr: 0.83929
2024-08-21 19:52:16.476 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][22/77] Data 0.027 (0.026) Batch 0.066 (0.067) Remain 00:00:34 loss: 0.2895 data: 0.0054 Lr: 0.83929
2024-08-21 19:52:16.540 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][23/77] Data 0.027 (0.027) Batch 0.064 (0.067) Remain 00:00:34 loss: 0.2814 data: -0.0095 Lr: 0.83766
2024-08-21 19:52:16.541 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_100
2024-08-21 19:52:16.541 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][23/77] Data 0.027 (0.026) Batch 0.064 (0.067) Remain 00:00:34 loss: 0.2814 data: -0.0070 Lr: 0.83766
2024-08-21 19:52:16.541 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_100
2024-08-21 19:52:16.572 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -333.2380676269531
2024-08-21 19:52:16.572 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -333.2380676269531
2024-08-21 19:52:16.572 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -169.3326416015625
2024-08-21 19:52:16.572 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -163.90542602539062
2024-08-21 19:52:16.639 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][24/77] Data 0.059 (0.028) Batch 0.098 (0.068) Remain 00:00:35 loss: 0.2858 data: -0.0063 Lr: 0.83604
2024-08-21 19:52:16.639 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][24/77] Data 0.059 (0.027) Batch 0.098 (0.068) Remain 00:00:35 loss: 0.2858 data: 0.0036 Lr: 0.83604
2024-08-21 19:52:16.705 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][25/77] Data 0.028 (0.028) Batch 0.066 (0.068) Remain 00:00:35 loss: 0.2697 data: 0.0165 Lr: 0.83442
2024-08-21 19:52:16.705 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][25/77] Data 0.027 (0.027) Batch 0.066 (0.068) Remain 00:00:35 loss: 0.2697 data: 0.0106 Lr: 0.83442
2024-08-21 19:52:16.770 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][26/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:34 loss: 0.2188 data: -0.0164 Lr: 0.83279
2024-08-21 19:52:16.770 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][26/77] Data 0.027 (0.027) Batch 0.065 (0.068) Remain 00:00:34 loss: 0.2188 data: -0.0029 Lr: 0.83279
2024-08-21 19:52:16.838 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][27/77] Data 0.027 (0.027) Batch 0.068 (0.068) Remain 00:00:34 loss: 0.3187 data: 0.0169 Lr: 0.83117
2024-08-21 19:52:16.839 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][27/77] Data 0.028 (0.028) Batch 0.069 (0.068) Remain 00:00:34 loss: 0.3187 data: 0.0028 Lr: 0.83117
2024-08-21 19:52:16.906 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][28/77] Data 0.029 (0.028) Batch 0.068 (0.068) Remain 00:00:34 loss: 0.2331 data: -0.0073 Lr: 0.82955
2024-08-21 19:52:16.906 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][28/77] Data 0.027 (0.027) Batch 0.068 (0.068) Remain 00:00:34 loss: 0.2331 data: -0.0012 Lr: 0.82955
2024-08-21 19:52:16.976 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][29/77] Data 0.028 (0.028) Batch 0.069 (0.068) Remain 00:00:34 loss: 0.3149 data: -0.0012 Lr: 0.82792
2024-08-21 19:52:16.976 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][29/77] Data 0.027 (0.027) Batch 0.069 (0.068) Remain 00:00:34 loss: 0.3149 data: -0.0033 Lr: 0.82792
2024-08-21 19:52:17.042 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][30/77] Data 0.028 (0.028) Batch 0.066 (0.068) Remain 00:00:34 loss: 0.2111 data: -0.0090 Lr: 0.82630
2024-08-21 19:52:17.042 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][30/77] Data 0.027 (0.027) Batch 0.066 (0.068) Remain 00:00:34 loss: 0.2111 data: -0.0072 Lr: 0.82630
2024-08-21 19:52:17.108 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][31/77] Data 0.028 (0.028) Batch 0.067 (0.068) Remain 00:00:34 loss: 0.3079 data: 0.0136 Lr: 0.82468
2024-08-21 19:52:17.108 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][31/77] Data 0.027 (0.027) Batch 0.067 (0.068) Remain 00:00:34 loss: 0.3079 data: -0.0186 Lr: 0.82468
2024-08-21 19:52:17.174 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][32/77] Data 0.028 (0.028) Batch 0.066 (0.068) Remain 00:00:34 loss: 0.2183 data: 0.0071 Lr: 0.82305
2024-08-21 19:52:17.174 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][32/77] Data 0.027 (0.027) Batch 0.066 (0.068) Remain 00:00:34 loss: 0.2183 data: -0.0037 Lr: 0.82305
2024-08-21 19:52:17.241 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][33/77] Data 0.028 (0.028) Batch 0.066 (0.068) Remain 00:00:34 loss: 0.2403 data: 0.0149 Lr: 0.82143
2024-08-21 19:52:17.241 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][33/77] Data 0.027 (0.027) Batch 0.066 (0.068) Remain 00:00:34 loss: 0.2403 data: -0.0027 Lr: 0.82143
2024-08-21 19:52:17.306 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][34/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:34 loss: 0.3028 data: 0.0117 Lr: 0.81981
2024-08-21 19:52:17.306 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][34/77] Data 0.027 (0.027) Batch 0.065 (0.068) Remain 00:00:34 loss: 0.3028 data: -0.0164 Lr: 0.81981
2024-08-21 19:52:17.372 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][35/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:34 loss: 0.2077 data: 0.0046 Lr: 0.81818
2024-08-21 19:52:17.372 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][35/77] Data 0.027 (0.027) Batch 0.066 (0.068) Remain 00:00:34 loss: 0.2077 data: 0.0049 Lr: 0.81818
2024-08-21 19:52:17.438 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][36/77] Data 0.028 (0.028) Batch 0.066 (0.068) Remain 00:00:34 loss: 0.2814 data: -0.0107 Lr: 0.81656
2024-08-21 19:52:17.438 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][36/77] Data 0.027 (0.027) Batch 0.066 (0.068) Remain 00:00:34 loss: 0.2814 data: -0.0141 Lr: 0.81656
2024-08-21 19:52:17.505 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][37/77] Data 0.028 (0.028) Batch 0.067 (0.068) Remain 00:00:34 loss: 0.1938 data: -0.0088 Lr: 0.81494
2024-08-21 19:52:17.505 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][37/77] Data 0.027 (0.027) Batch 0.067 (0.068) Remain 00:00:34 loss: 0.1938 data: 0.0044 Lr: 0.81494
2024-08-21 19:52:17.575 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][38/77] Data 0.028 (0.028) Batch 0.070 (0.068) Remain 00:00:34 loss: 0.2396 data: 0.0169 Lr: 0.81331
2024-08-21 19:52:17.575 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][38/77] Data 0.028 (0.027) Batch 0.070 (0.068) Remain 00:00:34 loss: 0.2396 data: 0.0043 Lr: 0.81331
2024-08-21 19:52:17.647 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][39/77] Data 0.030 (0.028) Batch 0.072 (0.068) Remain 00:00:33 loss: 0.2582 data: 0.0070 Lr: 0.81169
2024-08-21 19:52:17.647 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][39/77] Data 0.028 (0.027) Batch 0.072 (0.068) Remain 00:00:33 loss: 0.2582 data: 0.0037 Lr: 0.81169
2024-08-21 19:52:17.717 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][40/77] Data 0.029 (0.028) Batch 0.070 (0.068) Remain 00:00:33 loss: 0.1748 data: 0.0044 Lr: 0.81006
2024-08-21 19:52:17.717 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][40/77] Data 0.028 (0.027) Batch 0.070 (0.068) Remain 00:00:33 loss: 0.1748 data: -0.0223 Lr: 0.81006
2024-08-21 19:52:17.788 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][41/77] Data 0.029 (0.028) Batch 0.071 (0.068) Remain 00:00:33 loss: 0.2578 data: 0.0002 Lr: 0.80844
2024-08-21 19:52:17.788 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][41/77] Data 0.028 (0.027) Batch 0.071 (0.068) Remain 00:00:33 loss: 0.2578 data: 0.0003 Lr: 0.80844
2024-08-21 19:52:17.857 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][42/77] Data 0.029 (0.028) Batch 0.069 (0.068) Remain 00:00:33 loss: 0.2609 data: -0.0108 Lr: 0.80682
2024-08-21 19:52:17.857 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][42/77] Data 0.028 (0.027) Batch 0.069 (0.068) Remain 00:00:33 loss: 0.2609 data: -0.0043 Lr: 0.80682
2024-08-21 19:52:17.922 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][43/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:33 loss: 0.4019 data: -0.0178 Lr: 0.80519
2024-08-21 19:52:17.922 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][43/77] Data 0.027 (0.027) Batch 0.065 (0.068) Remain 00:00:33 loss: 0.4019 data: -0.0002 Lr: 0.80519
2024-08-21 19:52:17.994 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][44/77] Data 0.027 (0.028) Batch 0.072 (0.068) Remain 00:00:33 loss: 0.4527 data: -0.0085 Lr: 0.80357
2024-08-21 19:52:17.994 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][44/77] Data 0.027 (0.027) Batch 0.072 (0.068) Remain 00:00:33 loss: 0.4527 data: 0.0022 Lr: 0.80357
2024-08-21 19:52:18.058 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][45/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:33 loss: 0.2723 data: -0.0076 Lr: 0.80195
2024-08-21 19:52:18.058 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][45/77] Data 0.027 (0.027) Batch 0.065 (0.068) Remain 00:00:33 loss: 0.2723 data: -0.0029 Lr: 0.80195
2024-08-21 19:52:18.123 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][46/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:33 loss: 0.2986 data: -0.0116 Lr: 0.80032
2024-08-21 19:52:18.123 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][46/77] Data 0.027 (0.027) Batch 0.065 (0.068) Remain 00:00:33 loss: 0.2986 data: 0.0039 Lr: 0.80032
2024-08-21 19:52:18.190 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][47/77] Data 0.027 (0.028) Batch 0.067 (0.068) Remain 00:00:33 loss: 0.2841 data: -0.0083 Lr: 0.79870
2024-08-21 19:52:18.190 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][47/77] Data 0.027 (0.027) Batch 0.067 (0.068) Remain 00:00:33 loss: 0.2841 data: 0.0109 Lr: 0.79870
2024-08-21 19:52:18.256 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][48/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:33 loss: 0.1764 data: -0.0018 Lr: 0.79708
2024-08-21 19:52:18.256 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][48/77] Data 0.027 (0.027) Batch 0.066 (0.068) Remain 00:00:33 loss: 0.1764 data: 0.0121 Lr: 0.79708
2024-08-21 19:52:18.321 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][49/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:33 loss: 0.3041 data: -0.0057 Lr: 0.79545
2024-08-21 19:52:18.321 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][49/77] Data 0.027 (0.027) Batch 0.065 (0.068) Remain 00:00:33 loss: 0.3041 data: -0.0056 Lr: 0.79545
2024-08-21 19:52:18.387 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][50/77] Data 0.027 (0.028) Batch 0.067 (0.068) Remain 00:00:33 loss: 0.2695 data: -0.0059 Lr: 0.79383
2024-08-21 19:52:18.387 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][50/77] Data 0.027 (0.027) Batch 0.067 (0.068) Remain 00:00:33 loss: 0.2695 data: -0.0177 Lr: 0.79383
2024-08-21 19:52:18.454 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][51/77] Data 0.028 (0.028) Batch 0.067 (0.068) Remain 00:00:33 loss: 0.1977 data: 0.0019 Lr: 0.79221
2024-08-21 19:52:18.454 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][51/77] Data 0.027 (0.027) Batch 0.067 (0.068) Remain 00:00:33 loss: 0.1977 data: 0.0120 Lr: 0.79221
2024-08-21 19:52:18.520 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][52/77] Data 0.028 (0.028) Batch 0.066 (0.068) Remain 00:00:33 loss: 0.1861 data: -0.0249 Lr: 0.79058
2024-08-21 19:52:18.520 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][52/77] Data 0.027 (0.027) Batch 0.066 (0.068) Remain 00:00:33 loss: 0.1861 data: 0.0017 Lr: 0.79058
2024-08-21 19:52:18.588 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][53/77] Data 0.028 (0.028) Batch 0.067 (0.068) Remain 00:00:32 loss: 0.2033 data: -0.0014 Lr: 0.78896
2024-08-21 19:52:18.588 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][53/77] Data 0.027 (0.027) Batch 0.067 (0.068) Remain 00:00:32 loss: 0.2033 data: 0.0034 Lr: 0.78896
2024-08-21 19:52:18.654 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][54/77] Data 0.028 (0.028) Batch 0.066 (0.068) Remain 00:00:32 loss: 0.2667 data: 0.0218 Lr: 0.78734
2024-08-21 19:52:18.654 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][54/77] Data 0.027 (0.027) Batch 0.066 (0.068) Remain 00:00:32 loss: 0.2667 data: 0.0069 Lr: 0.78734
2024-08-21 19:52:18.718 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][55/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:32 loss: 0.1959 data: 0.0029 Lr: 0.78571
2024-08-21 19:52:18.719 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][55/77] Data 0.027 (0.027) Batch 0.065 (0.068) Remain 00:00:32 loss: 0.1959 data: 0.0152 Lr: 0.78571
2024-08-21 19:52:18.783 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][56/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:32 loss: 0.2772 data: 0.0070 Lr: 0.78409
2024-08-21 19:52:18.783 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][56/77] Data 0.027 (0.027) Batch 0.065 (0.068) Remain 00:00:32 loss: 0.2772 data: -0.0028 Lr: 0.78409
2024-08-21 19:52:18.849 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][57/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:32 loss: 0.2550 data: 0.0044 Lr: 0.78247
2024-08-21 19:52:18.849 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][57/77] Data 0.027 (0.027) Batch 0.065 (0.068) Remain 00:00:32 loss: 0.2550 data: -0.0048 Lr: 0.78247
2024-08-21 19:52:18.914 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][58/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:32 loss: 0.1554 data: 0.0036 Lr: 0.78084
2024-08-21 19:52:18.914 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][58/77] Data 0.027 (0.027) Batch 0.065 (0.067) Remain 00:00:32 loss: 0.1554 data: 0.0017 Lr: 0.78084
2024-08-21 19:52:18.978 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][59/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:32 loss: 0.2714 data: -0.0058 Lr: 0.77922
2024-08-21 19:52:18.978 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][59/77] Data 0.027 (0.027) Batch 0.065 (0.067) Remain 00:00:32 loss: 0.2714 data: 0.0104 Lr: 0.77922
2024-08-21 19:52:19.043 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][60/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:32 loss: 0.1998 data: 0.0062 Lr: 0.77760
2024-08-21 19:52:19.043 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][60/77] Data 0.027 (0.027) Batch 0.065 (0.067) Remain 00:00:32 loss: 0.1998 data: -0.0078 Lr: 0.77760
2024-08-21 19:52:19.108 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][61/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:32 loss: 0.2022 data: -0.0027 Lr: 0.77597
2024-08-21 19:52:19.108 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][61/77] Data 0.027 (0.027) Batch 0.065 (0.067) Remain 00:00:32 loss: 0.2022 data: 0.0060 Lr: 0.77597
2024-08-21 19:52:19.178 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][62/77] Data 0.028 (0.028) Batch 0.070 (0.067) Remain 00:00:32 loss: 0.1744 data: 0.0132 Lr: 0.77435
2024-08-21 19:52:19.178 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][62/77] Data 0.032 (0.027) Batch 0.070 (0.067) Remain 00:00:32 loss: 0.1744 data: -0.0130 Lr: 0.77435
2024-08-21 19:52:19.244 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][63/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:32 loss: 0.2041 data: 0.0036 Lr: 0.77273
2024-08-21 19:52:19.244 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][63/77] Data 0.027 (0.027) Batch 0.065 (0.067) Remain 00:00:32 loss: 0.2041 data: 0.0162 Lr: 0.77273
2024-08-21 19:52:19.308 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][64/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:32 loss: 0.1735 data: 0.0171 Lr: 0.77110
2024-08-21 19:52:19.308 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][64/77] Data 0.027 (0.027) Batch 0.065 (0.067) Remain 00:00:32 loss: 0.1735 data: 0.0150 Lr: 0.77110
2024-08-21 19:52:19.373 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][65/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:31 loss: 0.2626 data: -0.0064 Lr: 0.76948
2024-08-21 19:52:19.373 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][65/77] Data 0.027 (0.027) Batch 0.065 (0.067) Remain 00:00:31 loss: 0.2626 data: 0.0033 Lr: 0.76948
2024-08-21 19:52:19.438 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][66/77] Data 0.027 (0.028) Batch 0.066 (0.067) Remain 00:00:31 loss: 0.2081 data: 0.0051 Lr: 0.76786
2024-08-21 19:52:19.438 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][66/77] Data 0.027 (0.027) Batch 0.066 (0.067) Remain 00:00:31 loss: 0.2081 data: 0.0126 Lr: 0.76786
2024-08-21 19:52:19.507 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][67/77] Data 0.027 (0.028) Batch 0.068 (0.067) Remain 00:00:31 loss: 0.1938 data: -0.0006 Lr: 0.76623
2024-08-21 19:52:19.507 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][67/77] Data 0.027 (0.027) Batch 0.068 (0.067) Remain 00:00:31 loss: 0.1938 data: 0.0009 Lr: 0.76623
2024-08-21 19:52:19.574 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][68/77] Data 0.028 (0.028) Batch 0.067 (0.067) Remain 00:00:31 loss: 0.2109 data: -0.0102 Lr: 0.76461
2024-08-21 19:52:19.574 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][68/77] Data 0.027 (0.027) Batch 0.067 (0.067) Remain 00:00:31 loss: 0.2109 data: -0.0143 Lr: 0.76461
2024-08-21 19:52:19.640 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][69/77] Data 0.028 (0.028) Batch 0.066 (0.067) Remain 00:00:31 loss: 0.2081 data: -0.0094 Lr: 0.76299
2024-08-21 19:52:19.641 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][69/77] Data 0.027 (0.027) Batch 0.066 (0.067) Remain 00:00:31 loss: 0.2081 data: 0.0056 Lr: 0.76299
2024-08-21 19:52:19.707 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][70/77] Data 0.028 (0.028) Batch 0.066 (0.067) Remain 00:00:31 loss: 0.1933 data: 0.0183 Lr: 0.76136
2024-08-21 19:52:19.707 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][70/77] Data 0.027 (0.027) Batch 0.066 (0.067) Remain 00:00:31 loss: 0.1933 data: -0.0179 Lr: 0.76136
2024-08-21 19:52:19.774 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][71/77] Data 0.028 (0.028) Batch 0.068 (0.067) Remain 00:00:31 loss: 0.1376 data: 0.0079 Lr: 0.75974
2024-08-21 19:52:19.774 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][71/77] Data 0.029 (0.027) Batch 0.068 (0.067) Remain 00:00:31 loss: 0.1376 data: 0.0003 Lr: 0.75974
2024-08-21 19:52:19.841 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][72/77] Data 0.028 (0.028) Batch 0.066 (0.067) Remain 00:00:31 loss: 0.1204 data: -0.0032 Lr: 0.75812
2024-08-21 19:52:19.841 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][72/77] Data 0.027 (0.027) Batch 0.066 (0.067) Remain 00:00:31 loss: 0.1204 data: 0.0072 Lr: 0.75812
2024-08-21 19:52:19.907 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][73/77] Data 0.028 (0.028) Batch 0.067 (0.067) Remain 00:00:31 loss: 0.1542 data: -0.0001 Lr: 0.75649
2024-08-21 19:52:19.907 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_150
2024-08-21 19:52:19.907 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][73/77] Data 0.027 (0.027) Batch 0.067 (0.067) Remain 00:00:31 loss: 0.1542 data: -0.0136 Lr: 0.75649
2024-08-21 19:52:19.907 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_150
2024-08-21 19:52:19.935 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -393.9523010253906
2024-08-21 19:52:19.935 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -393.9523010253906
2024-08-21 19:52:19.935 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -188.40049743652344
2024-08-21 19:52:19.935 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -205.55181884765625
2024-08-21 19:52:20.004 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][74/77] Data 0.056 (0.028) Batch 0.097 (0.068) Remain 00:00:31 loss: 0.3094 data: 0.0015 Lr: 0.75487
2024-08-21 19:52:20.004 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][74/77] Data 0.056 (0.028) Batch 0.097 (0.068) Remain 00:00:31 loss: 0.3094 data: -0.0041 Lr: 0.75487
2024-08-21 19:52:20.071 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][75/77] Data 0.028 (0.028) Batch 0.067 (0.068) Remain 00:00:31 loss: 0.2534 data: 0.0104 Lr: 0.75325
2024-08-21 19:52:20.071 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][75/77] Data 0.027 (0.028) Batch 0.067 (0.068) Remain 00:00:31 loss: 0.2534 data: -0.0017 Lr: 0.75325
2024-08-21 19:52:20.138 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][76/77] Data 0.028 (0.028) Batch 0.067 (0.068) Remain 00:00:31 loss: 0.1001 data: -0.0117 Lr: 0.75162
2024-08-21 19:52:20.138 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][76/77] Data 0.027 (0.028) Batch 0.067 (0.068) Remain 00:00:31 loss: 0.1001 data: 0.0095 Lr: 0.75162
2024-08-21 19:52:20.182 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][77/77] Data 0.031 (0.028) Batch 0.044 (0.067) Remain 00:00:31 loss: 0.2523 data: -0.0051 Lr: 0.75000
2024-08-21 19:52:20.182 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 19:52:20.182 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][77/77] Data 0.031 (0.028) Batch 0.044 (0.067) Remain 00:00:31 loss: 0.2523 data: -0.0003 Lr: 0.75000
2024-08-21 19:52:20.182 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 19:52:25.727 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0563, Accuracy: 0.9821
2024-08-21 19:52:25.727 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0563, Accuracy: 0.9821
2024-08-21 19:52:25.728 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 19:52:25.728 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 19:52:25.728 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 19:52:25.728 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 19:52:25.835 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][1/77] Data 0.061 (0.061) Batch 0.106 (0.106) Remain 00:00:49 loss: 0.2107 data: -0.0045 Lr: 0.74838
2024-08-21 19:52:25.835 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][1/77] Data 0.061 (0.061) Batch 0.106 (0.106) Remain 00:00:49 loss: 0.2107 data: -0.0011 Lr: 0.74838
2024-08-21 19:52:25.909 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][2/77] Data 0.029 (0.029) Batch 0.073 (0.073) Remain 00:00:33 loss: 0.2823 data: 0.0084 Lr: 0.74675
2024-08-21 19:52:25.909 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][2/77] Data 0.029 (0.029) Batch 0.074 (0.074) Remain 00:00:33 loss: 0.2823 data: 0.0096 Lr: 0.74675
2024-08-21 19:52:25.982 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][3/77] Data 0.031 (0.030) Batch 0.073 (0.073) Remain 00:00:33 loss: 0.2101 data: -0.0070 Lr: 0.74513
2024-08-21 19:52:25.982 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][3/77] Data 0.029 (0.029) Batch 0.074 (0.073) Remain 00:00:33 loss: 0.2101 data: 0.0068 Lr: 0.74513
2024-08-21 19:52:26.056 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][4/77] Data 0.030 (0.030) Batch 0.074 (0.074) Remain 00:00:33 loss: 0.2044 data: 0.0159 Lr: 0.74351
2024-08-21 19:52:26.056 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][4/77] Data 0.029 (0.029) Batch 0.074 (0.074) Remain 00:00:33 loss: 0.2044 data: 0.0033 Lr: 0.74351
2024-08-21 19:52:26.127 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][5/77] Data 0.029 (0.030) Batch 0.071 (0.073) Remain 00:00:33 loss: 0.1806 data: -0.0087 Lr: 0.74188
2024-08-21 19:52:26.127 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][5/77] Data 0.029 (0.029) Batch 0.071 (0.073) Remain 00:00:33 loss: 0.1806 data: 0.0059 Lr: 0.74188
2024-08-21 19:52:26.195 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][6/77] Data 0.028 (0.029) Batch 0.068 (0.072) Remain 00:00:32 loss: 0.3011 data: 0.0066 Lr: 0.74026
2024-08-21 19:52:26.195 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][6/77] Data 0.028 (0.029) Batch 0.068 (0.072) Remain 00:00:32 loss: 0.3011 data: 0.0003 Lr: 0.74026
2024-08-21 19:52:26.262 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][7/77] Data 0.028 (0.029) Batch 0.067 (0.071) Remain 00:00:32 loss: 0.2604 data: 0.0009 Lr: 0.73864
2024-08-21 19:52:26.262 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][7/77] Data 0.028 (0.029) Batch 0.067 (0.071) Remain 00:00:32 loss: 0.2604 data: -0.0049 Lr: 0.73864
2024-08-21 19:52:26.329 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][8/77] Data 0.028 (0.029) Batch 0.068 (0.071) Remain 00:00:32 loss: 0.2427 data: 0.0145 Lr: 0.73701
2024-08-21 19:52:26.329 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][8/77] Data 0.028 (0.029) Batch 0.068 (0.071) Remain 00:00:32 loss: 0.2427 data: 0.0171 Lr: 0.73701
2024-08-21 19:52:26.396 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][9/77] Data 0.028 (0.029) Batch 0.067 (0.070) Remain 00:00:31 loss: 0.1861 data: -0.0157 Lr: 0.73539
2024-08-21 19:52:26.396 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][9/77] Data 0.028 (0.029) Batch 0.067 (0.070) Remain 00:00:31 loss: 0.1861 data: 0.0071 Lr: 0.73539
2024-08-21 19:52:26.462 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][10/77] Data 0.028 (0.029) Batch 0.066 (0.070) Remain 00:00:31 loss: 0.2109 data: -0.0009 Lr: 0.73377
2024-08-21 19:52:26.462 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][10/77] Data 0.028 (0.029) Batch 0.066 (0.070) Remain 00:00:31 loss: 0.2109 data: -0.0004 Lr: 0.73377
2024-08-21 19:52:26.528 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][11/77] Data 0.028 (0.029) Batch 0.067 (0.069) Remain 00:00:31 loss: 0.2342 data: 0.0062 Lr: 0.73214
2024-08-21 19:52:26.529 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][11/77] Data 0.028 (0.028) Batch 0.067 (0.069) Remain 00:00:31 loss: 0.2342 data: -0.0147 Lr: 0.73214
2024-08-21 19:52:26.703 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][12/77] Data 0.033 (0.029) Batch 0.175 (0.079) Remain 00:00:35 loss: 0.1784 data: 0.0201 Lr: 0.73052
2024-08-21 19:52:26.703 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][12/77] Data 0.032 (0.029) Batch 0.175 (0.079) Remain 00:00:35 loss: 0.1784 data: 0.0268 Lr: 0.73052
2024-08-21 19:52:26.778 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][13/77] Data 0.029 (0.029) Batch 0.075 (0.079) Remain 00:00:35 loss: 0.1758 data: 0.0123 Lr: 0.72890
2024-08-21 19:52:26.778 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][13/77] Data 0.031 (0.029) Batch 0.075 (0.079) Remain 00:00:35 loss: 0.1758 data: -0.0012 Lr: 0.72890
2024-08-21 19:52:26.852 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][14/77] Data 0.029 (0.029) Batch 0.075 (0.078) Remain 00:00:35 loss: 0.2359 data: -0.0102 Lr: 0.72727
2024-08-21 19:52:26.853 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][14/77] Data 0.030 (0.029) Batch 0.075 (0.078) Remain 00:00:35 loss: 0.2359 data: -0.0023 Lr: 0.72727
2024-08-21 19:52:26.927 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][15/77] Data 0.029 (0.029) Batch 0.075 (0.078) Remain 00:00:34 loss: 0.1784 data: -0.0138 Lr: 0.72565
2024-08-21 19:52:26.927 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][15/77] Data 0.030 (0.029) Batch 0.075 (0.078) Remain 00:00:34 loss: 0.1784 data: -0.0008 Lr: 0.72565
2024-08-21 19:52:26.995 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][16/77] Data 0.028 (0.029) Batch 0.068 (0.077) Remain 00:00:34 loss: 0.2232 data: 0.0008 Lr: 0.72403
2024-08-21 19:52:26.995 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][16/77] Data 0.028 (0.029) Batch 0.068 (0.077) Remain 00:00:34 loss: 0.2232 data: -0.0084 Lr: 0.72403
2024-08-21 19:52:27.063 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][17/77] Data 0.028 (0.029) Batch 0.068 (0.077) Remain 00:00:34 loss: 0.1644 data: 0.0065 Lr: 0.72240
2024-08-21 19:52:27.063 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][17/77] Data 0.028 (0.029) Batch 0.068 (0.077) Remain 00:00:34 loss: 0.1644 data: 0.0021 Lr: 0.72240
2024-08-21 19:52:27.132 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][18/77] Data 0.028 (0.029) Batch 0.070 (0.076) Remain 00:00:33 loss: 0.2250 data: 0.0004 Lr: 0.72078
2024-08-21 19:52:27.132 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][18/77] Data 0.029 (0.029) Batch 0.070 (0.076) Remain 00:00:33 loss: 0.2250 data: -0.0179 Lr: 0.72078
2024-08-21 19:52:27.200 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][19/77] Data 0.028 (0.029) Batch 0.068 (0.076) Remain 00:00:33 loss: 0.2599 data: -0.0039 Lr: 0.71916
2024-08-21 19:52:27.200 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][19/77] Data 0.028 (0.029) Batch 0.068 (0.076) Remain 00:00:33 loss: 0.2599 data: 0.0078 Lr: 0.71916
2024-08-21 19:52:27.268 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][20/77] Data 0.028 (0.029) Batch 0.068 (0.075) Remain 00:00:33 loss: 0.1632 data: -0.0017 Lr: 0.71753
2024-08-21 19:52:27.268 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][20/77] Data 0.028 (0.029) Batch 0.068 (0.075) Remain 00:00:33 loss: 0.1632 data: 0.0066 Lr: 0.71753
2024-08-21 19:52:27.337 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][21/77] Data 0.028 (0.029) Batch 0.069 (0.075) Remain 00:00:33 loss: 0.1879 data: -0.0026 Lr: 0.71591
2024-08-21 19:52:27.337 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][21/77] Data 0.028 (0.029) Batch 0.069 (0.075) Remain 00:00:33 loss: 0.1879 data: 0.0077 Lr: 0.71591
2024-08-21 19:52:27.410 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][22/77] Data 0.029 (0.029) Batch 0.073 (0.075) Remain 00:00:33 loss: 0.1470 data: -0.0002 Lr: 0.71429
2024-08-21 19:52:27.410 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][22/77] Data 0.029 (0.029) Batch 0.073 (0.075) Remain 00:00:33 loss: 0.1470 data: -0.0077 Lr: 0.71429
2024-08-21 19:52:27.484 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][23/77] Data 0.029 (0.029) Batch 0.074 (0.075) Remain 00:00:32 loss: 0.1915 data: -0.0122 Lr: 0.71266
2024-08-21 19:52:27.484 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][23/77] Data 0.030 (0.029) Batch 0.074 (0.075) Remain 00:00:32 loss: 0.1915 data: 0.0054 Lr: 0.71266
2024-08-21 19:52:27.563 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][24/77] Data 0.030 (0.029) Batch 0.079 (0.075) Remain 00:00:32 loss: 0.2027 data: 0.0237 Lr: 0.71104
2024-08-21 19:52:27.563 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][24/77] Data 0.029 (0.029) Batch 0.079 (0.075) Remain 00:00:32 loss: 0.2027 data: 0.0091 Lr: 0.71104
2024-08-21 19:52:27.634 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][25/77] Data 0.029 (0.029) Batch 0.071 (0.075) Remain 00:00:32 loss: 0.2321 data: -0.0124 Lr: 0.70942
2024-08-21 19:52:27.634 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][25/77] Data 0.029 (0.029) Batch 0.071 (0.075) Remain 00:00:32 loss: 0.2321 data: 0.0053 Lr: 0.70942
2024-08-21 19:52:27.700 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][26/77] Data 0.028 (0.029) Batch 0.066 (0.075) Remain 00:00:32 loss: 0.1683 data: 0.0091 Lr: 0.70779
2024-08-21 19:52:27.701 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][26/77] Data 0.027 (0.029) Batch 0.066 (0.075) Remain 00:00:32 loss: 0.1683 data: 0.0113 Lr: 0.70779
2024-08-21 19:52:27.767 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][27/77] Data 0.028 (0.029) Batch 0.066 (0.074) Remain 00:00:32 loss: 0.2188 data: -0.0085 Lr: 0.70617
2024-08-21 19:52:27.767 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][27/77] Data 0.028 (0.029) Batch 0.066 (0.074) Remain 00:00:32 loss: 0.2188 data: -0.0077 Lr: 0.70617
2024-08-21 19:52:27.834 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][28/77] Data 0.027 (0.029) Batch 0.067 (0.074) Remain 00:00:32 loss: 0.1970 data: -0.0011 Lr: 0.70455
2024-08-21 19:52:27.834 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][28/77] Data 0.027 (0.029) Batch 0.067 (0.074) Remain 00:00:32 loss: 0.1970 data: 0.0013 Lr: 0.70455
2024-08-21 19:52:27.905 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][29/77] Data 0.028 (0.029) Batch 0.071 (0.074) Remain 00:00:32 loss: 0.1990 data: 0.0135 Lr: 0.70292
2024-08-21 19:52:27.905 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][29/77] Data 0.028 (0.029) Batch 0.071 (0.074) Remain 00:00:32 loss: 0.1990 data: -0.0093 Lr: 0.70292
2024-08-21 19:52:27.978 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][30/77] Data 0.031 (0.029) Batch 0.073 (0.074) Remain 00:00:31 loss: 0.1589 data: 0.0096 Lr: 0.70130
2024-08-21 19:52:27.978 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][30/77] Data 0.031 (0.029) Batch 0.073 (0.074) Remain 00:00:31 loss: 0.1589 data: -0.0269 Lr: 0.70130
2024-08-21 19:52:28.045 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][31/77] Data 0.028 (0.029) Batch 0.067 (0.074) Remain 00:00:31 loss: 0.1930 data: 0.0054 Lr: 0.69968
2024-08-21 19:52:28.045 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][31/77] Data 0.027 (0.029) Batch 0.067 (0.074) Remain 00:00:31 loss: 0.1930 data: 0.0053 Lr: 0.69968
2024-08-21 19:52:28.118 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][32/77] Data 0.028 (0.029) Batch 0.073 (0.074) Remain 00:00:31 loss: 0.1771 data: -0.0337 Lr: 0.69805
2024-08-21 19:52:28.118 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][32/77] Data 0.028 (0.029) Batch 0.073 (0.074) Remain 00:00:31 loss: 0.1771 data: -0.0045 Lr: 0.69805
2024-08-21 19:52:28.191 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][33/77] Data 0.030 (0.029) Batch 0.073 (0.074) Remain 00:00:31 loss: 0.2450 data: -0.0012 Lr: 0.69643
2024-08-21 19:52:28.191 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][33/77] Data 0.029 (0.029) Batch 0.073 (0.074) Remain 00:00:31 loss: 0.2450 data: 0.0139 Lr: 0.69643
2024-08-21 19:52:28.263 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][34/77] Data 0.029 (0.029) Batch 0.072 (0.074) Remain 00:00:31 loss: 0.1901 data: 0.0190 Lr: 0.69481
2024-08-21 19:52:28.263 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][34/77] Data 0.029 (0.029) Batch 0.072 (0.074) Remain 00:00:31 loss: 0.1901 data: 0.0132 Lr: 0.69481
2024-08-21 19:52:28.335 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][35/77] Data 0.030 (0.029) Batch 0.072 (0.074) Remain 00:00:31 loss: 0.1869 data: -0.0044 Lr: 0.69318
2024-08-21 19:52:28.335 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][35/77] Data 0.028 (0.029) Batch 0.072 (0.074) Remain 00:00:31 loss: 0.1869 data: 0.0115 Lr: 0.69318
2024-08-21 19:52:28.406 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][36/77] Data 0.029 (0.029) Batch 0.071 (0.073) Remain 00:00:31 loss: 0.1736 data: -0.0074 Lr: 0.69156
2024-08-21 19:52:28.406 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][36/77] Data 0.028 (0.029) Batch 0.071 (0.073) Remain 00:00:31 loss: 0.1736 data: -0.0134 Lr: 0.69156
2024-08-21 19:52:28.473 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][37/77] Data 0.028 (0.029) Batch 0.067 (0.073) Remain 00:00:31 loss: 0.2019 data: -0.0128 Lr: 0.68994
2024-08-21 19:52:28.473 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][37/77] Data 0.027 (0.029) Batch 0.067 (0.073) Remain 00:00:31 loss: 0.2019 data: -0.0070 Lr: 0.68994
2024-08-21 19:52:28.539 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][38/77] Data 0.027 (0.029) Batch 0.066 (0.073) Remain 00:00:31 loss: 0.2536 data: 0.0128 Lr: 0.68831
2024-08-21 19:52:28.539 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][38/77] Data 0.027 (0.029) Batch 0.066 (0.073) Remain 00:00:31 loss: 0.2536 data: 0.0175 Lr: 0.68831
2024-08-21 19:52:28.605 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][39/77] Data 0.028 (0.029) Batch 0.066 (0.073) Remain 00:00:30 loss: 0.2233 data: 0.0293 Lr: 0.68669
2024-08-21 19:52:28.605 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][39/77] Data 0.027 (0.029) Batch 0.066 (0.073) Remain 00:00:30 loss: 0.2233 data: 0.0159 Lr: 0.68669
2024-08-21 19:52:28.680 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][40/77] Data 0.029 (0.029) Batch 0.075 (0.073) Remain 00:00:30 loss: 0.2450 data: -0.0067 Lr: 0.68506
2024-08-21 19:52:28.680 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][40/77] Data 0.037 (0.029) Batch 0.075 (0.073) Remain 00:00:30 loss: 0.2450 data: -0.0013 Lr: 0.68506
2024-08-21 19:52:28.746 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][41/77] Data 0.027 (0.029) Batch 0.066 (0.073) Remain 00:00:30 loss: 0.1964 data: -0.0067 Lr: 0.68344
2024-08-21 19:52:28.746 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][41/77] Data 0.028 (0.029) Batch 0.066 (0.073) Remain 00:00:30 loss: 0.1964 data: -0.0098 Lr: 0.68344
2024-08-21 19:52:28.815 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][42/77] Data 0.028 (0.029) Batch 0.070 (0.073) Remain 00:00:30 loss: 0.2412 data: 0.0099 Lr: 0.68182
2024-08-21 19:52:28.815 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][42/77] Data 0.028 (0.029) Batch 0.070 (0.073) Remain 00:00:30 loss: 0.2412 data: -0.0034 Lr: 0.68182
2024-08-21 19:52:28.917 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][43/77] Data 0.046 (0.029) Batch 0.102 (0.073) Remain 00:00:30 loss: 0.1330 data: -0.0086 Lr: 0.68019
2024-08-21 19:52:28.917 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][43/77] Data 0.033 (0.029) Batch 0.102 (0.073) Remain 00:00:30 loss: 0.1330 data: -0.0002 Lr: 0.68019
2024-08-21 19:52:28.988 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][44/77] Data 0.032 (0.029) Batch 0.071 (0.073) Remain 00:00:30 loss: 0.2352 data: 0.0017 Lr: 0.67857
2024-08-21 19:52:28.988 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][44/77] Data 0.029 (0.029) Batch 0.071 (0.073) Remain 00:00:30 loss: 0.2352 data: 0.0015 Lr: 0.67857
2024-08-21 19:52:29.062 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][45/77] Data 0.028 (0.029) Batch 0.073 (0.073) Remain 00:00:30 loss: 0.1543 data: -0.0039 Lr: 0.67695
2024-08-21 19:52:29.062 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][45/77] Data 0.029 (0.029) Batch 0.073 (0.073) Remain 00:00:30 loss: 0.1543 data: -0.0113 Lr: 0.67695
2024-08-21 19:52:29.143 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][46/77] Data 0.039 (0.029) Batch 0.082 (0.074) Remain 00:00:30 loss: 0.2102 data: -0.0087 Lr: 0.67532
2024-08-21 19:52:29.144 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_200
2024-08-21 19:52:29.144 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][46/77] Data 0.030 (0.029) Batch 0.082 (0.074) Remain 00:00:30 loss: 0.2102 data: -0.0039 Lr: 0.67532
2024-08-21 19:52:29.144 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_200
2024-08-21 19:52:29.184 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -475.5408630371094
2024-08-21 19:52:29.185 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -222.51181030273438
2024-08-21 19:52:29.184 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -475.5408630371094
2024-08-21 19:52:29.185 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -253.029052734375
2024-08-21 19:52:29.261 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][47/77] Data 0.070 (0.030) Batch 0.117 (0.074) Remain 00:00:30 loss: 0.1650 data: 0.0070 Lr: 0.67370
2024-08-21 19:52:29.261 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][47/77] Data 0.071 (0.030) Batch 0.117 (0.074) Remain 00:00:30 loss: 0.1650 data: 0.0089 Lr: 0.67370
2024-08-21 19:52:29.343 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][48/77] Data 0.031 (0.030) Batch 0.083 (0.075) Remain 00:00:30 loss: 0.1767 data: -0.0072 Lr: 0.67208
2024-08-21 19:52:29.343 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][48/77] Data 0.030 (0.030) Batch 0.083 (0.075) Remain 00:00:30 loss: 0.1767 data: -0.0044 Lr: 0.67208
2024-08-21 19:52:29.416 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][49/77] Data 0.029 (0.030) Batch 0.073 (0.075) Remain 00:00:30 loss: 0.1702 data: 0.0042 Lr: 0.67045
2024-08-21 19:52:29.416 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][49/77] Data 0.030 (0.030) Batch 0.073 (0.075) Remain 00:00:30 loss: 0.1702 data: 0.0119 Lr: 0.67045
2024-08-21 19:52:29.488 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][50/77] Data 0.029 (0.030) Batch 0.072 (0.075) Remain 00:00:30 loss: 0.1572 data: -0.0272 Lr: 0.66883
2024-08-21 19:52:29.489 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][50/77] Data 0.029 (0.030) Batch 0.072 (0.075) Remain 00:00:30 loss: 0.1572 data: -0.0082 Lr: 0.66883
2024-08-21 19:52:29.561 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][51/77] Data 0.029 (0.030) Batch 0.073 (0.075) Remain 00:00:30 loss: 0.1499 data: -0.0063 Lr: 0.66721
2024-08-21 19:52:29.561 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][51/77] Data 0.029 (0.030) Batch 0.073 (0.075) Remain 00:00:30 loss: 0.1499 data: 0.0005 Lr: 0.66721
2024-08-21 19:52:29.635 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][52/77] Data 0.030 (0.030) Batch 0.074 (0.075) Remain 00:00:30 loss: 0.1313 data: -0.0056 Lr: 0.66558
2024-08-21 19:52:29.636 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][52/77] Data 0.030 (0.030) Batch 0.074 (0.075) Remain 00:00:30 loss: 0.1313 data: 0.0022 Lr: 0.66558
2024-08-21 19:52:29.709 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][53/77] Data 0.029 (0.030) Batch 0.074 (0.075) Remain 00:00:30 loss: 0.2138 data: 0.0066 Lr: 0.66396
2024-08-21 19:52:29.709 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][53/77] Data 0.030 (0.030) Batch 0.074 (0.075) Remain 00:00:30 loss: 0.2138 data: 0.0006 Lr: 0.66396
2024-08-21 19:52:29.782 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][54/77] Data 0.029 (0.030) Batch 0.073 (0.074) Remain 00:00:30 loss: 0.1629 data: 0.0115 Lr: 0.66234
2024-08-21 19:52:29.782 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][54/77] Data 0.029 (0.030) Batch 0.073 (0.074) Remain 00:00:30 loss: 0.1629 data: 0.0042 Lr: 0.66234
2024-08-21 19:52:29.849 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][55/77] Data 0.028 (0.030) Batch 0.067 (0.074) Remain 00:00:30 loss: 0.1104 data: 0.0112 Lr: 0.66071
2024-08-21 19:52:29.849 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][55/77] Data 0.028 (0.030) Batch 0.067 (0.074) Remain 00:00:30 loss: 0.1104 data: 0.0062 Lr: 0.66071
2024-08-21 19:52:29.921 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][56/77] Data 0.028 (0.030) Batch 0.073 (0.074) Remain 00:00:30 loss: 0.2505 data: 0.0039 Lr: 0.65909
2024-08-21 19:52:29.922 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][56/77] Data 0.028 (0.030) Batch 0.073 (0.074) Remain 00:00:30 loss: 0.2505 data: -0.0098 Lr: 0.65909
2024-08-21 19:52:29.990 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][57/77] Data 0.029 (0.030) Batch 0.069 (0.074) Remain 00:00:30 loss: 0.1393 data: -0.0106 Lr: 0.65747
2024-08-21 19:52:29.990 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][57/77] Data 0.029 (0.030) Batch 0.069 (0.074) Remain 00:00:30 loss: 0.1393 data: -0.0118 Lr: 0.65747
2024-08-21 19:52:30.059 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][58/77] Data 0.028 (0.030) Batch 0.069 (0.074) Remain 00:00:30 loss: 0.1816 data: -0.0058 Lr: 0.65584
2024-08-21 19:52:30.060 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][58/77] Data 0.028 (0.030) Batch 0.069 (0.074) Remain 00:00:30 loss: 0.1816 data: -0.0061 Lr: 0.65584
2024-08-21 19:52:30.131 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][59/77] Data 0.032 (0.030) Batch 0.071 (0.074) Remain 00:00:29 loss: 0.1829 data: 0.0112 Lr: 0.65422
2024-08-21 19:52:30.131 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][59/77] Data 0.028 (0.030) Batch 0.071 (0.074) Remain 00:00:29 loss: 0.1829 data: 0.0194 Lr: 0.65422
2024-08-21 19:52:30.198 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][60/77] Data 0.028 (0.030) Batch 0.068 (0.074) Remain 00:00:29 loss: 0.1219 data: -0.0083 Lr: 0.65260
2024-08-21 19:52:30.199 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][60/77] Data 0.028 (0.030) Batch 0.068 (0.074) Remain 00:00:29 loss: 0.1219 data: -0.0125 Lr: 0.65260
2024-08-21 19:52:30.266 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][61/77] Data 0.028 (0.030) Batch 0.068 (0.074) Remain 00:00:29 loss: 0.1026 data: 0.0105 Lr: 0.65097
2024-08-21 19:52:30.266 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][61/77] Data 0.028 (0.030) Batch 0.068 (0.074) Remain 00:00:29 loss: 0.1026 data: -0.0034 Lr: 0.65097
2024-08-21 19:52:30.338 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][62/77] Data 0.028 (0.030) Batch 0.072 (0.074) Remain 00:00:29 loss: 0.3017 data: 0.0028 Lr: 0.64935
2024-08-21 19:52:30.339 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][62/77] Data 0.028 (0.030) Batch 0.072 (0.074) Remain 00:00:29 loss: 0.3017 data: 0.0012 Lr: 0.64935
2024-08-21 19:52:30.411 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][63/77] Data 0.029 (0.030) Batch 0.072 (0.074) Remain 00:00:29 loss: 0.1439 data: 0.0114 Lr: 0.64773
2024-08-21 19:52:30.411 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][63/77] Data 0.029 (0.029) Batch 0.072 (0.074) Remain 00:00:29 loss: 0.1439 data: -0.0043 Lr: 0.64773
2024-08-21 19:52:30.483 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][64/77] Data 0.029 (0.030) Batch 0.072 (0.074) Remain 00:00:29 loss: 0.1950 data: -0.0030 Lr: 0.64610
2024-08-21 19:52:30.483 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][64/77] Data 0.029 (0.029) Batch 0.072 (0.074) Remain 00:00:29 loss: 0.1950 data: 0.0092 Lr: 0.64610
2024-08-21 19:52:30.555 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][65/77] Data 0.029 (0.030) Batch 0.072 (0.074) Remain 00:00:29 loss: 0.1481 data: 0.0200 Lr: 0.64448
2024-08-21 19:52:30.555 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][65/77] Data 0.029 (0.029) Batch 0.072 (0.074) Remain 00:00:29 loss: 0.1481 data: 0.0101 Lr: 0.64448
2024-08-21 19:52:30.621 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][66/77] Data 0.027 (0.030) Batch 0.066 (0.074) Remain 00:00:29 loss: 0.1708 data: -0.0093 Lr: 0.64286
2024-08-21 19:52:30.621 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][66/77] Data 0.027 (0.029) Batch 0.066 (0.074) Remain 00:00:29 loss: 0.1708 data: 0.0158 Lr: 0.64286
2024-08-21 19:52:30.688 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][67/77] Data 0.027 (0.030) Batch 0.067 (0.074) Remain 00:00:29 loss: 0.1717 data: -0.0030 Lr: 0.64123
2024-08-21 19:52:30.688 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][67/77] Data 0.028 (0.029) Batch 0.067 (0.074) Remain 00:00:29 loss: 0.1717 data: -0.0095 Lr: 0.64123
2024-08-21 19:52:30.754 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][68/77] Data 0.027 (0.030) Batch 0.066 (0.073) Remain 00:00:28 loss: 0.1176 data: -0.0148 Lr: 0.63961
2024-08-21 19:52:30.754 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][68/77] Data 0.027 (0.029) Batch 0.066 (0.073) Remain 00:00:28 loss: 0.1176 data: -0.0122 Lr: 0.63961
2024-08-21 19:52:30.821 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][69/77] Data 0.027 (0.030) Batch 0.067 (0.073) Remain 00:00:28 loss: 0.1610 data: -0.0094 Lr: 0.63799
2024-08-21 19:52:30.821 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][69/77] Data 0.027 (0.029) Batch 0.067 (0.073) Remain 00:00:28 loss: 0.1610 data: -0.0098 Lr: 0.63799
2024-08-21 19:52:30.895 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][70/77] Data 0.028 (0.030) Batch 0.075 (0.073) Remain 00:00:28 loss: 0.1315 data: -0.0026 Lr: 0.63636
2024-08-21 19:52:30.896 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][70/77] Data 0.033 (0.029) Batch 0.075 (0.073) Remain 00:00:28 loss: 0.1315 data: -0.0017 Lr: 0.63636
2024-08-21 19:52:30.966 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][71/77] Data 0.029 (0.030) Batch 0.071 (0.073) Remain 00:00:28 loss: 0.1535 data: 0.0111 Lr: 0.63474
2024-08-21 19:52:30.967 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][71/77] Data 0.028 (0.029) Batch 0.071 (0.073) Remain 00:00:28 loss: 0.1535 data: -0.0179 Lr: 0.63474
2024-08-21 19:52:31.037 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][72/77] Data 0.029 (0.030) Batch 0.071 (0.073) Remain 00:00:28 loss: 0.1682 data: 0.0086 Lr: 0.63312
2024-08-21 19:52:31.038 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][72/77] Data 0.028 (0.029) Batch 0.071 (0.073) Remain 00:00:28 loss: 0.1682 data: -0.0067 Lr: 0.63312
2024-08-21 19:52:31.109 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][73/77] Data 0.029 (0.030) Batch 0.072 (0.073) Remain 00:00:28 loss: 0.1353 data: 0.0017 Lr: 0.63149
2024-08-21 19:52:31.110 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][73/77] Data 0.029 (0.029) Batch 0.072 (0.073) Remain 00:00:28 loss: 0.1353 data: -0.0150 Lr: 0.63149
2024-08-21 19:52:31.181 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][74/77] Data 0.029 (0.030) Batch 0.071 (0.073) Remain 00:00:28 loss: 0.1715 data: -0.0004 Lr: 0.62987
2024-08-21 19:52:31.181 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][74/77] Data 0.029 (0.029) Batch 0.071 (0.073) Remain 00:00:28 loss: 0.1715 data: 0.0011 Lr: 0.62987
2024-08-21 19:52:31.253 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][75/77] Data 0.029 (0.030) Batch 0.072 (0.073) Remain 00:00:28 loss: 0.1643 data: -0.0026 Lr: 0.62825
2024-08-21 19:52:31.253 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][75/77] Data 0.028 (0.029) Batch 0.072 (0.073) Remain 00:00:28 loss: 0.1643 data: 0.0077 Lr: 0.62825
2024-08-21 19:52:31.323 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][76/77] Data 0.029 (0.030) Batch 0.071 (0.073) Remain 00:00:28 loss: 0.1376 data: -0.0095 Lr: 0.62662
2024-08-21 19:52:31.323 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][76/77] Data 0.028 (0.029) Batch 0.070 (0.073) Remain 00:00:28 loss: 0.1376 data: 0.0026 Lr: 0.62662
2024-08-21 19:52:31.365 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][77/77] Data 0.030 (0.030) Batch 0.041 (0.073) Remain 00:00:28 loss: 0.2094 data: -0.0129 Lr: 0.62500
2024-08-21 19:52:31.365 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 19:52:31.365 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][77/77] Data 0.030 (0.029) Batch 0.042 (0.073) Remain 00:00:28 loss: 0.2094 data: 0.0030 Lr: 0.62500
2024-08-21 19:52:31.365 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 19:52:36.799 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0486, Accuracy: 0.9843
2024-08-21 19:52:36.799 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0486, Accuracy: 0.9843
2024-08-21 19:52:36.799 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 19:52:36.799 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 19:52:36.800 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 19:52:36.800 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 19:52:36.908 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][1/77] Data 0.062 (0.062) Batch 0.107 (0.107) Remain 00:00:41 loss: 0.1434 data: -0.0047 Lr: 0.62338
2024-08-21 19:52:36.908 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][1/77] Data 0.059 (0.059) Batch 0.108 (0.108) Remain 00:00:41 loss: 0.1434 data: 0.0073 Lr: 0.62338
2024-08-21 19:52:36.980 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][2/77] Data 0.029 (0.029) Batch 0.072 (0.072) Remain 00:00:27 loss: 0.1587 data: -0.0086 Lr: 0.62175
2024-08-21 19:52:36.980 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][2/77] Data 0.029 (0.029) Batch 0.072 (0.072) Remain 00:00:27 loss: 0.1587 data: -0.0142 Lr: 0.62175
2024-08-21 19:52:37.052 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][3/77] Data 0.029 (0.029) Batch 0.072 (0.072) Remain 00:00:27 loss: 0.1448 data: 0.0005 Lr: 0.62013
2024-08-21 19:52:37.052 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][3/77] Data 0.028 (0.029) Batch 0.072 (0.072) Remain 00:00:27 loss: 0.1448 data: -0.0071 Lr: 0.62013
2024-08-21 19:52:37.119 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][4/77] Data 0.028 (0.029) Batch 0.067 (0.070) Remain 00:00:26 loss: 0.2313 data: -0.0244 Lr: 0.61851
2024-08-21 19:52:37.119 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][4/77] Data 0.027 (0.028) Batch 0.067 (0.070) Remain 00:00:26 loss: 0.2313 data: -0.0053 Lr: 0.61851
2024-08-21 19:52:37.187 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][5/77] Data 0.028 (0.029) Batch 0.068 (0.070) Remain 00:00:26 loss: 0.0985 data: 0.0004 Lr: 0.61688
2024-08-21 19:52:37.187 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][5/77] Data 0.028 (0.028) Batch 0.068 (0.070) Remain 00:00:26 loss: 0.0985 data: 0.0133 Lr: 0.61688
2024-08-21 19:52:37.267 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][6/77] Data 0.027 (0.028) Batch 0.080 (0.072) Remain 00:00:27 loss: 0.1920 data: 0.0016 Lr: 0.61526
2024-08-21 19:52:37.268 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][6/77] Data 0.028 (0.029) Batch 0.080 (0.072) Remain 00:00:27 loss: 0.1920 data: 0.0166 Lr: 0.61526
2024-08-21 19:52:37.341 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][7/77] Data 0.034 (0.030) Batch 0.074 (0.072) Remain 00:00:27 loss: 0.1572 data: -0.0050 Lr: 0.61364
2024-08-21 19:52:37.341 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][7/77] Data 0.027 (0.028) Batch 0.074 (0.072) Remain 00:00:27 loss: 0.1572 data: -0.0014 Lr: 0.61364
2024-08-21 19:52:37.409 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][8/77] Data 0.028 (0.029) Batch 0.068 (0.072) Remain 00:00:27 loss: 0.1722 data: -0.0056 Lr: 0.61201
2024-08-21 19:52:37.410 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][8/77] Data 0.027 (0.028) Batch 0.068 (0.072) Remain 00:00:27 loss: 0.1722 data: -0.0114 Lr: 0.61201
2024-08-21 19:52:37.481 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][9/77] Data 0.029 (0.029) Batch 0.072 (0.072) Remain 00:00:27 loss: 0.1131 data: 0.0029 Lr: 0.61039
2024-08-21 19:52:37.481 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][9/77] Data 0.028 (0.028) Batch 0.072 (0.072) Remain 00:00:27 loss: 0.1131 data: 0.0002 Lr: 0.61039
2024-08-21 19:52:37.553 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][10/77] Data 0.029 (0.029) Batch 0.071 (0.072) Remain 00:00:26 loss: 0.1186 data: 0.0060 Lr: 0.60877
2024-08-21 19:52:37.553 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][10/77] Data 0.028 (0.028) Batch 0.071 (0.072) Remain 00:00:26 loss: 0.1186 data: -0.0066 Lr: 0.60877
2024-08-21 19:52:37.624 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][11/77] Data 0.029 (0.029) Batch 0.071 (0.072) Remain 00:00:26 loss: 0.2154 data: 0.0084 Lr: 0.60714
2024-08-21 19:52:37.624 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][11/77] Data 0.028 (0.028) Batch 0.071 (0.072) Remain 00:00:26 loss: 0.2154 data: 0.0268 Lr: 0.60714
2024-08-21 19:52:37.696 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][12/77] Data 0.029 (0.029) Batch 0.072 (0.072) Remain 00:00:26 loss: 0.1222 data: 0.0143 Lr: 0.60552
2024-08-21 19:52:37.696 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][12/77] Data 0.028 (0.028) Batch 0.072 (0.072) Remain 00:00:26 loss: 0.1222 data: -0.0109 Lr: 0.60552
2024-08-21 19:52:37.772 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][13/77] Data 0.029 (0.029) Batch 0.076 (0.072) Remain 00:00:26 loss: 0.1540 data: 0.0112 Lr: 0.60390
2024-08-21 19:52:37.772 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][13/77] Data 0.028 (0.028) Batch 0.076 (0.072) Remain 00:00:26 loss: 0.1540 data: -0.0053 Lr: 0.60390
2024-08-21 19:52:37.843 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][14/77] Data 0.029 (0.029) Batch 0.071 (0.072) Remain 00:00:26 loss: 0.1585 data: 0.0104 Lr: 0.60227
2024-08-21 19:52:37.843 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][14/77] Data 0.028 (0.028) Batch 0.071 (0.072) Remain 00:00:26 loss: 0.1585 data: -0.0045 Lr: 0.60227
2024-08-21 19:52:37.909 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][15/77] Data 0.028 (0.029) Batch 0.066 (0.072) Remain 00:00:26 loss: 0.1526 data: -0.0038 Lr: 0.60065
2024-08-21 19:52:37.909 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][15/77] Data 0.027 (0.028) Batch 0.066 (0.072) Remain 00:00:26 loss: 0.1526 data: -0.0167 Lr: 0.60065
2024-08-21 19:52:37.976 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][16/77] Data 0.028 (0.029) Batch 0.067 (0.071) Remain 00:00:26 loss: 0.1369 data: -0.0055 Lr: 0.59903
2024-08-21 19:52:37.977 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][16/77] Data 0.027 (0.028) Batch 0.067 (0.071) Remain 00:00:26 loss: 0.1369 data: 0.0192 Lr: 0.59903
2024-08-21 19:52:38.055 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][17/77] Data 0.029 (0.029) Batch 0.078 (0.072) Remain 00:00:26 loss: 0.1479 data: -0.0110 Lr: 0.59740
2024-08-21 19:52:38.055 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][17/77] Data 0.028 (0.028) Batch 0.078 (0.072) Remain 00:00:26 loss: 0.1479 data: -0.0025 Lr: 0.59740
2024-08-21 19:52:38.121 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][18/77] Data 0.028 (0.029) Batch 0.067 (0.071) Remain 00:00:26 loss: 0.1236 data: 0.0192 Lr: 0.59578
2024-08-21 19:52:38.121 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][18/77] Data 0.027 (0.028) Batch 0.067 (0.071) Remain 00:00:26 loss: 0.1236 data: 0.0059 Lr: 0.59578
2024-08-21 19:52:38.189 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][19/77] Data 0.028 (0.029) Batch 0.067 (0.071) Remain 00:00:26 loss: 0.1158 data: 0.0032 Lr: 0.59416
2024-08-21 19:52:38.189 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_250
2024-08-21 19:52:38.189 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][19/77] Data 0.028 (0.028) Batch 0.067 (0.071) Remain 00:00:26 loss: 0.1158 data: 0.0162 Lr: 0.59416
2024-08-21 19:52:38.189 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_250
2024-08-21 19:52:38.217 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -496.0804443359375
2024-08-21 19:52:38.217 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -247.06309509277344
2024-08-21 19:52:38.217 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -496.0804443359375
2024-08-21 19:52:38.218 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -249.0173797607422
2024-08-21 19:52:38.287 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][20/77] Data 0.060 (0.031) Batch 0.099 (0.073) Remain 00:00:26 loss: 0.1299 data: 0.0024 Lr: 0.59253
2024-08-21 19:52:38.288 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][20/77] Data 0.056 (0.029) Batch 0.099 (0.073) Remain 00:00:26 loss: 0.1299 data: 0.0002 Lr: 0.59253
2024-08-21 19:52:38.354 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][21/77] Data 0.028 (0.030) Batch 0.067 (0.072) Remain 00:00:26 loss: 0.1526 data: -0.0058 Lr: 0.59091
2024-08-21 19:52:38.354 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][21/77] Data 0.027 (0.029) Batch 0.067 (0.072) Remain 00:00:26 loss: 0.1526 data: -0.0005 Lr: 0.59091
2024-08-21 19:52:38.420 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][22/77] Data 0.028 (0.030) Batch 0.066 (0.072) Remain 00:00:26 loss: 0.1205 data: 0.0166 Lr: 0.58929
2024-08-21 19:52:38.420 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][22/77] Data 0.027 (0.029) Batch 0.066 (0.072) Remain 00:00:26 loss: 0.1205 data: -0.0062 Lr: 0.58929
2024-08-21 19:52:38.491 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][23/77] Data 0.028 (0.030) Batch 0.070 (0.072) Remain 00:00:26 loss: 0.1125 data: 0.0021 Lr: 0.58766
2024-08-21 19:52:38.491 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][23/77] Data 0.028 (0.029) Batch 0.070 (0.072) Remain 00:00:26 loss: 0.1125 data: -0.0117 Lr: 0.58766
2024-08-21 19:52:38.563 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][24/77] Data 0.029 (0.030) Batch 0.072 (0.072) Remain 00:00:26 loss: 0.0835 data: -0.0047 Lr: 0.58604
2024-08-21 19:52:38.563 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][24/77] Data 0.028 (0.029) Batch 0.072 (0.072) Remain 00:00:26 loss: 0.0835 data: -0.0134 Lr: 0.58604
2024-08-21 19:52:38.638 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][25/77] Data 0.035 (0.030) Batch 0.076 (0.072) Remain 00:00:26 loss: 0.1311 data: -0.0039 Lr: 0.58442
2024-08-21 19:52:38.638 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][25/77] Data 0.028 (0.029) Batch 0.075 (0.072) Remain 00:00:26 loss: 0.1311 data: -0.0034 Lr: 0.58442
2024-08-21 19:52:38.704 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][26/77] Data 0.028 (0.030) Batch 0.066 (0.072) Remain 00:00:25 loss: 0.1240 data: 0.0041 Lr: 0.58279
2024-08-21 19:52:38.704 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][26/77] Data 0.027 (0.029) Batch 0.066 (0.072) Remain 00:00:25 loss: 0.1240 data: -0.0055 Lr: 0.58279
2024-08-21 19:52:38.770 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][27/77] Data 0.028 (0.030) Batch 0.066 (0.072) Remain 00:00:25 loss: 0.1754 data: -0.0054 Lr: 0.58117
2024-08-21 19:52:38.770 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][27/77] Data 0.027 (0.029) Batch 0.066 (0.072) Remain 00:00:25 loss: 0.1754 data: -0.0092 Lr: 0.58117
2024-08-21 19:52:38.835 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][28/77] Data 0.028 (0.030) Batch 0.065 (0.071) Remain 00:00:25 loss: 0.2012 data: 0.0072 Lr: 0.57955
2024-08-21 19:52:38.835 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][28/77] Data 0.027 (0.029) Batch 0.065 (0.071) Remain 00:00:25 loss: 0.2012 data: -0.0027 Lr: 0.57955
2024-08-21 19:52:38.900 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][29/77] Data 0.027 (0.030) Batch 0.065 (0.071) Remain 00:00:25 loss: 0.1287 data: -0.0074 Lr: 0.57792
2024-08-21 19:52:38.900 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][29/77] Data 0.027 (0.029) Batch 0.065 (0.071) Remain 00:00:25 loss: 0.1287 data: 0.0011 Lr: 0.57792
2024-08-21 19:52:38.965 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][30/77] Data 0.027 (0.030) Batch 0.065 (0.071) Remain 00:00:25 loss: 0.2373 data: 0.0148 Lr: 0.57630
2024-08-21 19:52:38.966 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][30/77] Data 0.027 (0.029) Batch 0.065 (0.071) Remain 00:00:25 loss: 0.2373 data: -0.0176 Lr: 0.57630
2024-08-21 19:52:39.031 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][31/77] Data 0.027 (0.030) Batch 0.065 (0.071) Remain 00:00:25 loss: 0.0588 data: -0.0003 Lr: 0.57468
2024-08-21 19:52:39.031 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][31/77] Data 0.027 (0.029) Batch 0.065 (0.071) Remain 00:00:25 loss: 0.0588 data: 0.0021 Lr: 0.57468
2024-08-21 19:52:39.096 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][32/77] Data 0.027 (0.030) Batch 0.065 (0.071) Remain 00:00:24 loss: 0.0871 data: -0.0023 Lr: 0.57305
2024-08-21 19:52:39.096 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][32/77] Data 0.027 (0.028) Batch 0.065 (0.071) Remain 00:00:24 loss: 0.0871 data: -0.0045 Lr: 0.57305
2024-08-21 19:52:39.165 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][33/77] Data 0.028 (0.030) Batch 0.069 (0.071) Remain 00:00:24 loss: 0.1276 data: -0.0035 Lr: 0.57143
2024-08-21 19:52:39.165 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][33/77] Data 0.029 (0.029) Batch 0.069 (0.071) Remain 00:00:24 loss: 0.1276 data: -0.0185 Lr: 0.57143
2024-08-21 19:52:39.239 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][34/77] Data 0.036 (0.030) Batch 0.074 (0.071) Remain 00:00:24 loss: 0.0901 data: -0.0073 Lr: 0.56981
2024-08-21 19:52:39.239 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][34/77] Data 0.027 (0.028) Batch 0.074 (0.071) Remain 00:00:24 loss: 0.0901 data: 0.0006 Lr: 0.56981
2024-08-21 19:52:39.304 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][35/77] Data 0.027 (0.030) Batch 0.065 (0.070) Remain 00:00:24 loss: 0.1068 data: 0.0062 Lr: 0.56818
2024-08-21 19:52:39.304 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][35/77] Data 0.027 (0.028) Batch 0.065 (0.070) Remain 00:00:24 loss: 0.1068 data: -0.0006 Lr: 0.56818
2024-08-21 19:52:39.369 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][36/77] Data 0.027 (0.030) Batch 0.065 (0.070) Remain 00:00:24 loss: 0.1094 data: -0.0185 Lr: 0.56656
2024-08-21 19:52:39.369 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][36/77] Data 0.027 (0.028) Batch 0.065 (0.070) Remain 00:00:24 loss: 0.1094 data: 0.0126 Lr: 0.56656
2024-08-21 19:52:39.435 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][37/77] Data 0.027 (0.030) Batch 0.065 (0.070) Remain 00:00:24 loss: 0.1363 data: 0.0076 Lr: 0.56494
2024-08-21 19:52:39.435 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][37/77] Data 0.027 (0.028) Batch 0.066 (0.070) Remain 00:00:24 loss: 0.1363 data: 0.0157 Lr: 0.56494
2024-08-21 19:52:39.503 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][38/77] Data 0.027 (0.028) Batch 0.068 (0.070) Remain 00:00:24 loss: 0.1554 data: -0.0074 Lr: 0.56331
2024-08-21 19:52:39.503 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][38/77] Data 0.027 (0.030) Batch 0.068 (0.070) Remain 00:00:24 loss: 0.1554 data: 0.0169 Lr: 0.56331
2024-08-21 19:52:39.573 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][39/77] Data 0.030 (0.030) Batch 0.070 (0.070) Remain 00:00:24 loss: 0.1537 data: 0.0041 Lr: 0.56169
2024-08-21 19:52:39.573 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][39/77] Data 0.028 (0.028) Batch 0.071 (0.070) Remain 00:00:24 loss: 0.1537 data: -0.0038 Lr: 0.56169
2024-08-21 19:52:39.644 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][40/77] Data 0.028 (0.030) Batch 0.070 (0.070) Remain 00:00:24 loss: 0.0960 data: -0.0162 Lr: 0.56006
2024-08-21 19:52:39.644 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][40/77] Data 0.027 (0.028) Batch 0.070 (0.070) Remain 00:00:24 loss: 0.0960 data: 0.0020 Lr: 0.56006
2024-08-21 19:52:39.718 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][41/77] Data 0.029 (0.030) Batch 0.074 (0.070) Remain 00:00:24 loss: 0.2661 data: -0.0118 Lr: 0.55844
2024-08-21 19:52:39.718 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][41/77] Data 0.028 (0.028) Batch 0.074 (0.070) Remain 00:00:24 loss: 0.2661 data: -0.0029 Lr: 0.55844
2024-08-21 19:52:39.796 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][42/77] Data 0.035 (0.030) Batch 0.079 (0.070) Remain 00:00:24 loss: 0.1841 data: 0.0077 Lr: 0.55682
2024-08-21 19:52:39.797 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][42/77] Data 0.028 (0.028) Batch 0.079 (0.070) Remain 00:00:24 loss: 0.1841 data: -0.0087 Lr: 0.55682
2024-08-21 19:52:39.870 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][43/77] Data 0.029 (0.030) Batch 0.073 (0.071) Remain 00:00:24 loss: 0.1256 data: 0.0034 Lr: 0.55519
2024-08-21 19:52:39.870 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][43/77] Data 0.028 (0.028) Batch 0.074 (0.071) Remain 00:00:24 loss: 0.1256 data: -0.0074 Lr: 0.55519
2024-08-21 19:52:39.944 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][44/77] Data 0.029 (0.030) Batch 0.074 (0.071) Remain 00:00:24 loss: 0.1482 data: -0.0086 Lr: 0.55357
2024-08-21 19:52:39.944 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][44/77] Data 0.032 (0.028) Batch 0.074 (0.071) Remain 00:00:24 loss: 0.1482 data: -0.0160 Lr: 0.55357
2024-08-21 19:52:40.016 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][45/77] Data 0.029 (0.030) Batch 0.072 (0.071) Remain 00:00:24 loss: 0.1248 data: 0.0038 Lr: 0.55195
2024-08-21 19:52:40.016 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][45/77] Data 0.028 (0.028) Batch 0.072 (0.071) Remain 00:00:24 loss: 0.1248 data: -0.0082 Lr: 0.55195
2024-08-21 19:52:40.086 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][46/77] Data 0.029 (0.030) Batch 0.070 (0.071) Remain 00:00:24 loss: 0.1889 data: -0.0025 Lr: 0.55032
2024-08-21 19:52:40.086 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][46/77] Data 0.028 (0.028) Batch 0.070 (0.071) Remain 00:00:24 loss: 0.1889 data: -0.0056 Lr: 0.55032
2024-08-21 19:52:40.156 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][47/77] Data 0.028 (0.030) Batch 0.070 (0.071) Remain 00:00:23 loss: 0.1761 data: 0.0173 Lr: 0.54870
2024-08-21 19:52:40.157 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][47/77] Data 0.028 (0.028) Batch 0.070 (0.071) Remain 00:00:23 loss: 0.1761 data: 0.0061 Lr: 0.54870
2024-08-21 19:52:40.228 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][48/77] Data 0.028 (0.030) Batch 0.071 (0.071) Remain 00:00:23 loss: 0.1540 data: -0.0020 Lr: 0.54708
2024-08-21 19:52:40.228 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][48/77] Data 0.028 (0.028) Batch 0.071 (0.071) Remain 00:00:23 loss: 0.1540 data: 0.0014 Lr: 0.54708
2024-08-21 19:52:40.301 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][49/77] Data 0.029 (0.030) Batch 0.074 (0.071) Remain 00:00:23 loss: 0.1151 data: -0.0034 Lr: 0.54545
2024-08-21 19:52:40.302 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][49/77] Data 0.032 (0.028) Batch 0.074 (0.071) Remain 00:00:23 loss: 0.1151 data: -0.0062 Lr: 0.54545
2024-08-21 19:52:40.384 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][50/77] Data 0.029 (0.030) Batch 0.083 (0.071) Remain 00:00:23 loss: 0.1674 data: -0.0050 Lr: 0.54383
2024-08-21 19:52:40.384 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][50/77] Data 0.036 (0.029) Batch 0.083 (0.071) Remain 00:00:23 loss: 0.1674 data: -0.0072 Lr: 0.54383
2024-08-21 19:52:40.450 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][51/77] Data 0.027 (0.029) Batch 0.066 (0.071) Remain 00:00:23 loss: 0.2378 data: -0.0151 Lr: 0.54221
2024-08-21 19:52:40.450 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][51/77] Data 0.027 (0.029) Batch 0.066 (0.071) Remain 00:00:23 loss: 0.2378 data: -0.0041 Lr: 0.54221
2024-08-21 19:52:40.528 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][52/77] Data 0.027 (0.029) Batch 0.078 (0.071) Remain 00:00:23 loss: 0.1498 data: 0.0062 Lr: 0.54058
2024-08-21 19:52:40.528 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][52/77] Data 0.034 (0.029) Batch 0.078 (0.071) Remain 00:00:23 loss: 0.1498 data: -0.0049 Lr: 0.54058
2024-08-21 19:52:40.609 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][53/77] Data 0.027 (0.029) Batch 0.081 (0.071) Remain 00:00:23 loss: 0.1655 data: -0.0032 Lr: 0.53896
2024-08-21 19:52:40.610 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][53/77] Data 0.034 (0.029) Batch 0.081 (0.071) Remain 00:00:23 loss: 0.1655 data: 0.0042 Lr: 0.53896
2024-08-21 19:52:40.676 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][54/77] Data 0.027 (0.029) Batch 0.066 (0.071) Remain 00:00:23 loss: 0.1077 data: 0.0097 Lr: 0.53734
2024-08-21 19:52:40.676 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][54/77] Data 0.027 (0.029) Batch 0.066 (0.071) Remain 00:00:23 loss: 0.1077 data: 0.0007 Lr: 0.53734
2024-08-21 19:52:40.744 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][55/77] Data 0.028 (0.029) Batch 0.068 (0.071) Remain 00:00:23 loss: 0.1042 data: -0.0165 Lr: 0.53571
2024-08-21 19:52:40.744 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][55/77] Data 0.028 (0.029) Batch 0.068 (0.071) Remain 00:00:23 loss: 0.1042 data: -0.0001 Lr: 0.53571
2024-08-21 19:52:40.809 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][56/77] Data 0.028 (0.029) Batch 0.065 (0.071) Remain 00:00:23 loss: 0.0695 data: -0.0279 Lr: 0.53409
2024-08-21 19:52:40.809 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][56/77] Data 0.027 (0.029) Batch 0.065 (0.071) Remain 00:00:23 loss: 0.0695 data: 0.0034 Lr: 0.53409
2024-08-21 19:52:40.877 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][57/77] Data 0.027 (0.029) Batch 0.067 (0.071) Remain 00:00:23 loss: 0.0841 data: -0.0152 Lr: 0.53247
2024-08-21 19:52:40.877 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][57/77] Data 0.027 (0.029) Batch 0.068 (0.071) Remain 00:00:23 loss: 0.0841 data: -0.0163 Lr: 0.53247
2024-08-21 19:52:40.954 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][58/77] Data 0.028 (0.029) Batch 0.077 (0.071) Remain 00:00:23 loss: 0.1493 data: -0.0062 Lr: 0.53084
2024-08-21 19:52:40.954 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][58/77] Data 0.030 (0.029) Batch 0.077 (0.071) Remain 00:00:23 loss: 0.1493 data: 0.0013 Lr: 0.53084
2024-08-21 19:52:41.019 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][59/77] Data 0.027 (0.029) Batch 0.065 (0.071) Remain 00:00:23 loss: 0.2542 data: -0.0070 Lr: 0.52922
2024-08-21 19:52:41.019 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][59/77] Data 0.027 (0.029) Batch 0.065 (0.071) Remain 00:00:23 loss: 0.2542 data: 0.0049 Lr: 0.52922
2024-08-21 19:52:41.085 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][60/77] Data 0.027 (0.029) Batch 0.066 (0.071) Remain 00:00:23 loss: 0.1579 data: -0.0038 Lr: 0.52760
2024-08-21 19:52:41.085 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][60/77] Data 0.027 (0.029) Batch 0.066 (0.071) Remain 00:00:23 loss: 0.1579 data: 0.0050 Lr: 0.52760
2024-08-21 19:52:41.151 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][61/77] Data 0.027 (0.029) Batch 0.066 (0.071) Remain 00:00:22 loss: 0.1805 data: 0.0051 Lr: 0.52597
2024-08-21 19:52:41.151 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][61/77] Data 0.027 (0.029) Batch 0.066 (0.071) Remain 00:00:22 loss: 0.1805 data: 0.0004 Lr: 0.52597
2024-08-21 19:52:41.216 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][62/77] Data 0.027 (0.029) Batch 0.066 (0.071) Remain 00:00:22 loss: 0.1287 data: -0.0009 Lr: 0.52435
2024-08-21 19:52:41.216 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][62/77] Data 0.027 (0.029) Batch 0.066 (0.071) Remain 00:00:22 loss: 0.1287 data: 0.0161 Lr: 0.52435
2024-08-21 19:52:41.281 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][63/77] Data 0.027 (0.029) Batch 0.065 (0.071) Remain 00:00:22 loss: 0.0480 data: -0.0041 Lr: 0.52273
2024-08-21 19:52:41.282 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][63/77] Data 0.027 (0.029) Batch 0.065 (0.071) Remain 00:00:22 loss: 0.0480 data: 0.0031 Lr: 0.52273
2024-08-21 19:52:41.348 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][64/77] Data 0.027 (0.029) Batch 0.066 (0.070) Remain 00:00:22 loss: 0.0777 data: -0.0126 Lr: 0.52110
2024-08-21 19:52:41.348 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][64/77] Data 0.027 (0.029) Batch 0.066 (0.070) Remain 00:00:22 loss: 0.0777 data: 0.0043 Lr: 0.52110
2024-08-21 19:52:41.424 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][65/77] Data 0.028 (0.029) Batch 0.077 (0.071) Remain 00:00:22 loss: 0.0652 data: -0.0044 Lr: 0.51948
2024-08-21 19:52:41.424 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][65/77] Data 0.028 (0.028) Batch 0.077 (0.071) Remain 00:00:22 loss: 0.0652 data: 0.0093 Lr: 0.51948
2024-08-21 19:52:41.503 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][66/77] Data 0.027 (0.029) Batch 0.078 (0.071) Remain 00:00:22 loss: 0.0536 data: -0.0004 Lr: 0.51786
2024-08-21 19:52:41.503 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][66/77] Data 0.030 (0.029) Batch 0.078 (0.071) Remain 00:00:22 loss: 0.0536 data: -0.0055 Lr: 0.51786
2024-08-21 19:52:41.576 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][67/77] Data 0.027 (0.029) Batch 0.074 (0.071) Remain 00:00:22 loss: 0.0794 data: -0.0107 Lr: 0.51623
2024-08-21 19:52:41.576 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][67/77] Data 0.028 (0.029) Batch 0.074 (0.071) Remain 00:00:22 loss: 0.0794 data: -0.0045 Lr: 0.51623
2024-08-21 19:52:41.641 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][68/77] Data 0.027 (0.029) Batch 0.065 (0.071) Remain 00:00:22 loss: 0.1061 data: 0.0190 Lr: 0.51461
2024-08-21 19:52:41.641 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][68/77] Data 0.027 (0.028) Batch 0.065 (0.071) Remain 00:00:22 loss: 0.1061 data: -0.0070 Lr: 0.51461
2024-08-21 19:52:41.707 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][69/77] Data 0.027 (0.029) Batch 0.065 (0.071) Remain 00:00:22 loss: 0.1470 data: 0.0135 Lr: 0.51299
2024-08-21 19:52:41.707 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_300
2024-08-21 19:52:41.707 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][69/77] Data 0.027 (0.028) Batch 0.065 (0.071) Remain 00:00:22 loss: 0.1470 data: -0.0104 Lr: 0.51299
2024-08-21 19:52:41.707 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_300
2024-08-21 19:52:41.733 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -534.2822875976562
2024-08-21 19:52:41.733 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -534.2822875976562
2024-08-21 19:52:41.733 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -262.61199951171875
2024-08-21 19:52:41.733 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -271.6702880859375
2024-08-21 19:52:41.802 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][70/77] Data 0.054 (0.029) Batch 0.096 (0.071) Remain 00:00:22 loss: 0.1335 data: 0.0258 Lr: 0.51136
2024-08-21 19:52:41.802 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][70/77] Data 0.054 (0.029) Batch 0.096 (0.071) Remain 00:00:22 loss: 0.1335 data: -0.0014 Lr: 0.51136
2024-08-21 19:52:41.868 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][71/77] Data 0.027 (0.029) Batch 0.066 (0.071) Remain 00:00:22 loss: 0.0892 data: 0.0188 Lr: 0.50974
2024-08-21 19:52:41.868 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][71/77] Data 0.027 (0.029) Batch 0.066 (0.071) Remain 00:00:22 loss: 0.0892 data: -0.0099 Lr: 0.50974
2024-08-21 19:52:41.935 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][72/77] Data 0.028 (0.029) Batch 0.066 (0.071) Remain 00:00:22 loss: 0.1639 data: -0.0225 Lr: 0.50812
2024-08-21 19:52:41.935 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][72/77] Data 0.028 (0.029) Batch 0.066 (0.071) Remain 00:00:22 loss: 0.1639 data: 0.0022 Lr: 0.50812
2024-08-21 19:52:42.002 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][73/77] Data 0.027 (0.029) Batch 0.067 (0.071) Remain 00:00:22 loss: 0.1310 data: 0.0146 Lr: 0.50649
2024-08-21 19:52:42.002 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][73/77] Data 0.027 (0.029) Batch 0.067 (0.071) Remain 00:00:22 loss: 0.1310 data: 0.0028 Lr: 0.50649
2024-08-21 19:52:42.072 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][74/77] Data 0.028 (0.029) Batch 0.071 (0.071) Remain 00:00:22 loss: 0.1121 data: 0.0089 Lr: 0.50487
2024-08-21 19:52:42.072 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][74/77] Data 0.028 (0.029) Batch 0.071 (0.071) Remain 00:00:22 loss: 0.1121 data: 0.0203 Lr: 0.50487
2024-08-21 19:52:42.161 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][75/77] Data 0.029 (0.029) Batch 0.088 (0.071) Remain 00:00:22 loss: 0.1512 data: 0.0108 Lr: 0.50325
2024-08-21 19:52:42.161 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][75/77] Data 0.045 (0.029) Batch 0.089 (0.071) Remain 00:00:22 loss: 0.1512 data: 0.0153 Lr: 0.50325
2024-08-21 19:52:42.233 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][76/77] Data 0.028 (0.029) Batch 0.072 (0.071) Remain 00:00:22 loss: 0.1218 data: -0.0068 Lr: 0.50162
2024-08-21 19:52:42.233 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][76/77] Data 0.029 (0.029) Batch 0.072 (0.071) Remain 00:00:22 loss: 0.1218 data: -0.0105 Lr: 0.50162
2024-08-21 19:52:42.278 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][77/77] Data 0.032 (0.029) Batch 0.046 (0.071) Remain 00:00:21 loss: 0.1031 data: 0.0080 Lr: 0.50000
2024-08-21 19:52:42.279 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 19:52:42.279 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][77/77] Data 0.031 (0.029) Batch 0.046 (0.071) Remain 00:00:21 loss: 0.1031 data: 0.0143 Lr: 0.50000
2024-08-21 19:52:42.279 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 19:52:46.643 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0384, Accuracy: 0.9865
2024-08-21 19:52:46.643 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0384, Accuracy: 0.9865
2024-08-21 19:52:46.643 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 19:52:46.643 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 19:52:46.643 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 19:52:46.643 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 19:52:46.738 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][1/77] Data 0.057 (0.057) Batch 0.095 (0.095) Remain 00:00:29 loss: 0.1658 data: 0.0083 Lr: 0.49838
2024-08-21 19:52:46.738 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][1/77] Data 0.043 (0.043) Batch 0.095 (0.095) Remain 00:00:29 loss: 0.1658 data: 0.0052 Lr: 0.49838
2024-08-21 19:52:46.802 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][2/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:19 loss: 0.1151 data: 0.0093 Lr: 0.49675
2024-08-21 19:52:46.802 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][2/77] Data 0.023 (0.023) Batch 0.064 (0.064) Remain 00:00:19 loss: 0.1151 data: 0.0029 Lr: 0.49675
2024-08-21 19:52:46.866 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][3/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:19 loss: 0.1291 data: -0.0161 Lr: 0.49513
2024-08-21 19:52:46.866 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][3/77] Data 0.021 (0.022) Batch 0.064 (0.064) Remain 00:00:19 loss: 0.1291 data: -0.0005 Lr: 0.49513
2024-08-21 19:52:46.931 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][4/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:19 loss: 0.1344 data: 0.0085 Lr: 0.49351
2024-08-21 19:52:46.931 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][4/77] Data 0.022 (0.022) Batch 0.064 (0.064) Remain 00:00:19 loss: 0.1344 data: -0.0001 Lr: 0.49351
2024-08-21 19:52:47.011 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][5/77] Data 0.027 (0.027) Batch 0.080 (0.068) Remain 00:00:20 loss: 0.1415 data: -0.0002 Lr: 0.49188
2024-08-21 19:52:47.011 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][5/77] Data 0.027 (0.023) Batch 0.080 (0.068) Remain 00:00:20 loss: 0.1415 data: 0.0131 Lr: 0.49188
2024-08-21 19:52:47.076 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][6/77] Data 0.027 (0.027) Batch 0.065 (0.067) Remain 00:00:20 loss: 0.1329 data: -0.0010 Lr: 0.49026
2024-08-21 19:52:47.076 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][6/77] Data 0.023 (0.023) Batch 0.065 (0.067) Remain 00:00:20 loss: 0.1329 data: 0.0141 Lr: 0.49026
2024-08-21 19:52:47.140 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][7/77] Data 0.027 (0.027) Batch 0.065 (0.067) Remain 00:00:20 loss: 0.1477 data: -0.0143 Lr: 0.48864
2024-08-21 19:52:47.140 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][7/77] Data 0.022 (0.023) Batch 0.065 (0.067) Remain 00:00:20 loss: 0.1477 data: -0.0092 Lr: 0.48864
2024-08-21 19:52:47.205 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][8/77] Data 0.027 (0.027) Batch 0.065 (0.067) Remain 00:00:20 loss: 0.0949 data: 0.0009 Lr: 0.48701
2024-08-21 19:52:47.205 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][8/77] Data 0.022 (0.023) Batch 0.065 (0.067) Remain 00:00:20 loss: 0.0949 data: -0.0091 Lr: 0.48701
2024-08-21 19:52:47.270 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][9/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:19 loss: 0.0945 data: 0.0181 Lr: 0.48539
2024-08-21 19:52:47.270 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][9/77] Data 0.022 (0.023) Batch 0.065 (0.066) Remain 00:00:19 loss: 0.0945 data: 0.0132 Lr: 0.48539
2024-08-21 19:52:47.335 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][10/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:19 loss: 0.0847 data: -0.0050 Lr: 0.48377
2024-08-21 19:52:47.335 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][10/77] Data 0.022 (0.023) Batch 0.065 (0.066) Remain 00:00:19 loss: 0.0847 data: 0.0072 Lr: 0.48377
2024-08-21 19:52:47.400 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][11/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:19 loss: 0.1087 data: 0.0128 Lr: 0.48214
2024-08-21 19:52:47.400 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][11/77] Data 0.021 (0.023) Batch 0.065 (0.066) Remain 00:00:19 loss: 0.1087 data: 0.0006 Lr: 0.48214
2024-08-21 19:52:47.465 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][12/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:19 loss: 0.1161 data: 0.0025 Lr: 0.48052
2024-08-21 19:52:47.465 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][12/77] Data 0.021 (0.022) Batch 0.065 (0.066) Remain 00:00:19 loss: 0.1161 data: -0.0087 Lr: 0.48052
2024-08-21 19:52:47.530 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][13/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:19 loss: 0.0677 data: 0.0144 Lr: 0.47890
2024-08-21 19:52:47.530 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][13/77] Data 0.022 (0.022) Batch 0.065 (0.066) Remain 00:00:19 loss: 0.0677 data: 0.0046 Lr: 0.47890
2024-08-21 19:52:47.595 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][14/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:19 loss: 0.1154 data: -0.0103 Lr: 0.47727
2024-08-21 19:52:47.595 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][14/77] Data 0.021 (0.022) Batch 0.065 (0.066) Remain 00:00:19 loss: 0.1154 data: 0.0026 Lr: 0.47727
2024-08-21 19:52:47.659 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][15/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:19 loss: 0.0741 data: 0.0008 Lr: 0.47565
2024-08-21 19:52:47.659 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][15/77] Data 0.023 (0.022) Batch 0.065 (0.066) Remain 00:00:19 loss: 0.0741 data: -0.0049 Lr: 0.47565
2024-08-21 19:52:47.725 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][16/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:19 loss: 0.1352 data: -0.0114 Lr: 0.47403
2024-08-21 19:52:47.725 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][16/77] Data 0.021 (0.022) Batch 0.065 (0.066) Remain 00:00:19 loss: 0.1352 data: 0.0060 Lr: 0.47403
2024-08-21 19:52:47.792 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][17/77] Data 0.027 (0.027) Batch 0.067 (0.066) Remain 00:00:19 loss: 0.1030 data: -0.0005 Lr: 0.47240
2024-08-21 19:52:47.792 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][17/77] Data 0.022 (0.022) Batch 0.067 (0.066) Remain 00:00:19 loss: 0.1030 data: -0.0153 Lr: 0.47240
2024-08-21 19:52:47.856 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][18/77] Data 0.027 (0.027) Batch 0.064 (0.066) Remain 00:00:19 loss: 0.1112 data: 0.0003 Lr: 0.47078
2024-08-21 19:52:47.856 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][18/77] Data 0.021 (0.022) Batch 0.064 (0.066) Remain 00:00:19 loss: 0.1112 data: -0.0132 Lr: 0.47078
2024-08-21 19:52:47.923 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][19/77] Data 0.033 (0.027) Batch 0.067 (0.066) Remain 00:00:19 loss: 0.1197 data: 0.0128 Lr: 0.46916
2024-08-21 19:52:47.923 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][19/77] Data 0.022 (0.022) Batch 0.067 (0.066) Remain 00:00:19 loss: 0.1197 data: 0.0028 Lr: 0.46916
2024-08-21 19:52:47.987 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][20/77] Data 0.027 (0.027) Batch 0.064 (0.066) Remain 00:00:18 loss: 0.1510 data: -0.0035 Lr: 0.46753
2024-08-21 19:52:47.987 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][20/77] Data 0.022 (0.022) Batch 0.064 (0.066) Remain 00:00:18 loss: 0.1510 data: -0.0049 Lr: 0.46753
2024-08-21 19:52:48.052 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][21/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:18 loss: 0.1274 data: 0.0170 Lr: 0.46591
2024-08-21 19:52:48.052 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][21/77] Data 0.022 (0.022) Batch 0.065 (0.066) Remain 00:00:18 loss: 0.1274 data: 0.0059 Lr: 0.46591
2024-08-21 19:52:48.116 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][22/77] Data 0.027 (0.027) Batch 0.064 (0.066) Remain 00:00:18 loss: 0.2249 data: -0.0123 Lr: 0.46429
2024-08-21 19:52:48.116 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][22/77] Data 0.022 (0.022) Batch 0.064 (0.066) Remain 00:00:18 loss: 0.2249 data: -0.0073 Lr: 0.46429
2024-08-21 19:52:48.178 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][23/77] Data 0.027 (0.027) Batch 0.063 (0.065) Remain 00:00:18 loss: 0.1440 data: 0.0074 Lr: 0.46266
2024-08-21 19:52:48.178 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][23/77] Data 0.022 (0.022) Batch 0.063 (0.065) Remain 00:00:18 loss: 0.1440 data: -0.0111 Lr: 0.46266
2024-08-21 19:52:48.258 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][24/77] Data 0.039 (0.028) Batch 0.080 (0.066) Remain 00:00:18 loss: 0.1046 data: 0.0008 Lr: 0.46104
2024-08-21 19:52:48.258 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][24/77] Data 0.023 (0.022) Batch 0.080 (0.066) Remain 00:00:18 loss: 0.1046 data: 0.0007 Lr: 0.46104
2024-08-21 19:52:48.331 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][25/77] Data 0.035 (0.028) Batch 0.073 (0.066) Remain 00:00:18 loss: 0.2121 data: 0.0049 Lr: 0.45942
2024-08-21 19:52:48.331 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][25/77] Data 0.021 (0.022) Batch 0.073 (0.066) Remain 00:00:18 loss: 0.2121 data: 0.0033 Lr: 0.45942
2024-08-21 19:52:48.395 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][26/77] Data 0.027 (0.028) Batch 0.064 (0.066) Remain 00:00:18 loss: 0.1102 data: 0.0082 Lr: 0.45779
2024-08-21 19:52:48.395 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][26/77] Data 0.022 (0.022) Batch 0.064 (0.066) Remain 00:00:18 loss: 0.1102 data: 0.0042 Lr: 0.45779
2024-08-21 19:52:48.459 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][27/77] Data 0.027 (0.028) Batch 0.064 (0.066) Remain 00:00:18 loss: 0.1571 data: -0.0030 Lr: 0.45617
2024-08-21 19:52:48.459 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][27/77] Data 0.023 (0.022) Batch 0.064 (0.066) Remain 00:00:18 loss: 0.1571 data: 0.0168 Lr: 0.45617
2024-08-21 19:52:48.523 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][28/77] Data 0.027 (0.028) Batch 0.064 (0.066) Remain 00:00:18 loss: 0.1478 data: 0.0003 Lr: 0.45455
2024-08-21 19:52:48.523 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][28/77] Data 0.023 (0.022) Batch 0.064 (0.066) Remain 00:00:18 loss: 0.1478 data: -0.0069 Lr: 0.45455
2024-08-21 19:52:48.587 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][29/77] Data 0.027 (0.028) Batch 0.064 (0.066) Remain 00:00:18 loss: 0.1863 data: 0.0033 Lr: 0.45292
2024-08-21 19:52:48.587 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][29/77] Data 0.023 (0.022) Batch 0.064 (0.066) Remain 00:00:18 loss: 0.1863 data: 0.0206 Lr: 0.45292
2024-08-21 19:52:48.651 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][30/77] Data 0.027 (0.028) Batch 0.064 (0.066) Remain 00:00:18 loss: 0.1179 data: -0.0038 Lr: 0.45130
2024-08-21 19:52:48.651 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][30/77] Data 0.023 (0.022) Batch 0.064 (0.066) Remain 00:00:18 loss: 0.1179 data: -0.0056 Lr: 0.45130
2024-08-21 19:52:48.715 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][31/77] Data 0.027 (0.028) Batch 0.064 (0.066) Remain 00:00:18 loss: 0.1313 data: -0.0020 Lr: 0.44968
2024-08-21 19:52:48.715 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][31/77] Data 0.023 (0.022) Batch 0.064 (0.066) Remain 00:00:18 loss: 0.1313 data: -0.0062 Lr: 0.44968
2024-08-21 19:52:48.779 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][32/77] Data 0.027 (0.028) Batch 0.064 (0.066) Remain 00:00:18 loss: 0.1416 data: 0.0069 Lr: 0.44805
2024-08-21 19:52:48.779 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][32/77] Data 0.023 (0.022) Batch 0.064 (0.066) Remain 00:00:18 loss: 0.1416 data: -0.0094 Lr: 0.44805
2024-08-21 19:52:48.849 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][33/77] Data 0.027 (0.028) Batch 0.070 (0.066) Remain 00:00:18 loss: 0.1455 data: 0.0004 Lr: 0.44643
2024-08-21 19:52:48.849 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][33/77] Data 0.022 (0.022) Batch 0.070 (0.066) Remain 00:00:18 loss: 0.1455 data: -0.0007 Lr: 0.44643
2024-08-21 19:52:48.925 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][34/77] Data 0.027 (0.028) Batch 0.076 (0.066) Remain 00:00:18 loss: 0.1329 data: -0.0012 Lr: 0.44481
2024-08-21 19:52:48.925 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][34/77] Data 0.033 (0.023) Batch 0.076 (0.066) Remain 00:00:18 loss: 0.1329 data: 0.0023 Lr: 0.44481
2024-08-21 19:52:49.002 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][35/77] Data 0.034 (0.028) Batch 0.077 (0.067) Remain 00:00:18 loss: 0.0972 data: -0.0101 Lr: 0.44318
2024-08-21 19:52:49.003 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][35/77] Data 0.031 (0.023) Batch 0.078 (0.067) Remain 00:00:18 loss: 0.0972 data: 0.0042 Lr: 0.44318
2024-08-21 19:52:49.083 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][36/77] Data 0.041 (0.028) Batch 0.081 (0.067) Remain 00:00:18 loss: 0.0755 data: -0.0069 Lr: 0.44156
2024-08-21 19:52:49.083 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][36/77] Data 0.030 (0.023) Batch 0.081 (0.067) Remain 00:00:18 loss: 0.0755 data: -0.0051 Lr: 0.44156
2024-08-21 19:52:49.156 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][37/77] Data 0.036 (0.029) Batch 0.073 (0.067) Remain 00:00:18 loss: 0.2141 data: -0.0185 Lr: 0.43994
2024-08-21 19:52:49.156 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][37/77] Data 0.023 (0.023) Batch 0.072 (0.067) Remain 00:00:18 loss: 0.2141 data: -0.0017 Lr: 0.43994
2024-08-21 19:52:49.220 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][38/77] Data 0.027 (0.029) Batch 0.065 (0.067) Remain 00:00:18 loss: 0.0798 data: -0.0115 Lr: 0.43831
2024-08-21 19:52:49.220 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][38/77] Data 0.021 (0.023) Batch 0.065 (0.067) Remain 00:00:18 loss: 0.0798 data: -0.0026 Lr: 0.43831
2024-08-21 19:52:49.285 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][39/77] Data 0.027 (0.029) Batch 0.065 (0.067) Remain 00:00:18 loss: 0.0781 data: 0.0002 Lr: 0.43669
2024-08-21 19:52:49.285 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][39/77] Data 0.022 (0.023) Batch 0.065 (0.067) Remain 00:00:18 loss: 0.0781 data: 0.0017 Lr: 0.43669
2024-08-21 19:52:49.349 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][40/77] Data 0.027 (0.029) Batch 0.064 (0.067) Remain 00:00:18 loss: 0.1188 data: 0.0048 Lr: 0.43506
2024-08-21 19:52:49.349 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][40/77] Data 0.022 (0.023) Batch 0.064 (0.067) Remain 00:00:18 loss: 0.1188 data: 0.0082 Lr: 0.43506
2024-08-21 19:52:49.414 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][41/77] Data 0.027 (0.029) Batch 0.065 (0.067) Remain 00:00:17 loss: 0.1566 data: -0.0055 Lr: 0.43344
2024-08-21 19:52:49.414 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][41/77] Data 0.023 (0.023) Batch 0.065 (0.067) Remain 00:00:17 loss: 0.1566 data: 0.0003 Lr: 0.43344
2024-08-21 19:52:49.477 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][42/77] Data 0.027 (0.028) Batch 0.063 (0.067) Remain 00:00:17 loss: 0.1835 data: -0.0115 Lr: 0.43182
2024-08-21 19:52:49.477 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_350
2024-08-21 19:52:49.477 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][42/77] Data 0.023 (0.023) Batch 0.063 (0.067) Remain 00:00:17 loss: 0.1835 data: -0.0019 Lr: 0.43182
2024-08-21 19:52:49.477 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_350
2024-08-21 19:52:49.499 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -551.7533569335938
2024-08-21 19:52:49.500 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -281.30084228515625
2024-08-21 19:52:49.500 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -551.7533569335938
2024-08-21 19:52:49.500 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -270.45257568359375
2024-08-21 19:52:49.566 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][43/77] Data 0.050 (0.029) Batch 0.090 (0.067) Remain 00:00:17 loss: 0.1377 data: 0.0048 Lr: 0.43019
2024-08-21 19:52:49.566 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][43/77] Data 0.044 (0.023) Batch 0.090 (0.067) Remain 00:00:17 loss: 0.1377 data: 0.0038 Lr: 0.43019
2024-08-21 19:52:49.632 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][44/77] Data 0.027 (0.029) Batch 0.066 (0.067) Remain 00:00:17 loss: 0.0799 data: -0.0127 Lr: 0.42857
2024-08-21 19:52:49.632 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][44/77] Data 0.023 (0.023) Batch 0.066 (0.067) Remain 00:00:17 loss: 0.0799 data: -0.0067 Lr: 0.42857
2024-08-21 19:52:49.697 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][45/77] Data 0.027 (0.029) Batch 0.065 (0.067) Remain 00:00:17 loss: 0.1538 data: 0.0013 Lr: 0.42695
2024-08-21 19:52:49.697 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][45/77] Data 0.022 (0.023) Batch 0.065 (0.067) Remain 00:00:17 loss: 0.1538 data: 0.0039 Lr: 0.42695
2024-08-21 19:52:49.761 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][46/77] Data 0.027 (0.029) Batch 0.064 (0.067) Remain 00:00:17 loss: 0.1058 data: -0.0181 Lr: 0.42532
2024-08-21 19:52:49.761 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][46/77] Data 0.022 (0.023) Batch 0.064 (0.067) Remain 00:00:17 loss: 0.1058 data: 0.0130 Lr: 0.42532
2024-08-21 19:52:49.838 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][47/77] Data 0.028 (0.029) Batch 0.077 (0.067) Remain 00:00:17 loss: 0.1383 data: -0.0068 Lr: 0.42370
2024-08-21 19:52:49.838 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][47/77] Data 0.022 (0.023) Batch 0.077 (0.067) Remain 00:00:17 loss: 0.1383 data: -0.0153 Lr: 0.42370
2024-08-21 19:52:49.908 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][48/77] Data 0.032 (0.029) Batch 0.069 (0.067) Remain 00:00:17 loss: 0.1230 data: -0.0043 Lr: 0.42208
2024-08-21 19:52:49.908 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][48/77] Data 0.021 (0.023) Batch 0.070 (0.067) Remain 00:00:17 loss: 0.1230 data: 0.0115 Lr: 0.42208
2024-08-21 19:52:49.981 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][49/77] Data 0.030 (0.029) Batch 0.073 (0.068) Remain 00:00:17 loss: 0.1189 data: -0.0033 Lr: 0.42045
2024-08-21 19:52:49.981 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][49/77] Data 0.021 (0.023) Batch 0.073 (0.068) Remain 00:00:17 loss: 0.1189 data: 0.0087 Lr: 0.42045
2024-08-21 19:52:50.046 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][50/77] Data 0.027 (0.029) Batch 0.065 (0.067) Remain 00:00:17 loss: 0.1229 data: 0.0039 Lr: 0.41883
2024-08-21 19:52:50.046 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][50/77] Data 0.021 (0.023) Batch 0.065 (0.067) Remain 00:00:17 loss: 0.1229 data: 0.0019 Lr: 0.41883
2024-08-21 19:52:50.120 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][51/77] Data 0.032 (0.029) Batch 0.074 (0.068) Remain 00:00:17 loss: 0.1353 data: 0.0043 Lr: 0.41721
2024-08-21 19:52:50.120 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][51/77] Data 0.022 (0.023) Batch 0.074 (0.068) Remain 00:00:17 loss: 0.1353 data: -0.0014 Lr: 0.41721
2024-08-21 19:52:50.184 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][52/77] Data 0.027 (0.029) Batch 0.064 (0.068) Remain 00:00:17 loss: 0.1119 data: 0.0039 Lr: 0.41558
2024-08-21 19:52:50.184 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][52/77] Data 0.021 (0.023) Batch 0.064 (0.068) Remain 00:00:17 loss: 0.1119 data: 0.0043 Lr: 0.41558
2024-08-21 19:52:50.249 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][53/77] Data 0.027 (0.029) Batch 0.065 (0.068) Remain 00:00:17 loss: 0.1621 data: 0.0042 Lr: 0.41396
2024-08-21 19:52:50.249 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][53/77] Data 0.022 (0.023) Batch 0.065 (0.068) Remain 00:00:17 loss: 0.1621 data: 0.0098 Lr: 0.41396
2024-08-21 19:52:50.314 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][54/77] Data 0.027 (0.029) Batch 0.065 (0.067) Remain 00:00:17 loss: 0.1062 data: 0.0206 Lr: 0.41234
2024-08-21 19:52:50.314 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][54/77] Data 0.023 (0.023) Batch 0.065 (0.067) Remain 00:00:17 loss: 0.1062 data: -0.0146 Lr: 0.41234
2024-08-21 19:52:50.377 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][55/77] Data 0.027 (0.029) Batch 0.063 (0.067) Remain 00:00:17 loss: 0.1143 data: -0.0055 Lr: 0.41071
2024-08-21 19:52:50.377 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][55/77] Data 0.021 (0.023) Batch 0.063 (0.067) Remain 00:00:17 loss: 0.1143 data: 0.0108 Lr: 0.41071
2024-08-21 19:52:50.443 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][56/77] Data 0.027 (0.029) Batch 0.066 (0.067) Remain 00:00:17 loss: 0.1117 data: 0.0123 Lr: 0.40909
2024-08-21 19:52:50.443 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][56/77] Data 0.022 (0.023) Batch 0.066 (0.067) Remain 00:00:17 loss: 0.1117 data: -0.0007 Lr: 0.40909
2024-08-21 19:52:50.508 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][57/77] Data 0.027 (0.029) Batch 0.065 (0.067) Remain 00:00:16 loss: 0.1887 data: 0.0129 Lr: 0.40747
2024-08-21 19:52:50.508 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][57/77] Data 0.021 (0.023) Batch 0.065 (0.067) Remain 00:00:16 loss: 0.1887 data: 0.0060 Lr: 0.40747
2024-08-21 19:52:50.572 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][58/77] Data 0.027 (0.029) Batch 0.064 (0.067) Remain 00:00:16 loss: 0.1176 data: 0.0061 Lr: 0.40584
2024-08-21 19:52:50.572 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][58/77] Data 0.022 (0.023) Batch 0.064 (0.067) Remain 00:00:16 loss: 0.1176 data: -0.0133 Lr: 0.40584
2024-08-21 19:52:50.634 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][59/77] Data 0.027 (0.029) Batch 0.062 (0.067) Remain 00:00:16 loss: 0.1418 data: 0.0014 Lr: 0.40422
2024-08-21 19:52:50.634 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][59/77] Data 0.021 (0.023) Batch 0.062 (0.067) Remain 00:00:16 loss: 0.1418 data: 0.0114 Lr: 0.40422
2024-08-21 19:52:50.696 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][60/77] Data 0.027 (0.029) Batch 0.062 (0.067) Remain 00:00:16 loss: 0.0772 data: -0.0059 Lr: 0.40260
2024-08-21 19:52:50.696 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][60/77] Data 0.022 (0.023) Batch 0.062 (0.067) Remain 00:00:16 loss: 0.0772 data: -0.0132 Lr: 0.40260
2024-08-21 19:52:50.760 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][61/77] Data 0.027 (0.029) Batch 0.064 (0.067) Remain 00:00:16 loss: 0.0972 data: -0.0065 Lr: 0.40097
2024-08-21 19:52:50.760 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][61/77] Data 0.022 (0.023) Batch 0.064 (0.067) Remain 00:00:16 loss: 0.0972 data: 0.0008 Lr: 0.40097
2024-08-21 19:52:50.825 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][62/77] Data 0.027 (0.029) Batch 0.065 (0.067) Remain 00:00:16 loss: 0.0720 data: -0.0120 Lr: 0.39935
2024-08-21 19:52:50.825 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][62/77] Data 0.022 (0.023) Batch 0.065 (0.067) Remain 00:00:16 loss: 0.0720 data: 0.0184 Lr: 0.39935
2024-08-21 19:52:50.898 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][63/77] Data 0.027 (0.029) Batch 0.073 (0.067) Remain 00:00:16 loss: 0.1101 data: 0.0034 Lr: 0.39773
2024-08-21 19:52:50.898 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][63/77] Data 0.022 (0.023) Batch 0.073 (0.067) Remain 00:00:16 loss: 0.1101 data: 0.0043 Lr: 0.39773
2024-08-21 19:52:50.961 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][64/77] Data 0.027 (0.029) Batch 0.063 (0.067) Remain 00:00:16 loss: 0.1110 data: -0.0036 Lr: 0.39610
2024-08-21 19:52:50.961 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][64/77] Data 0.021 (0.023) Batch 0.063 (0.067) Remain 00:00:16 loss: 0.1110 data: 0.0031 Lr: 0.39610
2024-08-21 19:52:51.025 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][65/77] Data 0.027 (0.029) Batch 0.064 (0.067) Remain 00:00:16 loss: 0.1256 data: 0.0011 Lr: 0.39448
2024-08-21 19:52:51.025 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][65/77] Data 0.022 (0.023) Batch 0.064 (0.067) Remain 00:00:16 loss: 0.1256 data: 0.0020 Lr: 0.39448
2024-08-21 19:52:51.089 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][66/77] Data 0.027 (0.029) Batch 0.064 (0.067) Remain 00:00:16 loss: 0.1137 data: -0.0039 Lr: 0.39286
2024-08-21 19:52:51.089 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][66/77] Data 0.022 (0.023) Batch 0.064 (0.067) Remain 00:00:16 loss: 0.1137 data: 0.0113 Lr: 0.39286
2024-08-21 19:52:51.153 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][67/77] Data 0.027 (0.029) Batch 0.064 (0.067) Remain 00:00:16 loss: 0.1086 data: 0.0163 Lr: 0.39123
2024-08-21 19:52:51.154 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][67/77] Data 0.022 (0.023) Batch 0.064 (0.067) Remain 00:00:16 loss: 0.1086 data: -0.0025 Lr: 0.39123
2024-08-21 19:52:51.218 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][68/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:16 loss: 0.0771 data: -0.0100 Lr: 0.38961
2024-08-21 19:52:51.219 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][68/77] Data 0.021 (0.023) Batch 0.065 (0.067) Remain 00:00:16 loss: 0.0771 data: 0.0062 Lr: 0.38961
2024-08-21 19:52:51.283 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][69/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:16 loss: 0.1107 data: -0.0002 Lr: 0.38799
2024-08-21 19:52:51.283 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][69/77] Data 0.022 (0.023) Batch 0.065 (0.067) Remain 00:00:16 loss: 0.1107 data: -0.0260 Lr: 0.38799
2024-08-21 19:52:51.348 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][70/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:15 loss: 0.1000 data: -0.0036 Lr: 0.38636
2024-08-21 19:52:51.348 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][70/77] Data 0.022 (0.023) Batch 0.065 (0.067) Remain 00:00:15 loss: 0.1000 data: -0.0002 Lr: 0.38636
2024-08-21 19:52:51.414 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][71/77] Data 0.027 (0.028) Batch 0.066 (0.067) Remain 00:00:15 loss: 0.1291 data: 0.0014 Lr: 0.38474
2024-08-21 19:52:51.414 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][71/77] Data 0.021 (0.023) Batch 0.066 (0.067) Remain 00:00:15 loss: 0.1291 data: 0.0028 Lr: 0.38474
2024-08-21 19:52:51.479 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][72/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:15 loss: 0.1064 data: 0.0001 Lr: 0.38312
2024-08-21 19:52:51.480 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][72/77] Data 0.027 (0.023) Batch 0.065 (0.067) Remain 00:00:15 loss: 0.1064 data: 0.0056 Lr: 0.38312
2024-08-21 19:52:51.545 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][73/77] Data 0.027 (0.028) Batch 0.066 (0.067) Remain 00:00:15 loss: 0.1287 data: -0.0077 Lr: 0.38149
2024-08-21 19:52:51.546 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][73/77] Data 0.027 (0.023) Batch 0.066 (0.067) Remain 00:00:15 loss: 0.1287 data: -0.0081 Lr: 0.38149
2024-08-21 19:52:51.612 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][74/77] Data 0.027 (0.028) Batch 0.067 (0.067) Remain 00:00:15 loss: 0.1416 data: 0.0020 Lr: 0.37987
2024-08-21 19:52:51.613 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][74/77] Data 0.027 (0.023) Batch 0.067 (0.067) Remain 00:00:15 loss: 0.1416 data: -0.0110 Lr: 0.37987
2024-08-21 19:52:51.687 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][75/77] Data 0.027 (0.028) Batch 0.074 (0.067) Remain 00:00:15 loss: 0.1375 data: 0.0091 Lr: 0.37825
2024-08-21 19:52:51.687 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][75/77] Data 0.035 (0.023) Batch 0.074 (0.067) Remain 00:00:15 loss: 0.1375 data: 0.0016 Lr: 0.37825
2024-08-21 19:52:51.764 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][76/77] Data 0.027 (0.028) Batch 0.078 (0.067) Remain 00:00:15 loss: 0.1171 data: 0.0090 Lr: 0.37662
2024-08-21 19:52:51.764 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][76/77] Data 0.032 (0.023) Batch 0.078 (0.067) Remain 00:00:15 loss: 0.1171 data: -0.0059 Lr: 0.37662
2024-08-21 19:52:51.809 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][77/77] Data 0.030 (0.028) Batch 0.045 (0.067) Remain 00:00:15 loss: 0.0939 data: 0.0059 Lr: 0.37500
2024-08-21 19:52:51.809 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 19:52:51.809 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][77/77] Data 0.033 (0.023) Batch 0.045 (0.067) Remain 00:00:15 loss: 0.0939 data: -0.0222 Lr: 0.37500
2024-08-21 19:52:51.810 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 19:52:56.480 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0348, Accuracy: 0.9882
2024-08-21 19:52:56.480 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 19:52:56.480 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0348, Accuracy: 0.9882
2024-08-21 19:52:56.480 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 19:52:56.480 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 19:52:56.481 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 19:52:56.585 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][1/77] Data 0.058 (0.058) Batch 0.104 (0.104) Remain 00:00:23 loss: 0.0726 data: -0.0048 Lr: 0.37338
2024-08-21 19:52:56.585 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][1/77] Data 0.059 (0.059) Batch 0.104 (0.104) Remain 00:00:23 loss: 0.0726 data: 0.0207 Lr: 0.37338
2024-08-21 19:52:56.656 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][2/77] Data 0.029 (0.029) Batch 0.072 (0.072) Remain 00:00:16 loss: 0.0917 data: 0.0003 Lr: 0.37175
2024-08-21 19:52:56.657 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][2/77] Data 0.029 (0.029) Batch 0.072 (0.072) Remain 00:00:16 loss: 0.0917 data: 0.0018 Lr: 0.37175
2024-08-21 19:52:56.728 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][3/77] Data 0.028 (0.028) Batch 0.072 (0.072) Remain 00:00:16 loss: 0.1028 data: -0.0084 Lr: 0.37013
2024-08-21 19:52:56.729 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][3/77] Data 0.029 (0.029) Batch 0.072 (0.072) Remain 00:00:16 loss: 0.1028 data: -0.0102 Lr: 0.37013
2024-08-21 19:52:56.797 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][4/77] Data 0.029 (0.029) Batch 0.069 (0.071) Remain 00:00:16 loss: 0.0803 data: 0.0040 Lr: 0.36851
2024-08-21 19:52:56.797 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][4/77] Data 0.029 (0.029) Batch 0.069 (0.071) Remain 00:00:16 loss: 0.0803 data: 0.0095 Lr: 0.36851
2024-08-21 19:52:56.864 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][5/77] Data 0.027 (0.028) Batch 0.067 (0.070) Remain 00:00:15 loss: 0.0631 data: 0.0098 Lr: 0.36688
2024-08-21 19:52:56.864 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][5/77] Data 0.027 (0.029) Batch 0.067 (0.070) Remain 00:00:15 loss: 0.0631 data: 0.0200 Lr: 0.36688
2024-08-21 19:52:56.935 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][6/77] Data 0.028 (0.028) Batch 0.071 (0.070) Remain 00:00:15 loss: 0.1153 data: -0.0010 Lr: 0.36526
2024-08-21 19:52:56.935 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][6/77] Data 0.029 (0.029) Batch 0.071 (0.070) Remain 00:00:15 loss: 0.1153 data: -0.0033 Lr: 0.36526
2024-08-21 19:52:57.006 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][7/77] Data 0.028 (0.028) Batch 0.071 (0.070) Remain 00:00:15 loss: 0.0929 data: -0.0009 Lr: 0.36364
2024-08-21 19:52:57.006 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][7/77] Data 0.029 (0.029) Batch 0.071 (0.070) Remain 00:00:15 loss: 0.0929 data: -0.0046 Lr: 0.36364
2024-08-21 19:52:57.076 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][8/77] Data 0.028 (0.028) Batch 0.071 (0.070) Remain 00:00:15 loss: 0.1019 data: 0.0052 Lr: 0.36201
2024-08-21 19:52:57.077 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][8/77] Data 0.029 (0.029) Batch 0.071 (0.070) Remain 00:00:15 loss: 0.1019 data: -0.0108 Lr: 0.36201
2024-08-21 19:52:57.158 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][9/77] Data 0.028 (0.028) Batch 0.082 (0.072) Remain 00:00:15 loss: 0.1078 data: -0.0215 Lr: 0.36039
2024-08-21 19:52:57.159 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][9/77] Data 0.029 (0.029) Batch 0.082 (0.072) Remain 00:00:15 loss: 0.1078 data: -0.0174 Lr: 0.36039
2024-08-21 19:52:57.242 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][10/77] Data 0.029 (0.028) Batch 0.084 (0.073) Remain 00:00:16 loss: 0.1112 data: -0.0005 Lr: 0.35877
2024-08-21 19:52:57.242 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][10/77] Data 0.036 (0.029) Batch 0.084 (0.073) Remain 00:00:16 loss: 0.1112 data: 0.0161 Lr: 0.35877
2024-08-21 19:52:57.322 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][11/77] Data 0.027 (0.028) Batch 0.080 (0.074) Remain 00:00:16 loss: 0.1441 data: 0.0047 Lr: 0.35714
2024-08-21 19:52:57.322 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][11/77] Data 0.035 (0.030) Batch 0.080 (0.074) Remain 00:00:16 loss: 0.1441 data: -0.0004 Lr: 0.35714
2024-08-21 19:52:57.390 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][12/77] Data 0.028 (0.028) Batch 0.068 (0.073) Remain 00:00:16 loss: 0.1762 data: 0.0077 Lr: 0.35552
2024-08-21 19:52:57.390 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][12/77] Data 0.028 (0.030) Batch 0.068 (0.073) Remain 00:00:16 loss: 0.1762 data: -0.0003 Lr: 0.35552
2024-08-21 19:52:57.459 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][13/77] Data 0.027 (0.028) Batch 0.070 (0.073) Remain 00:00:15 loss: 0.1311 data: -0.0017 Lr: 0.35390
2024-08-21 19:52:57.459 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][13/77] Data 0.027 (0.030) Batch 0.070 (0.073) Remain 00:00:15 loss: 0.1311 data: -0.0061 Lr: 0.35390
2024-08-21 19:52:57.526 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][14/77] Data 0.028 (0.028) Batch 0.067 (0.072) Remain 00:00:15 loss: 0.1111 data: -0.0142 Lr: 0.35227
2024-08-21 19:52:57.526 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][14/77] Data 0.027 (0.029) Batch 0.067 (0.072) Remain 00:00:15 loss: 0.1111 data: 0.0031 Lr: 0.35227
2024-08-21 19:52:57.597 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][15/77] Data 0.029 (0.028) Batch 0.070 (0.072) Remain 00:00:15 loss: 0.0688 data: 0.0090 Lr: 0.35065
2024-08-21 19:52:57.597 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_400
2024-08-21 19:52:57.597 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][15/77] Data 0.028 (0.029) Batch 0.070 (0.072) Remain 00:00:15 loss: 0.0688 data: 0.0112 Lr: 0.35065
2024-08-21 19:52:57.597 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_400
2024-08-21 19:52:57.630 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -563.8003540039062
2024-08-21 19:52:57.630 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -274.00146484375
2024-08-21 19:52:57.630 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -563.8003540039062
2024-08-21 19:52:57.631 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -289.7989196777344
2024-08-21 19:52:57.704 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][16/77] Data 0.064 (0.031) Batch 0.107 (0.075) Remain 00:00:16 loss: 0.0731 data: -0.0122 Lr: 0.34903
2024-08-21 19:52:57.704 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][16/77] Data 0.063 (0.031) Batch 0.107 (0.075) Remain 00:00:16 loss: 0.0731 data: -0.0009 Lr: 0.34903
2024-08-21 19:52:57.796 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][17/77] Data 0.033 (0.031) Batch 0.092 (0.076) Remain 00:00:16 loss: 0.0728 data: 0.0150 Lr: 0.34740
2024-08-21 19:52:57.796 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][17/77] Data 0.029 (0.031) Batch 0.092 (0.076) Remain 00:00:16 loss: 0.0728 data: 0.0024 Lr: 0.34740
2024-08-21 19:52:57.868 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][18/77] Data 0.029 (0.031) Batch 0.073 (0.076) Remain 00:00:16 loss: 0.1243 data: -0.0023 Lr: 0.34578
2024-08-21 19:52:57.869 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][18/77] Data 0.029 (0.031) Batch 0.073 (0.076) Remain 00:00:16 loss: 0.1243 data: 0.0074 Lr: 0.34578
2024-08-21 19:52:57.941 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][19/77] Data 0.030 (0.031) Batch 0.072 (0.075) Remain 00:00:16 loss: 0.1778 data: -0.0049 Lr: 0.34416
2024-08-21 19:52:57.941 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][19/77] Data 0.029 (0.031) Batch 0.072 (0.075) Remain 00:00:16 loss: 0.1778 data: 0.0019 Lr: 0.34416
2024-08-21 19:52:58.013 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][20/77] Data 0.031 (0.031) Batch 0.072 (0.075) Remain 00:00:15 loss: 0.0841 data: 0.0180 Lr: 0.34253
2024-08-21 19:52:58.013 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][20/77] Data 0.029 (0.031) Batch 0.072 (0.075) Remain 00:00:15 loss: 0.0841 data: 0.0016 Lr: 0.34253
2024-08-21 19:52:58.085 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][21/77] Data 0.030 (0.031) Batch 0.072 (0.075) Remain 00:00:15 loss: 0.0661 data: 0.0004 Lr: 0.34091
2024-08-21 19:52:58.085 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][21/77] Data 0.029 (0.031) Batch 0.072 (0.075) Remain 00:00:15 loss: 0.0661 data: -0.0171 Lr: 0.34091
2024-08-21 19:52:58.156 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][22/77] Data 0.030 (0.030) Batch 0.071 (0.075) Remain 00:00:15 loss: 0.1302 data: 0.0093 Lr: 0.33929
2024-08-21 19:52:58.156 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][22/77] Data 0.028 (0.031) Batch 0.071 (0.075) Remain 00:00:15 loss: 0.1302 data: 0.0281 Lr: 0.33929
2024-08-21 19:52:58.223 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][23/77] Data 0.027 (0.031) Batch 0.067 (0.074) Remain 00:00:15 loss: 0.1229 data: -0.0006 Lr: 0.33766
2024-08-21 19:52:58.224 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][23/77] Data 0.028 (0.030) Batch 0.067 (0.074) Remain 00:00:15 loss: 0.1229 data: -0.0026 Lr: 0.33766
2024-08-21 19:52:58.289 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][24/77] Data 0.029 (0.030) Batch 0.066 (0.074) Remain 00:00:15 loss: 0.1172 data: -0.0075 Lr: 0.33604
2024-08-21 19:52:58.289 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][24/77] Data 0.027 (0.030) Batch 0.066 (0.074) Remain 00:00:15 loss: 0.1172 data: -0.0042 Lr: 0.33604
2024-08-21 19:52:58.358 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][25/77] Data 0.028 (0.030) Batch 0.069 (0.074) Remain 00:00:15 loss: 0.1076 data: -0.0091 Lr: 0.33442
2024-08-21 19:52:58.358 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][25/77] Data 0.027 (0.030) Batch 0.069 (0.074) Remain 00:00:15 loss: 0.1076 data: -0.0039 Lr: 0.33442
2024-08-21 19:52:58.426 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][26/77] Data 0.029 (0.030) Batch 0.068 (0.074) Remain 00:00:15 loss: 0.1556 data: 0.0128 Lr: 0.33279
2024-08-21 19:52:58.426 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][26/77] Data 0.029 (0.030) Batch 0.068 (0.074) Remain 00:00:15 loss: 0.1556 data: 0.0125 Lr: 0.33279
2024-08-21 19:52:58.493 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][27/77] Data 0.028 (0.030) Batch 0.067 (0.073) Remain 00:00:15 loss: 0.0839 data: 0.0105 Lr: 0.33117
2024-08-21 19:52:58.493 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][27/77] Data 0.027 (0.030) Batch 0.067 (0.073) Remain 00:00:15 loss: 0.0839 data: -0.0005 Lr: 0.33117
2024-08-21 19:52:58.563 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][28/77] Data 0.029 (0.030) Batch 0.070 (0.073) Remain 00:00:14 loss: 0.1242 data: 0.0011 Lr: 0.32955
2024-08-21 19:52:58.563 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][28/77] Data 0.028 (0.030) Batch 0.070 (0.073) Remain 00:00:14 loss: 0.1242 data: 0.0065 Lr: 0.32955
2024-08-21 19:52:58.635 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][29/77] Data 0.029 (0.030) Batch 0.072 (0.073) Remain 00:00:14 loss: 0.0676 data: 0.0052 Lr: 0.32792
2024-08-21 19:52:58.635 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][29/77] Data 0.029 (0.030) Batch 0.071 (0.073) Remain 00:00:14 loss: 0.0676 data: 0.0013 Lr: 0.32792
2024-08-21 19:52:58.701 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][30/77] Data 0.028 (0.030) Batch 0.066 (0.073) Remain 00:00:14 loss: 0.0935 data: -0.0017 Lr: 0.32630
2024-08-21 19:52:58.701 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][30/77] Data 0.027 (0.030) Batch 0.066 (0.073) Remain 00:00:14 loss: 0.0935 data: 0.0014 Lr: 0.32630
2024-08-21 19:52:58.767 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][31/77] Data 0.028 (0.030) Batch 0.066 (0.073) Remain 00:00:14 loss: 0.0935 data: 0.0064 Lr: 0.32468
2024-08-21 19:52:58.767 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][31/77] Data 0.027 (0.030) Batch 0.066 (0.073) Remain 00:00:14 loss: 0.0935 data: 0.0081 Lr: 0.32468
2024-08-21 19:52:58.834 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][32/77] Data 0.028 (0.030) Batch 0.068 (0.073) Remain 00:00:14 loss: 0.0904 data: 0.0188 Lr: 0.32305
2024-08-21 19:52:58.834 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][32/77] Data 0.027 (0.030) Batch 0.068 (0.073) Remain 00:00:14 loss: 0.0904 data: 0.0002 Lr: 0.32305
2024-08-21 19:52:58.900 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][33/77] Data 0.028 (0.030) Batch 0.066 (0.072) Remain 00:00:14 loss: 0.1054 data: 0.0007 Lr: 0.32143
2024-08-21 19:52:58.900 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][33/77] Data 0.027 (0.030) Batch 0.066 (0.072) Remain 00:00:14 loss: 0.1054 data: 0.0149 Lr: 0.32143
2024-08-21 19:52:58.968 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][34/77] Data 0.028 (0.030) Batch 0.068 (0.072) Remain 00:00:14 loss: 0.1034 data: -0.0118 Lr: 0.31981
2024-08-21 19:52:58.968 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][34/77] Data 0.027 (0.030) Batch 0.068 (0.072) Remain 00:00:14 loss: 0.1034 data: 0.0067 Lr: 0.31981
2024-08-21 19:52:59.037 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][35/77] Data 0.028 (0.030) Batch 0.069 (0.072) Remain 00:00:14 loss: 0.0758 data: 0.0114 Lr: 0.31818
2024-08-21 19:52:59.037 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][35/77] Data 0.027 (0.029) Batch 0.069 (0.072) Remain 00:00:14 loss: 0.0758 data: -0.0016 Lr: 0.31818
2024-08-21 19:52:59.109 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][36/77] Data 0.029 (0.030) Batch 0.072 (0.072) Remain 00:00:14 loss: 0.0556 data: -0.0139 Lr: 0.31656
2024-08-21 19:52:59.109 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][36/77] Data 0.028 (0.029) Batch 0.072 (0.072) Remain 00:00:14 loss: 0.0556 data: 0.0071 Lr: 0.31656
2024-08-21 19:52:59.182 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][37/77] Data 0.030 (0.030) Batch 0.073 (0.072) Remain 00:00:14 loss: 0.0365 data: 0.0010 Lr: 0.31494
2024-08-21 19:52:59.182 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][37/77] Data 0.029 (0.029) Batch 0.073 (0.072) Remain 00:00:14 loss: 0.0365 data: 0.0075 Lr: 0.31494
2024-08-21 19:52:59.256 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][38/77] Data 0.030 (0.030) Batch 0.074 (0.072) Remain 00:00:14 loss: 0.0748 data: -0.0011 Lr: 0.31331
2024-08-21 19:52:59.256 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][38/77] Data 0.028 (0.029) Batch 0.074 (0.072) Remain 00:00:14 loss: 0.0748 data: -0.0155 Lr: 0.31331
2024-08-21 19:52:59.329 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][39/77] Data 0.030 (0.030) Batch 0.073 (0.072) Remain 00:00:13 loss: 0.1201 data: -0.0121 Lr: 0.31169
2024-08-21 19:52:59.329 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][39/77] Data 0.029 (0.029) Batch 0.073 (0.072) Remain 00:00:13 loss: 0.1201 data: 0.0119 Lr: 0.31169
2024-08-21 19:52:59.402 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][40/77] Data 0.029 (0.030) Batch 0.073 (0.072) Remain 00:00:13 loss: 0.1331 data: -0.0064 Lr: 0.31006
2024-08-21 19:52:59.402 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][40/77] Data 0.029 (0.029) Batch 0.073 (0.072) Remain 00:00:13 loss: 0.1331 data: 0.0154 Lr: 0.31006
2024-08-21 19:52:59.476 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][41/77] Data 0.029 (0.030) Batch 0.074 (0.072) Remain 00:00:13 loss: 0.1245 data: 0.0134 Lr: 0.30844
2024-08-21 19:52:59.476 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][41/77] Data 0.029 (0.029) Batch 0.074 (0.072) Remain 00:00:13 loss: 0.1245 data: -0.0015 Lr: 0.30844
2024-08-21 19:52:59.550 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][42/77] Data 0.029 (0.030) Batch 0.074 (0.072) Remain 00:00:13 loss: 0.0729 data: 0.0074 Lr: 0.30682
2024-08-21 19:52:59.550 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][42/77] Data 0.029 (0.029) Batch 0.074 (0.072) Remain 00:00:13 loss: 0.0729 data: 0.0047 Lr: 0.30682
2024-08-21 19:52:59.623 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][43/77] Data 0.030 (0.030) Batch 0.074 (0.072) Remain 00:00:13 loss: 0.0943 data: 0.0085 Lr: 0.30519
2024-08-21 19:52:59.624 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][43/77] Data 0.029 (0.029) Batch 0.074 (0.072) Remain 00:00:13 loss: 0.0943 data: -0.0006 Lr: 0.30519
2024-08-21 19:52:59.695 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][44/77] Data 0.029 (0.030) Batch 0.071 (0.072) Remain 00:00:13 loss: 0.1219 data: 0.0050 Lr: 0.30357
2024-08-21 19:52:59.695 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][44/77] Data 0.028 (0.029) Batch 0.071 (0.072) Remain 00:00:13 loss: 0.1219 data: 0.0176 Lr: 0.30357
2024-08-21 19:52:59.766 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][45/77] Data 0.029 (0.030) Batch 0.072 (0.072) Remain 00:00:13 loss: 0.0678 data: 0.0038 Lr: 0.30195
2024-08-21 19:52:59.766 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][45/77] Data 0.029 (0.029) Batch 0.072 (0.072) Remain 00:00:13 loss: 0.0678 data: 0.0177 Lr: 0.30195
2024-08-21 19:52:59.838 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][46/77] Data 0.029 (0.030) Batch 0.071 (0.072) Remain 00:00:13 loss: 0.1162 data: -0.0016 Lr: 0.30032
2024-08-21 19:52:59.838 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][46/77] Data 0.029 (0.029) Batch 0.071 (0.072) Remain 00:00:13 loss: 0.1162 data: -0.0026 Lr: 0.30032
2024-08-21 19:52:59.906 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][47/77] Data 0.029 (0.030) Batch 0.069 (0.072) Remain 00:00:13 loss: 0.1479 data: -0.0064 Lr: 0.29870
2024-08-21 19:52:59.906 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][47/77] Data 0.029 (0.029) Batch 0.069 (0.072) Remain 00:00:13 loss: 0.1479 data: 0.0036 Lr: 0.29870
2024-08-21 19:52:59.973 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][48/77] Data 0.028 (0.030) Batch 0.066 (0.072) Remain 00:00:13 loss: 0.0460 data: -0.0077 Lr: 0.29708
2024-08-21 19:52:59.973 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][48/77] Data 0.027 (0.029) Batch 0.066 (0.072) Remain 00:00:13 loss: 0.0460 data: -0.0030 Lr: 0.29708
2024-08-21 19:53:00.039 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][49/77] Data 0.028 (0.029) Batch 0.066 (0.072) Remain 00:00:13 loss: 0.0727 data: 0.0010 Lr: 0.29545
2024-08-21 19:53:00.039 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][49/77] Data 0.027 (0.029) Batch 0.066 (0.072) Remain 00:00:13 loss: 0.0727 data: -0.0064 Lr: 0.29545
2024-08-21 19:53:00.105 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][50/77] Data 0.028 (0.029) Batch 0.067 (0.072) Remain 00:00:13 loss: 0.1121 data: 0.0031 Lr: 0.29383
2024-08-21 19:53:00.105 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][50/77] Data 0.028 (0.029) Batch 0.067 (0.072) Remain 00:00:13 loss: 0.1121 data: -0.0073 Lr: 0.29383
2024-08-21 19:53:00.171 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][51/77] Data 0.027 (0.029) Batch 0.066 (0.072) Remain 00:00:12 loss: 0.1700 data: 0.0146 Lr: 0.29221
2024-08-21 19:53:00.171 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][51/77] Data 0.027 (0.029) Batch 0.066 (0.072) Remain 00:00:12 loss: 0.1700 data: 0.0001 Lr: 0.29221
2024-08-21 19:53:00.238 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][52/77] Data 0.028 (0.029) Batch 0.067 (0.072) Remain 00:00:12 loss: 0.1332 data: -0.0058 Lr: 0.29058
2024-08-21 19:53:00.238 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][52/77] Data 0.027 (0.029) Batch 0.067 (0.072) Remain 00:00:12 loss: 0.1332 data: -0.0161 Lr: 0.29058
2024-08-21 19:53:00.309 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][53/77] Data 0.028 (0.029) Batch 0.071 (0.072) Remain 00:00:12 loss: 0.1928 data: -0.0125 Lr: 0.28896
2024-08-21 19:53:00.310 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][53/77] Data 0.028 (0.029) Batch 0.071 (0.072) Remain 00:00:12 loss: 0.1928 data: 0.0044 Lr: 0.28896
2024-08-21 19:53:00.381 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][54/77] Data 0.029 (0.029) Batch 0.072 (0.072) Remain 00:00:12 loss: 0.0541 data: -0.0028 Lr: 0.28734
2024-08-21 19:53:00.381 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][54/77] Data 0.029 (0.029) Batch 0.072 (0.072) Remain 00:00:12 loss: 0.0541 data: 0.0090 Lr: 0.28734
2024-08-21 19:53:00.453 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][55/77] Data 0.029 (0.029) Batch 0.072 (0.072) Remain 00:00:12 loss: 0.1133 data: -0.0214 Lr: 0.28571
2024-08-21 19:53:00.453 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][55/77] Data 0.029 (0.029) Batch 0.072 (0.072) Remain 00:00:12 loss: 0.1133 data: -0.0014 Lr: 0.28571
2024-08-21 19:53:00.524 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][56/77] Data 0.029 (0.029) Batch 0.071 (0.072) Remain 00:00:12 loss: 0.0480 data: -0.0066 Lr: 0.28409
2024-08-21 19:53:00.524 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][56/77] Data 0.029 (0.029) Batch 0.071 (0.072) Remain 00:00:12 loss: 0.0480 data: -0.0109 Lr: 0.28409
2024-08-21 19:53:00.591 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][57/77] Data 0.028 (0.029) Batch 0.068 (0.072) Remain 00:00:12 loss: 0.1022 data: -0.0083 Lr: 0.28247
2024-08-21 19:53:00.591 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][57/77] Data 0.028 (0.029) Batch 0.068 (0.072) Remain 00:00:12 loss: 0.1022 data: -0.0063 Lr: 0.28247
2024-08-21 19:53:00.662 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][58/77] Data 0.028 (0.029) Batch 0.071 (0.072) Remain 00:00:12 loss: 0.0926 data: -0.0136 Lr: 0.28084
2024-08-21 19:53:00.662 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][58/77] Data 0.028 (0.029) Batch 0.071 (0.072) Remain 00:00:12 loss: 0.0926 data: -0.0010 Lr: 0.28084
2024-08-21 19:53:00.728 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][59/77] Data 0.028 (0.029) Batch 0.066 (0.071) Remain 00:00:12 loss: 0.1126 data: -0.0025 Lr: 0.27922
2024-08-21 19:53:00.728 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][59/77] Data 0.027 (0.029) Batch 0.066 (0.071) Remain 00:00:12 loss: 0.1126 data: 0.0090 Lr: 0.27922
2024-08-21 19:53:00.793 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][60/77] Data 0.027 (0.029) Batch 0.066 (0.071) Remain 00:00:12 loss: 0.0875 data: -0.0015 Lr: 0.27760
2024-08-21 19:53:00.794 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][60/77] Data 0.027 (0.029) Batch 0.066 (0.071) Remain 00:00:12 loss: 0.0875 data: -0.0037 Lr: 0.27760
2024-08-21 19:53:00.863 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][61/77] Data 0.027 (0.029) Batch 0.069 (0.071) Remain 00:00:12 loss: 0.0755 data: -0.0114 Lr: 0.27597
2024-08-21 19:53:00.863 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][61/77] Data 0.028 (0.029) Batch 0.070 (0.071) Remain 00:00:12 loss: 0.0755 data: 0.0039 Lr: 0.27597
2024-08-21 19:53:00.933 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][62/77] Data 0.032 (0.029) Batch 0.070 (0.071) Remain 00:00:12 loss: 0.0753 data: -0.0047 Lr: 0.27435
2024-08-21 19:53:00.933 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][62/77] Data 0.027 (0.029) Batch 0.071 (0.071) Remain 00:00:12 loss: 0.0753 data: -0.0136 Lr: 0.27435
2024-08-21 19:53:00.999 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][63/77] Data 0.028 (0.029) Batch 0.066 (0.071) Remain 00:00:12 loss: 0.1045 data: 0.0097 Lr: 0.27273
2024-08-21 19:53:00.999 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][63/77] Data 0.027 (0.029) Batch 0.066 (0.071) Remain 00:00:12 loss: 0.1045 data: -0.0039 Lr: 0.27273
2024-08-21 19:53:01.064 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][64/77] Data 0.027 (0.029) Batch 0.066 (0.071) Remain 00:00:11 loss: 0.0710 data: 0.0087 Lr: 0.27110
2024-08-21 19:53:01.065 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][64/77] Data 0.027 (0.029) Batch 0.066 (0.071) Remain 00:00:11 loss: 0.0710 data: -0.0002 Lr: 0.27110
2024-08-21 19:53:01.131 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][65/77] Data 0.027 (0.029) Batch 0.067 (0.071) Remain 00:00:11 loss: 0.1363 data: 0.0106 Lr: 0.26948
2024-08-21 19:53:01.131 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_450
2024-08-21 19:53:01.131 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][65/77] Data 0.027 (0.029) Batch 0.067 (0.071) Remain 00:00:11 loss: 0.1363 data: 0.0039 Lr: 0.26948
2024-08-21 19:53:01.131 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_450
2024-08-21 19:53:01.157 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -569.8925170898438
2024-08-21 19:53:01.157 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -569.8925170898438
2024-08-21 19:53:01.157 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -270.25
2024-08-21 19:53:01.157 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -299.6426086425781
2024-08-21 19:53:01.226 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][66/77] Data 0.054 (0.030) Batch 0.094 (0.071) Remain 00:00:11 loss: 0.1094 data: -0.0167 Lr: 0.26786
2024-08-21 19:53:01.226 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][66/77] Data 0.054 (0.029) Batch 0.094 (0.071) Remain 00:00:11 loss: 0.1094 data: -0.0019 Lr: 0.26786
2024-08-21 19:53:01.297 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][67/77] Data 0.028 (0.030) Batch 0.071 (0.071) Remain 00:00:11 loss: 0.0490 data: -0.0091 Lr: 0.26623
2024-08-21 19:53:01.297 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][67/77] Data 0.029 (0.029) Batch 0.071 (0.071) Remain 00:00:11 loss: 0.0490 data: -0.0088 Lr: 0.26623
2024-08-21 19:53:01.368 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][68/77] Data 0.029 (0.030) Batch 0.071 (0.071) Remain 00:00:11 loss: 0.1425 data: 0.0082 Lr: 0.26461
2024-08-21 19:53:01.368 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][68/77] Data 0.029 (0.029) Batch 0.071 (0.071) Remain 00:00:11 loss: 0.1425 data: -0.0022 Lr: 0.26461
2024-08-21 19:53:01.439 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][69/77] Data 0.029 (0.030) Batch 0.071 (0.071) Remain 00:00:11 loss: 0.1114 data: -0.0219 Lr: 0.26299
2024-08-21 19:53:01.439 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][69/77] Data 0.029 (0.029) Batch 0.071 (0.071) Remain 00:00:11 loss: 0.1114 data: 0.0064 Lr: 0.26299
2024-08-21 19:53:01.510 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][70/77] Data 0.029 (0.029) Batch 0.071 (0.071) Remain 00:00:11 loss: 0.0843 data: -0.0131 Lr: 0.26136
2024-08-21 19:53:01.510 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][70/77] Data 0.028 (0.029) Batch 0.071 (0.071) Remain 00:00:11 loss: 0.0843 data: -0.0016 Lr: 0.26136
2024-08-21 19:53:01.576 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][71/77] Data 0.028 (0.029) Batch 0.066 (0.071) Remain 00:00:11 loss: 0.0778 data: -0.0031 Lr: 0.25974
2024-08-21 19:53:01.576 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][71/77] Data 0.027 (0.029) Batch 0.066 (0.071) Remain 00:00:11 loss: 0.0778 data: 0.0018 Lr: 0.25974
2024-08-21 19:53:01.641 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][72/77] Data 0.027 (0.029) Batch 0.065 (0.071) Remain 00:00:11 loss: 0.0869 data: 0.0094 Lr: 0.25812
2024-08-21 19:53:01.641 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][72/77] Data 0.027 (0.029) Batch 0.065 (0.071) Remain 00:00:11 loss: 0.0869 data: 0.0098 Lr: 0.25812
2024-08-21 19:53:01.708 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][73/77] Data 0.027 (0.029) Batch 0.067 (0.071) Remain 00:00:11 loss: 0.1049 data: 0.0064 Lr: 0.25649
2024-08-21 19:53:01.708 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][73/77] Data 0.027 (0.029) Batch 0.067 (0.071) Remain 00:00:11 loss: 0.1049 data: 0.0076 Lr: 0.25649
2024-08-21 19:53:01.778 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][74/77] Data 0.028 (0.029) Batch 0.070 (0.071) Remain 00:00:11 loss: 0.0834 data: 0.0072 Lr: 0.25487
2024-08-21 19:53:01.778 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][74/77] Data 0.028 (0.029) Batch 0.070 (0.071) Remain 00:00:11 loss: 0.0834 data: -0.0042 Lr: 0.25487
2024-08-21 19:53:01.849 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][75/77] Data 0.029 (0.029) Batch 0.071 (0.071) Remain 00:00:11 loss: 0.0921 data: -0.0094 Lr: 0.25325
2024-08-21 19:53:01.849 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][75/77] Data 0.029 (0.029) Batch 0.071 (0.071) Remain 00:00:11 loss: 0.0921 data: 0.0026 Lr: 0.25325
2024-08-21 19:53:01.920 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][76/77] Data 0.029 (0.029) Batch 0.071 (0.071) Remain 00:00:11 loss: 0.0910 data: 0.0090 Lr: 0.25162
2024-08-21 19:53:01.920 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][76/77] Data 0.028 (0.029) Batch 0.071 (0.071) Remain 00:00:11 loss: 0.0910 data: -0.0027 Lr: 0.25162
2024-08-21 19:53:01.966 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][77/77] Data 0.031 (0.029) Batch 0.046 (0.071) Remain 00:00:10 loss: 0.1460 data: 0.0192 Lr: 0.25000
2024-08-21 19:53:01.967 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 19:53:01.967 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][77/77] Data 0.031 (0.029) Batch 0.046 (0.071) Remain 00:00:10 loss: 0.1460 data: -0.0109 Lr: 0.25000
2024-08-21 19:53:01.967 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 19:53:05.935 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0323, Accuracy: 0.9886
2024-08-21 19:53:05.935 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0323, Accuracy: 0.9886
2024-08-21 19:53:05.935 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 19:53:05.935 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 19:53:05.935 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 19:53:05.935 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 19:53:06.034 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][1/77] Data 0.056 (0.056) Batch 0.098 (0.098) Remain 00:00:15 loss: 0.1153 data: -0.0011 Lr: 0.24838
2024-08-21 19:53:06.034 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][1/77] Data 0.056 (0.056) Batch 0.099 (0.099) Remain 00:00:15 loss: 0.1153 data: 0.0001 Lr: 0.24838
2024-08-21 19:53:06.100 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][2/77] Data 0.028 (0.028) Batch 0.066 (0.066) Remain 00:00:10 loss: 0.1321 data: 0.0130 Lr: 0.24675
2024-08-21 19:53:06.100 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][2/77] Data 0.028 (0.028) Batch 0.066 (0.066) Remain 00:00:10 loss: 0.1321 data: 0.0024 Lr: 0.24675
2024-08-21 19:53:06.166 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][3/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:10 loss: 0.0508 data: -0.0149 Lr: 0.24513
2024-08-21 19:53:06.166 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][3/77] Data 0.027 (0.028) Batch 0.065 (0.066) Remain 00:00:10 loss: 0.0508 data: -0.0094 Lr: 0.24513
2024-08-21 19:53:06.231 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][4/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:09 loss: 0.1163 data: 0.0154 Lr: 0.24351
2024-08-21 19:53:06.231 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][4/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:09 loss: 0.1163 data: 0.0173 Lr: 0.24351
2024-08-21 19:53:06.297 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][5/77] Data 0.027 (0.027) Batch 0.066 (0.066) Remain 00:00:09 loss: 0.0440 data: 0.0002 Lr: 0.24188
2024-08-21 19:53:06.297 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][5/77] Data 0.027 (0.027) Batch 0.066 (0.066) Remain 00:00:09 loss: 0.0440 data: 0.0112 Lr: 0.24188
2024-08-21 19:53:06.362 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][6/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:09 loss: 0.0464 data: -0.0313 Lr: 0.24026
2024-08-21 19:53:06.362 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][6/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:09 loss: 0.0464 data: -0.0011 Lr: 0.24026
2024-08-21 19:53:06.426 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][7/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:09 loss: 0.1221 data: -0.0014 Lr: 0.23864
2024-08-21 19:53:06.427 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][7/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:09 loss: 0.1221 data: 0.0004 Lr: 0.23864
2024-08-21 19:53:06.491 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][8/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:09 loss: 0.0705 data: 0.0011 Lr: 0.23701
2024-08-21 19:53:06.491 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][8/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:09 loss: 0.0705 data: -0.0148 Lr: 0.23701
2024-08-21 19:53:06.556 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][9/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:09 loss: 0.1271 data: 0.0141 Lr: 0.23539
2024-08-21 19:53:06.556 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][9/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:09 loss: 0.1271 data: 0.0075 Lr: 0.23539
2024-08-21 19:53:06.628 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][10/77] Data 0.029 (0.027) Batch 0.072 (0.066) Remain 00:00:09 loss: 0.1933 data: 0.0063 Lr: 0.23377
2024-08-21 19:53:06.628 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][10/77] Data 0.027 (0.027) Batch 0.072 (0.066) Remain 00:00:09 loss: 0.1933 data: -0.0173 Lr: 0.23377
2024-08-21 19:53:06.693 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][11/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:09 loss: 0.0387 data: 0.0060 Lr: 0.23214
2024-08-21 19:53:06.693 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][11/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:09 loss: 0.0387 data: 0.0108 Lr: 0.23214
2024-08-21 19:53:06.757 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][12/77] Data 0.027 (0.027) Batch 0.064 (0.066) Remain 00:00:09 loss: 0.0663 data: -0.0053 Lr: 0.23052
2024-08-21 19:53:06.757 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][12/77] Data 0.027 (0.027) Batch 0.064 (0.066) Remain 00:00:09 loss: 0.0663 data: 0.0054 Lr: 0.23052
2024-08-21 19:53:06.822 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][13/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:09 loss: 0.0642 data: -0.0085 Lr: 0.22890
2024-08-21 19:53:06.822 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][13/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:09 loss: 0.0642 data: 0.0079 Lr: 0.22890
2024-08-21 19:53:06.886 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][14/77] Data 0.027 (0.027) Batch 0.064 (0.066) Remain 00:00:09 loss: 0.1010 data: 0.0011 Lr: 0.22727
2024-08-21 19:53:06.886 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][14/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:09 loss: 0.1010 data: 0.0143 Lr: 0.22727
2024-08-21 19:53:06.950 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][15/77] Data 0.027 (0.027) Batch 0.064 (0.065) Remain 00:00:09 loss: 0.0620 data: -0.0009 Lr: 0.22565
2024-08-21 19:53:06.951 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][15/77] Data 0.027 (0.027) Batch 0.064 (0.065) Remain 00:00:09 loss: 0.0620 data: 0.0073 Lr: 0.22565
2024-08-21 19:53:07.015 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][16/77] Data 0.027 (0.027) Batch 0.064 (0.065) Remain 00:00:09 loss: 0.1323 data: 0.0100 Lr: 0.22403
2024-08-21 19:53:07.015 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][16/77] Data 0.027 (0.027) Batch 0.064 (0.065) Remain 00:00:09 loss: 0.1323 data: -0.0112 Lr: 0.22403
2024-08-21 19:53:07.079 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][17/77] Data 0.027 (0.027) Batch 0.064 (0.065) Remain 00:00:09 loss: 0.0604 data: -0.0082 Lr: 0.22240
2024-08-21 19:53:07.079 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][17/77] Data 0.027 (0.027) Batch 0.064 (0.065) Remain 00:00:09 loss: 0.0604 data: 0.0017 Lr: 0.22240
2024-08-21 19:53:07.143 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][18/77] Data 0.027 (0.027) Batch 0.064 (0.065) Remain 00:00:08 loss: 0.2008 data: 0.0112 Lr: 0.22078
2024-08-21 19:53:07.143 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][18/77] Data 0.027 (0.027) Batch 0.064 (0.065) Remain 00:00:08 loss: 0.2008 data: -0.0106 Lr: 0.22078
2024-08-21 19:53:07.208 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][19/77] Data 0.027 (0.027) Batch 0.064 (0.065) Remain 00:00:08 loss: 0.0806 data: 0.0047 Lr: 0.21916
2024-08-21 19:53:07.208 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][19/77] Data 0.027 (0.027) Batch 0.064 (0.065) Remain 00:00:08 loss: 0.0806 data: -0.0053 Lr: 0.21916
2024-08-21 19:53:07.272 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][20/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:08 loss: 0.1281 data: 0.0030 Lr: 0.21753
2024-08-21 19:53:07.272 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][20/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:08 loss: 0.1281 data: -0.0103 Lr: 0.21753
2024-08-21 19:53:07.337 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][21/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:08 loss: 0.1139 data: -0.0080 Lr: 0.21591
2024-08-21 19:53:07.337 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][21/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:08 loss: 0.1139 data: 0.0014 Lr: 0.21591
2024-08-21 19:53:07.402 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][22/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:08 loss: 0.1184 data: 0.0072 Lr: 0.21429
2024-08-21 19:53:07.402 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][22/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:08 loss: 0.1184 data: 0.0074 Lr: 0.21429
2024-08-21 19:53:07.466 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][23/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:08 loss: 0.1222 data: -0.0051 Lr: 0.21266
2024-08-21 19:53:07.467 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][23/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:08 loss: 0.1222 data: 0.0091 Lr: 0.21266
2024-08-21 19:53:07.532 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][24/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:08 loss: 0.0530 data: 0.0033 Lr: 0.21104
2024-08-21 19:53:07.532 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][24/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:08 loss: 0.0530 data: 0.0050 Lr: 0.21104
2024-08-21 19:53:07.599 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][25/77] Data 0.027 (0.027) Batch 0.067 (0.065) Remain 00:00:08 loss: 0.0683 data: 0.0152 Lr: 0.20942
2024-08-21 19:53:07.599 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][25/77] Data 0.027 (0.027) Batch 0.067 (0.065) Remain 00:00:08 loss: 0.0683 data: 0.0067 Lr: 0.20942
2024-08-21 19:53:07.669 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][26/77] Data 0.028 (0.027) Batch 0.070 (0.065) Remain 00:00:08 loss: 0.0825 data: -0.0023 Lr: 0.20779
2024-08-21 19:53:07.669 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][26/77] Data 0.028 (0.027) Batch 0.070 (0.065) Remain 00:00:08 loss: 0.0825 data: 0.0008 Lr: 0.20779
2024-08-21 19:53:07.738 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][27/77] Data 0.028 (0.027) Batch 0.069 (0.066) Remain 00:00:08 loss: 0.0618 data: 0.0118 Lr: 0.20617
2024-08-21 19:53:07.738 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][27/77] Data 0.028 (0.027) Batch 0.069 (0.066) Remain 00:00:08 loss: 0.0618 data: 0.0005 Lr: 0.20617
2024-08-21 19:53:07.805 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][28/77] Data 0.028 (0.027) Batch 0.067 (0.066) Remain 00:00:08 loss: 0.0803 data: -0.0045 Lr: 0.20455
2024-08-21 19:53:07.805 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][28/77] Data 0.027 (0.027) Batch 0.067 (0.066) Remain 00:00:08 loss: 0.0803 data: 0.0051 Lr: 0.20455
2024-08-21 19:53:07.883 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][29/77] Data 0.034 (0.028) Batch 0.079 (0.066) Remain 00:00:08 loss: 0.1692 data: -0.0048 Lr: 0.20292
2024-08-21 19:53:07.883 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][29/77] Data 0.036 (0.027) Batch 0.079 (0.066) Remain 00:00:08 loss: 0.1692 data: 0.0000 Lr: 0.20292
2024-08-21 19:53:07.964 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][30/77] Data 0.034 (0.028) Batch 0.081 (0.067) Remain 00:00:08 loss: 0.1181 data: 0.0166 Lr: 0.20130
2024-08-21 19:53:07.964 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][30/77] Data 0.035 (0.028) Batch 0.081 (0.067) Remain 00:00:08 loss: 0.1181 data: 0.0034 Lr: 0.20130
2024-08-21 19:53:08.044 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][31/77] Data 0.035 (0.028) Batch 0.080 (0.067) Remain 00:00:08 loss: 0.0905 data: 0.0001 Lr: 0.19968
2024-08-21 19:53:08.044 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][31/77] Data 0.027 (0.028) Batch 0.080 (0.067) Remain 00:00:08 loss: 0.0905 data: -0.0229 Lr: 0.19968
2024-08-21 19:53:08.127 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][32/77] Data 0.035 (0.028) Batch 0.083 (0.068) Remain 00:00:08 loss: 0.0826 data: 0.0063 Lr: 0.19805
2024-08-21 19:53:08.127 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][32/77] Data 0.037 (0.028) Batch 0.083 (0.068) Remain 00:00:08 loss: 0.0826 data: 0.0009 Lr: 0.19805
2024-08-21 19:53:08.207 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][33/77] Data 0.027 (0.028) Batch 0.080 (0.068) Remain 00:00:08 loss: 0.1176 data: -0.0038 Lr: 0.19643
2024-08-21 19:53:08.207 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][33/77] Data 0.032 (0.028) Batch 0.080 (0.068) Remain 00:00:08 loss: 0.1176 data: -0.0264 Lr: 0.19643
2024-08-21 19:53:08.283 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][34/77] Data 0.032 (0.028) Batch 0.075 (0.068) Remain 00:00:08 loss: 0.0912 data: -0.0025 Lr: 0.19481
2024-08-21 19:53:08.283 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][34/77] Data 0.027 (0.028) Batch 0.076 (0.068) Remain 00:00:08 loss: 0.0912 data: -0.0071 Lr: 0.19481
2024-08-21 19:53:08.361 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][35/77] Data 0.040 (0.028) Batch 0.078 (0.068) Remain 00:00:08 loss: 0.1146 data: -0.0320 Lr: 0.19318
2024-08-21 19:53:08.361 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][35/77] Data 0.033 (0.028) Batch 0.079 (0.068) Remain 00:00:08 loss: 0.1146 data: 0.0015 Lr: 0.19318
2024-08-21 19:53:08.435 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][36/77] Data 0.027 (0.028) Batch 0.073 (0.069) Remain 00:00:08 loss: 0.0841 data: 0.0079 Lr: 0.19156
2024-08-21 19:53:08.435 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][36/77] Data 0.027 (0.028) Batch 0.073 (0.069) Remain 00:00:08 loss: 0.0841 data: -0.0123 Lr: 0.19156
2024-08-21 19:53:08.513 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][37/77] Data 0.027 (0.028) Batch 0.078 (0.069) Remain 00:00:08 loss: 0.1275 data: 0.0035 Lr: 0.18994
2024-08-21 19:53:08.513 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][37/77] Data 0.036 (0.028) Batch 0.078 (0.069) Remain 00:00:08 loss: 0.1275 data: 0.0167 Lr: 0.18994
2024-08-21 19:53:08.586 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][38/77] Data 0.027 (0.028) Batch 0.074 (0.069) Remain 00:00:08 loss: 0.0657 data: 0.0007 Lr: 0.18831
2024-08-21 19:53:08.587 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_500
2024-08-21 19:53:08.587 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][38/77] Data 0.027 (0.028) Batch 0.074 (0.069) Remain 00:00:08 loss: 0.0657 data: -0.0054 Lr: 0.18831
2024-08-21 19:53:08.587 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_500
2024-08-21 19:53:08.613 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -578.3916625976562
2024-08-21 19:53:08.613 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -275.4422607421875
2024-08-21 19:53:08.613 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -578.3916625976562
2024-08-21 19:53:08.614 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -302.94940185546875
2024-08-21 19:53:08.689 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][39/77] Data 0.054 (0.029) Batch 0.102 (0.070) Remain 00:00:08 loss: 0.1021 data: 0.0020 Lr: 0.18669
2024-08-21 19:53:08.689 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][39/77] Data 0.055 (0.029) Batch 0.102 (0.070) Remain 00:00:08 loss: 0.1021 data: -0.0102 Lr: 0.18669
2024-08-21 19:53:08.764 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][40/77] Data 0.027 (0.029) Batch 0.076 (0.070) Remain 00:00:08 loss: 0.1224 data: -0.0045 Lr: 0.18506
2024-08-21 19:53:08.764 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][40/77] Data 0.037 (0.029) Batch 0.075 (0.070) Remain 00:00:08 loss: 0.1224 data: -0.0030 Lr: 0.18506
2024-08-21 19:53:08.836 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][41/77] Data 0.027 (0.029) Batch 0.071 (0.070) Remain 00:00:07 loss: 0.0669 data: 0.0053 Lr: 0.18344
2024-08-21 19:53:08.836 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][41/77] Data 0.029 (0.029) Batch 0.071 (0.070) Remain 00:00:07 loss: 0.0669 data: -0.0008 Lr: 0.18344
2024-08-21 19:53:08.901 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][42/77] Data 0.027 (0.029) Batch 0.066 (0.070) Remain 00:00:07 loss: 0.0637 data: -0.0071 Lr: 0.18182
2024-08-21 19:53:08.902 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][42/77] Data 0.027 (0.029) Batch 0.066 (0.070) Remain 00:00:07 loss: 0.0637 data: 0.0082 Lr: 0.18182
2024-08-21 19:53:08.967 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][43/77] Data 0.027 (0.029) Batch 0.065 (0.070) Remain 00:00:07 loss: 0.1099 data: -0.0066 Lr: 0.18019
2024-08-21 19:53:08.967 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][43/77] Data 0.027 (0.029) Batch 0.065 (0.070) Remain 00:00:07 loss: 0.1099 data: -0.0112 Lr: 0.18019
2024-08-21 19:53:09.033 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][44/77] Data 0.027 (0.029) Batch 0.066 (0.070) Remain 00:00:07 loss: 0.0489 data: 0.0052 Lr: 0.17857
2024-08-21 19:53:09.033 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][44/77] Data 0.027 (0.029) Batch 0.066 (0.070) Remain 00:00:07 loss: 0.0489 data: -0.0066 Lr: 0.17857
2024-08-21 19:53:09.097 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][45/77] Data 0.027 (0.029) Batch 0.065 (0.070) Remain 00:00:07 loss: 0.1440 data: 0.0016 Lr: 0.17695
2024-08-21 19:53:09.097 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][45/77] Data 0.027 (0.029) Batch 0.065 (0.070) Remain 00:00:07 loss: 0.1440 data: 0.0145 Lr: 0.17695
2024-08-21 19:53:09.163 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][46/77] Data 0.027 (0.029) Batch 0.065 (0.070) Remain 00:00:07 loss: 0.0627 data: -0.0031 Lr: 0.17532
2024-08-21 19:53:09.163 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][46/77] Data 0.027 (0.029) Batch 0.065 (0.070) Remain 00:00:07 loss: 0.0627 data: -0.0037 Lr: 0.17532
2024-08-21 19:53:09.231 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][47/77] Data 0.027 (0.029) Batch 0.068 (0.069) Remain 00:00:07 loss: 0.0860 data: 0.0088 Lr: 0.17370
2024-08-21 19:53:09.231 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][47/77] Data 0.027 (0.029) Batch 0.068 (0.069) Remain 00:00:07 loss: 0.0860 data: 0.0097 Lr: 0.17370
2024-08-21 19:53:09.300 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][48/77] Data 0.028 (0.029) Batch 0.069 (0.069) Remain 00:00:07 loss: 0.0486 data: -0.0063 Lr: 0.17208
2024-08-21 19:53:09.300 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][48/77] Data 0.029 (0.029) Batch 0.069 (0.069) Remain 00:00:07 loss: 0.0486 data: 0.0044 Lr: 0.17208
2024-08-21 19:53:09.371 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][49/77] Data 0.028 (0.029) Batch 0.071 (0.070) Remain 00:00:07 loss: 0.0677 data: 0.0184 Lr: 0.17045
2024-08-21 19:53:09.371 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][49/77] Data 0.028 (0.029) Batch 0.071 (0.070) Remain 00:00:07 loss: 0.0677 data: 0.0028 Lr: 0.17045
2024-08-21 19:53:09.440 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][50/77] Data 0.028 (0.029) Batch 0.070 (0.070) Remain 00:00:07 loss: 0.0767 data: 0.0187 Lr: 0.16883
2024-08-21 19:53:09.441 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][50/77] Data 0.028 (0.029) Batch 0.070 (0.070) Remain 00:00:07 loss: 0.0767 data: -0.0056 Lr: 0.16883
2024-08-21 19:53:09.510 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][51/77] Data 0.028 (0.029) Batch 0.069 (0.070) Remain 00:00:07 loss: 0.0648 data: -0.0055 Lr: 0.16721
2024-08-21 19:53:09.510 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][51/77] Data 0.028 (0.029) Batch 0.069 (0.070) Remain 00:00:07 loss: 0.0648 data: 0.0066 Lr: 0.16721
2024-08-21 19:53:09.575 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][52/77] Data 0.027 (0.029) Batch 0.065 (0.069) Remain 00:00:07 loss: 0.1287 data: -0.0154 Lr: 0.16558
2024-08-21 19:53:09.575 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][52/77] Data 0.027 (0.029) Batch 0.065 (0.069) Remain 00:00:07 loss: 0.1287 data: 0.0024 Lr: 0.16558
2024-08-21 19:53:09.640 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][53/77] Data 0.027 (0.029) Batch 0.065 (0.069) Remain 00:00:07 loss: 0.0694 data: 0.0010 Lr: 0.16396
2024-08-21 19:53:09.640 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][53/77] Data 0.027 (0.029) Batch 0.065 (0.069) Remain 00:00:07 loss: 0.0694 data: 0.0070 Lr: 0.16396
2024-08-21 19:53:09.705 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][54/77] Data 0.027 (0.029) Batch 0.066 (0.069) Remain 00:00:06 loss: 0.1354 data: -0.0035 Lr: 0.16234
2024-08-21 19:53:09.706 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][54/77] Data 0.027 (0.029) Batch 0.066 (0.069) Remain 00:00:06 loss: 0.1354 data: -0.0023 Lr: 0.16234
2024-08-21 19:53:09.771 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][55/77] Data 0.028 (0.029) Batch 0.066 (0.069) Remain 00:00:06 loss: 0.1063 data: 0.0044 Lr: 0.16071
2024-08-21 19:53:09.772 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][55/77] Data 0.027 (0.029) Batch 0.066 (0.069) Remain 00:00:06 loss: 0.1063 data: 0.0008 Lr: 0.16071
2024-08-21 19:53:09.841 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][56/77] Data 0.028 (0.029) Batch 0.069 (0.069) Remain 00:00:06 loss: 0.0948 data: -0.0034 Lr: 0.15909
2024-08-21 19:53:09.841 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][56/77] Data 0.028 (0.029) Batch 0.069 (0.069) Remain 00:00:06 loss: 0.0948 data: -0.0045 Lr: 0.15909
2024-08-21 19:53:09.916 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][57/77] Data 0.028 (0.029) Batch 0.075 (0.069) Remain 00:00:06 loss: 0.0545 data: 0.0192 Lr: 0.15747
2024-08-21 19:53:09.916 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][57/77] Data 0.032 (0.029) Batch 0.075 (0.069) Remain 00:00:06 loss: 0.0545 data: -0.0022 Lr: 0.15747
2024-08-21 19:53:09.987 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][58/77] Data 0.028 (0.029) Batch 0.071 (0.069) Remain 00:00:06 loss: 0.1029 data: -0.0038 Lr: 0.15584
2024-08-21 19:53:09.987 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][58/77] Data 0.028 (0.029) Batch 0.071 (0.069) Remain 00:00:06 loss: 0.1029 data: 0.0035 Lr: 0.15584
2024-08-21 19:53:10.057 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][59/77] Data 0.028 (0.029) Batch 0.070 (0.069) Remain 00:00:06 loss: 0.0652 data: -0.0079 Lr: 0.15422
2024-08-21 19:53:10.057 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][59/77] Data 0.028 (0.029) Batch 0.070 (0.069) Remain 00:00:06 loss: 0.0652 data: -0.0107 Lr: 0.15422
2024-08-21 19:53:10.127 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][60/77] Data 0.028 (0.029) Batch 0.070 (0.069) Remain 00:00:06 loss: 0.0644 data: -0.0010 Lr: 0.15260
2024-08-21 19:53:10.127 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][60/77] Data 0.028 (0.029) Batch 0.070 (0.069) Remain 00:00:06 loss: 0.0644 data: -0.0031 Lr: 0.15260
2024-08-21 19:53:10.210 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][61/77] Data 0.028 (0.029) Batch 0.083 (0.070) Remain 00:00:06 loss: 0.0508 data: -0.0009 Lr: 0.15097
2024-08-21 19:53:10.211 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][61/77] Data 0.031 (0.029) Batch 0.083 (0.070) Remain 00:00:06 loss: 0.0508 data: -0.0113 Lr: 0.15097
2024-08-21 19:53:10.281 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][62/77] Data 0.028 (0.029) Batch 0.071 (0.070) Remain 00:00:06 loss: 0.0871 data: -0.0007 Lr: 0.14935
2024-08-21 19:53:10.281 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][62/77] Data 0.028 (0.029) Batch 0.071 (0.070) Remain 00:00:06 loss: 0.0871 data: -0.0043 Lr: 0.14935
2024-08-21 19:53:10.353 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][63/77] Data 0.029 (0.029) Batch 0.072 (0.070) Remain 00:00:06 loss: 0.0821 data: -0.0075 Lr: 0.14773
2024-08-21 19:53:10.353 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][63/77] Data 0.028 (0.029) Batch 0.072 (0.070) Remain 00:00:06 loss: 0.0821 data: 0.0052 Lr: 0.14773
2024-08-21 19:53:10.423 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][64/77] Data 0.028 (0.029) Batch 0.070 (0.070) Remain 00:00:06 loss: 0.1587 data: 0.0079 Lr: 0.14610
2024-08-21 19:53:10.423 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][64/77] Data 0.028 (0.029) Batch 0.070 (0.070) Remain 00:00:06 loss: 0.1587 data: 0.0068 Lr: 0.14610
2024-08-21 19:53:10.492 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][65/77] Data 0.028 (0.029) Batch 0.069 (0.070) Remain 00:00:06 loss: 0.0946 data: -0.0062 Lr: 0.14448
2024-08-21 19:53:10.492 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][65/77] Data 0.028 (0.029) Batch 0.069 (0.070) Remain 00:00:06 loss: 0.0946 data: 0.0012 Lr: 0.14448
2024-08-21 19:53:10.560 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][66/77] Data 0.027 (0.029) Batch 0.067 (0.070) Remain 00:00:06 loss: 0.0839 data: 0.0081 Lr: 0.14286
2024-08-21 19:53:10.560 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][66/77] Data 0.027 (0.029) Batch 0.068 (0.070) Remain 00:00:06 loss: 0.0839 data: -0.0030 Lr: 0.14286
2024-08-21 19:53:10.628 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][67/77] Data 0.028 (0.029) Batch 0.068 (0.070) Remain 00:00:06 loss: 0.1037 data: -0.0052 Lr: 0.14123
2024-08-21 19:53:10.628 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][67/77] Data 0.028 (0.029) Batch 0.068 (0.070) Remain 00:00:06 loss: 0.1037 data: -0.0037 Lr: 0.14123
2024-08-21 19:53:10.700 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][68/77] Data 0.029 (0.029) Batch 0.072 (0.070) Remain 00:00:06 loss: 0.0691 data: -0.0219 Lr: 0.13961
2024-08-21 19:53:10.700 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][68/77] Data 0.028 (0.029) Batch 0.072 (0.070) Remain 00:00:06 loss: 0.0691 data: 0.0103 Lr: 0.13961
2024-08-21 19:53:10.771 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][69/77] Data 0.028 (0.029) Batch 0.071 (0.070) Remain 00:00:05 loss: 0.1038 data: -0.0128 Lr: 0.13799
2024-08-21 19:53:10.771 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][69/77] Data 0.028 (0.029) Batch 0.071 (0.070) Remain 00:00:05 loss: 0.1038 data: -0.0022 Lr: 0.13799
2024-08-21 19:53:10.842 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][70/77] Data 0.028 (0.029) Batch 0.071 (0.070) Remain 00:00:05 loss: 0.1654 data: -0.0059 Lr: 0.13636
2024-08-21 19:53:10.842 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][70/77] Data 0.029 (0.029) Batch 0.071 (0.070) Remain 00:00:05 loss: 0.1654 data: -0.0035 Lr: 0.13636
2024-08-21 19:53:10.912 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][71/77] Data 0.028 (0.029) Batch 0.070 (0.070) Remain 00:00:05 loss: 0.0831 data: -0.0177 Lr: 0.13474
2024-08-21 19:53:10.912 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][71/77] Data 0.028 (0.029) Batch 0.070 (0.070) Remain 00:00:05 loss: 0.0831 data: 0.0189 Lr: 0.13474
2024-08-21 19:53:10.977 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][72/77] Data 0.027 (0.028) Batch 0.065 (0.070) Remain 00:00:05 loss: 0.0880 data: 0.0072 Lr: 0.13312
2024-08-21 19:53:10.978 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][72/77] Data 0.027 (0.029) Batch 0.065 (0.070) Remain 00:00:05 loss: 0.0880 data: -0.0186 Lr: 0.13312
2024-08-21 19:53:11.060 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][73/77] Data 0.043 (0.029) Batch 0.083 (0.070) Remain 00:00:05 loss: 0.1304 data: -0.0020 Lr: 0.13149
2024-08-21 19:53:11.060 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][73/77] Data 0.027 (0.029) Batch 0.083 (0.070) Remain 00:00:05 loss: 0.1304 data: -0.0192 Lr: 0.13149
2024-08-21 19:53:11.128 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][74/77] Data 0.028 (0.029) Batch 0.068 (0.070) Remain 00:00:05 loss: 0.1350 data: 0.0071 Lr: 0.12987
2024-08-21 19:53:11.129 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][74/77] Data 0.029 (0.029) Batch 0.068 (0.070) Remain 00:00:05 loss: 0.1350 data: -0.0122 Lr: 0.12987
2024-08-21 19:53:11.194 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][75/77] Data 0.027 (0.029) Batch 0.066 (0.070) Remain 00:00:05 loss: 0.1029 data: 0.0052 Lr: 0.12825
2024-08-21 19:53:11.194 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][75/77] Data 0.027 (0.029) Batch 0.066 (0.070) Remain 00:00:05 loss: 0.1029 data: 0.0056 Lr: 0.12825
2024-08-21 19:53:11.260 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][76/77] Data 0.028 (0.029) Batch 0.066 (0.070) Remain 00:00:05 loss: 0.0981 data: -0.0066 Lr: 0.12662
2024-08-21 19:53:11.260 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][76/77] Data 0.027 (0.029) Batch 0.066 (0.070) Remain 00:00:05 loss: 0.0981 data: 0.0208 Lr: 0.12662
2024-08-21 19:53:11.301 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][77/77] Data 0.029 (0.029) Batch 0.041 (0.069) Remain 00:00:05 loss: 0.0675 data: 0.0074 Lr: 0.12500
2024-08-21 19:53:11.301 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 19:53:11.301 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][77/77] Data 0.029 (0.029) Batch 0.041 (0.069) Remain 00:00:05 loss: 0.0675 data: 0.0015 Lr: 0.12500
2024-08-21 19:53:11.302 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 19:53:15.674 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0302, Accuracy: 0.9895
2024-08-21 19:53:15.674 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0302, Accuracy: 0.9895
2024-08-21 19:53:15.674 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 19:53:15.674 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 19:53:15.674 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 19:53:15.675 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 19:53:15.767 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][1/77] Data 0.056 (0.056) Batch 0.092 (0.092) Remain 00:00:07 loss: 0.0710 data: -0.0064 Lr: 0.12338
2024-08-21 19:53:15.767 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][1/77] Data 0.043 (0.043) Batch 0.092 (0.092) Remain 00:00:07 loss: 0.0710 data: 0.0044 Lr: 0.12338
2024-08-21 19:53:15.832 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][2/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:04 loss: 0.0904 data: -0.0154 Lr: 0.12175
2024-08-21 19:53:15.832 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][2/77] Data 0.021 (0.021) Batch 0.065 (0.065) Remain 00:00:04 loss: 0.0904 data: -0.0069 Lr: 0.12175
2024-08-21 19:53:15.896 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][3/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:04 loss: 0.0997 data: 0.0051 Lr: 0.12013
2024-08-21 19:53:15.897 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][3/77] Data 0.022 (0.022) Batch 0.065 (0.065) Remain 00:00:04 loss: 0.0997 data: 0.0093 Lr: 0.12013
2024-08-21 19:53:15.961 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][4/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:04 loss: 0.0702 data: -0.0016 Lr: 0.11851
2024-08-21 19:53:15.961 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][4/77] Data 0.021 (0.022) Batch 0.065 (0.065) Remain 00:00:04 loss: 0.0702 data: -0.0150 Lr: 0.11851
2024-08-21 19:53:16.026 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][5/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:04 loss: 0.0845 data: -0.0099 Lr: 0.11688
2024-08-21 19:53:16.026 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][5/77] Data 0.023 (0.022) Batch 0.065 (0.065) Remain 00:00:04 loss: 0.0845 data: 0.0039 Lr: 0.11688
2024-08-21 19:53:16.090 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][6/77] Data 0.027 (0.027) Batch 0.064 (0.065) Remain 00:00:04 loss: 0.0807 data: 0.0066 Lr: 0.11526
2024-08-21 19:53:16.090 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][6/77] Data 0.021 (0.022) Batch 0.064 (0.065) Remain 00:00:04 loss: 0.0807 data: 0.0028 Lr: 0.11526
2024-08-21 19:53:16.164 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][7/77] Data 0.027 (0.027) Batch 0.074 (0.066) Remain 00:00:04 loss: 0.1022 data: 0.0012 Lr: 0.11364
2024-08-21 19:53:16.164 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][7/77] Data 0.021 (0.022) Batch 0.074 (0.066) Remain 00:00:04 loss: 0.1022 data: 0.0029 Lr: 0.11364
2024-08-21 19:53:16.239 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][8/77] Data 0.027 (0.027) Batch 0.076 (0.067) Remain 00:00:04 loss: 0.0668 data: -0.0092 Lr: 0.11201
2024-08-21 19:53:16.239 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][8/77] Data 0.033 (0.023) Batch 0.076 (0.067) Remain 00:00:04 loss: 0.0668 data: -0.0132 Lr: 0.11201
2024-08-21 19:53:16.312 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][9/77] Data 0.027 (0.027) Batch 0.073 (0.068) Remain 00:00:04 loss: 0.0669 data: 0.0104 Lr: 0.11039
2024-08-21 19:53:16.312 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][9/77] Data 0.031 (0.024) Batch 0.073 (0.068) Remain 00:00:04 loss: 0.0669 data: 0.0191 Lr: 0.11039
2024-08-21 19:53:16.403 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][10/77] Data 0.028 (0.027) Batch 0.091 (0.071) Remain 00:00:04 loss: 0.0863 data: -0.0118 Lr: 0.10877
2024-08-21 19:53:16.403 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][10/77] Data 0.037 (0.026) Batch 0.091 (0.071) Remain 00:00:04 loss: 0.0863 data: -0.0013 Lr: 0.10877
2024-08-21 19:53:16.494 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][11/77] Data 0.029 (0.027) Batch 0.091 (0.073) Remain 00:00:04 loss: 0.1087 data: 0.0034 Lr: 0.10714
2024-08-21 19:53:16.495 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_550
2024-08-21 19:53:16.495 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][11/77] Data 0.038 (0.027) Batch 0.091 (0.073) Remain 00:00:04 loss: 0.1087 data: -0.0117 Lr: 0.10714
2024-08-21 19:53:16.495 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_550
2024-08-21 19:53:16.529 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -584.8433837890625
2024-08-21 19:53:16.529 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -584.8433837890625
2024-08-21 19:53:16.529 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -283.0082702636719
2024-08-21 19:53:16.529 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -301.8351135253906
2024-08-21 19:53:16.602 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][12/77] Data 0.064 (0.031) Batch 0.107 (0.076) Remain 00:00:05 loss: 0.0734 data: -0.0093 Lr: 0.10552
2024-08-21 19:53:16.602 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][12/77] Data 0.063 (0.030) Batch 0.107 (0.076) Remain 00:00:05 loss: 0.0734 data: 0.0032 Lr: 0.10552
2024-08-21 19:53:16.671 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][13/77] Data 0.029 (0.031) Batch 0.069 (0.075) Remain 00:00:04 loss: 0.0667 data: 0.0014 Lr: 0.10390
2024-08-21 19:53:16.671 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][13/77] Data 0.028 (0.030) Batch 0.069 (0.075) Remain 00:00:04 loss: 0.0667 data: 0.0072 Lr: 0.10390
2024-08-21 19:53:16.752 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][14/77] Data 0.028 (0.030) Batch 0.081 (0.076) Remain 00:00:04 loss: 0.0568 data: -0.0302 Lr: 0.10227
2024-08-21 19:53:16.753 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][14/77] Data 0.027 (0.030) Batch 0.081 (0.076) Remain 00:00:04 loss: 0.0568 data: -0.0137 Lr: 0.10227
2024-08-21 19:53:16.821 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][15/77] Data 0.027 (0.030) Batch 0.068 (0.075) Remain 00:00:04 loss: 0.0765 data: -0.0052 Lr: 0.10065
2024-08-21 19:53:16.821 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][15/77] Data 0.029 (0.030) Batch 0.069 (0.075) Remain 00:00:04 loss: 0.0765 data: 0.0111 Lr: 0.10065
2024-08-21 19:53:16.888 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][16/77] Data 0.028 (0.030) Batch 0.067 (0.075) Remain 00:00:04 loss: 0.0395 data: 0.0036 Lr: 0.09903
2024-08-21 19:53:16.888 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][16/77] Data 0.028 (0.030) Batch 0.067 (0.075) Remain 00:00:04 loss: 0.0395 data: -0.0124 Lr: 0.09903
2024-08-21 19:53:16.954 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][17/77] Data 0.027 (0.030) Batch 0.066 (0.074) Remain 00:00:04 loss: 0.0689 data: 0.0048 Lr: 0.09740
2024-08-21 19:53:16.954 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][17/77] Data 0.021 (0.029) Batch 0.066 (0.074) Remain 00:00:04 loss: 0.0689 data: 0.0047 Lr: 0.09740
2024-08-21 19:53:17.020 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][18/77] Data 0.027 (0.030) Batch 0.066 (0.074) Remain 00:00:04 loss: 0.0892 data: 0.0172 Lr: 0.09578
2024-08-21 19:53:17.020 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][18/77] Data 0.021 (0.029) Batch 0.066 (0.074) Remain 00:00:04 loss: 0.0892 data: -0.0082 Lr: 0.09578
2024-08-21 19:53:17.085 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][19/77] Data 0.027 (0.029) Batch 0.065 (0.073) Remain 00:00:04 loss: 0.0433 data: 0.0141 Lr: 0.09416
2024-08-21 19:53:17.085 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][19/77] Data 0.021 (0.028) Batch 0.065 (0.073) Remain 00:00:04 loss: 0.0433 data: -0.0048 Lr: 0.09416
2024-08-21 19:53:17.150 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][20/77] Data 0.027 (0.029) Batch 0.065 (0.073) Remain 00:00:04 loss: 0.0712 data: 0.0081 Lr: 0.09253
2024-08-21 19:53:17.150 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][20/77] Data 0.022 (0.028) Batch 0.065 (0.073) Remain 00:00:04 loss: 0.0712 data: 0.0308 Lr: 0.09253
2024-08-21 19:53:17.219 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][21/77] Data 0.027 (0.029) Batch 0.069 (0.073) Remain 00:00:04 loss: 0.1545 data: 0.0044 Lr: 0.09091
2024-08-21 19:53:17.219 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][21/77] Data 0.021 (0.028) Batch 0.069 (0.073) Remain 00:00:04 loss: 0.1545 data: -0.0009 Lr: 0.09091
2024-08-21 19:53:17.284 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][22/77] Data 0.027 (0.029) Batch 0.066 (0.072) Remain 00:00:04 loss: 0.0662 data: -0.0057 Lr: 0.08929
2024-08-21 19:53:17.285 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][22/77] Data 0.022 (0.027) Batch 0.066 (0.072) Remain 00:00:04 loss: 0.0662 data: 0.0053 Lr: 0.08929
2024-08-21 19:53:17.350 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][23/77] Data 0.027 (0.029) Batch 0.065 (0.072) Remain 00:00:03 loss: 0.1177 data: -0.0005 Lr: 0.08766
2024-08-21 19:53:17.350 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][23/77] Data 0.022 (0.027) Batch 0.065 (0.072) Remain 00:00:03 loss: 0.1177 data: 0.0018 Lr: 0.08766
2024-08-21 19:53:17.415 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][24/77] Data 0.027 (0.029) Batch 0.065 (0.072) Remain 00:00:03 loss: 0.0732 data: -0.0035 Lr: 0.08604
2024-08-21 19:53:17.415 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][24/77] Data 0.023 (0.027) Batch 0.065 (0.072) Remain 00:00:03 loss: 0.0732 data: 0.0055 Lr: 0.08604
2024-08-21 19:53:17.481 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][25/77] Data 0.027 (0.029) Batch 0.066 (0.071) Remain 00:00:03 loss: 0.0845 data: -0.0067 Lr: 0.08442
2024-08-21 19:53:17.481 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][25/77] Data 0.022 (0.027) Batch 0.066 (0.071) Remain 00:00:03 loss: 0.0845 data: -0.0088 Lr: 0.08442
2024-08-21 19:53:17.546 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][26/77] Data 0.028 (0.029) Batch 0.065 (0.071) Remain 00:00:03 loss: 0.0610 data: 0.0067 Lr: 0.08279
2024-08-21 19:53:17.546 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][26/77] Data 0.023 (0.026) Batch 0.065 (0.071) Remain 00:00:03 loss: 0.0610 data: -0.0076 Lr: 0.08279
2024-08-21 19:53:17.611 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][27/77] Data 0.027 (0.029) Batch 0.065 (0.071) Remain 00:00:03 loss: 0.0367 data: 0.0105 Lr: 0.08117
2024-08-21 19:53:17.611 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][27/77] Data 0.022 (0.026) Batch 0.065 (0.071) Remain 00:00:03 loss: 0.0367 data: -0.0088 Lr: 0.08117
2024-08-21 19:53:17.675 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][28/77] Data 0.027 (0.029) Batch 0.065 (0.071) Remain 00:00:03 loss: 0.0384 data: -0.0124 Lr: 0.07955
2024-08-21 19:53:17.675 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][28/77] Data 0.021 (0.026) Batch 0.065 (0.071) Remain 00:00:03 loss: 0.0384 data: -0.0090 Lr: 0.07955
2024-08-21 19:53:17.740 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][29/77] Data 0.027 (0.029) Batch 0.065 (0.070) Remain 00:00:03 loss: 0.0819 data: 0.0209 Lr: 0.07792
2024-08-21 19:53:17.740 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][29/77] Data 0.021 (0.026) Batch 0.065 (0.070) Remain 00:00:03 loss: 0.0819 data: -0.0040 Lr: 0.07792
2024-08-21 19:53:17.805 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][30/77] Data 0.027 (0.029) Batch 0.065 (0.070) Remain 00:00:03 loss: 0.0608 data: 0.0119 Lr: 0.07630
2024-08-21 19:53:17.806 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][30/77] Data 0.023 (0.026) Batch 0.066 (0.070) Remain 00:00:03 loss: 0.0608 data: -0.0084 Lr: 0.07630
2024-08-21 19:53:17.883 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][31/77] Data 0.027 (0.029) Batch 0.077 (0.071) Remain 00:00:03 loss: 0.0989 data: -0.0029 Lr: 0.07468
2024-08-21 19:53:17.883 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][31/77] Data 0.034 (0.026) Batch 0.077 (0.071) Remain 00:00:03 loss: 0.0989 data: 0.0025 Lr: 0.07468
2024-08-21 19:53:17.955 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][32/77] Data 0.027 (0.029) Batch 0.072 (0.071) Remain 00:00:03 loss: 0.1261 data: -0.0170 Lr: 0.07305
2024-08-21 19:53:17.955 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][32/77] Data 0.033 (0.026) Batch 0.072 (0.071) Remain 00:00:03 loss: 0.1261 data: -0.0100 Lr: 0.07305
2024-08-21 19:53:18.022 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][33/77] Data 0.027 (0.029) Batch 0.067 (0.070) Remain 00:00:03 loss: 0.0698 data: -0.0237 Lr: 0.07143
2024-08-21 19:53:18.022 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][33/77] Data 0.027 (0.026) Batch 0.067 (0.070) Remain 00:00:03 loss: 0.0698 data: -0.0142 Lr: 0.07143
2024-08-21 19:53:18.098 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][34/77] Data 0.027 (0.028) Batch 0.076 (0.071) Remain 00:00:03 loss: 0.0497 data: -0.0005 Lr: 0.06981
2024-08-21 19:53:18.098 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][34/77] Data 0.031 (0.026) Batch 0.076 (0.071) Remain 00:00:03 loss: 0.0497 data: 0.0079 Lr: 0.06981
2024-08-21 19:53:18.176 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][35/77] Data 0.027 (0.028) Batch 0.078 (0.071) Remain 00:00:03 loss: 0.1776 data: -0.0102 Lr: 0.06818
2024-08-21 19:53:18.176 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][35/77] Data 0.033 (0.027) Batch 0.078 (0.071) Remain 00:00:03 loss: 0.1776 data: -0.0089 Lr: 0.06818
2024-08-21 19:53:18.248 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][36/77] Data 0.027 (0.028) Batch 0.072 (0.071) Remain 00:00:02 loss: 0.0544 data: -0.0073 Lr: 0.06656
2024-08-21 19:53:18.248 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][36/77] Data 0.033 (0.027) Batch 0.072 (0.071) Remain 00:00:02 loss: 0.0544 data: 0.0150 Lr: 0.06656
2024-08-21 19:53:18.325 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][37/77] Data 0.027 (0.028) Batch 0.078 (0.071) Remain 00:00:02 loss: 0.1090 data: 0.0178 Lr: 0.06494
2024-08-21 19:53:18.326 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][37/77] Data 0.033 (0.027) Batch 0.078 (0.071) Remain 00:00:02 loss: 0.1090 data: 0.0072 Lr: 0.06494
2024-08-21 19:53:18.392 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][38/77] Data 0.027 (0.028) Batch 0.067 (0.071) Remain 00:00:02 loss: 0.0473 data: 0.0097 Lr: 0.06331
2024-08-21 19:53:18.392 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][38/77] Data 0.029 (0.027) Batch 0.067 (0.071) Remain 00:00:02 loss: 0.0473 data: 0.0146 Lr: 0.06331
2024-08-21 19:53:18.458 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][39/77] Data 0.027 (0.028) Batch 0.066 (0.071) Remain 00:00:02 loss: 0.0392 data: 0.0174 Lr: 0.06169
2024-08-21 19:53:18.458 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][39/77] Data 0.027 (0.027) Batch 0.066 (0.071) Remain 00:00:02 loss: 0.0392 data: -0.0041 Lr: 0.06169
2024-08-21 19:53:18.523 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][40/77] Data 0.027 (0.028) Batch 0.065 (0.071) Remain 00:00:02 loss: 0.0361 data: -0.0229 Lr: 0.06006
2024-08-21 19:53:18.524 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][40/77] Data 0.027 (0.027) Batch 0.065 (0.071) Remain 00:00:02 loss: 0.0361 data: -0.0103 Lr: 0.06006
2024-08-21 19:53:18.591 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][41/77] Data 0.028 (0.028) Batch 0.068 (0.071) Remain 00:00:02 loss: 0.0638 data: 0.0221 Lr: 0.05844
2024-08-21 19:53:18.591 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][41/77] Data 0.027 (0.027) Batch 0.068 (0.071) Remain 00:00:02 loss: 0.0638 data: 0.0055 Lr: 0.05844
2024-08-21 19:53:18.661 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][42/77] Data 0.027 (0.028) Batch 0.070 (0.071) Remain 00:00:02 loss: 0.1610 data: 0.0027 Lr: 0.05682
2024-08-21 19:53:18.661 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][42/77] Data 0.027 (0.027) Batch 0.070 (0.071) Remain 00:00:02 loss: 0.1610 data: -0.0051 Lr: 0.05682
2024-08-21 19:53:18.732 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][43/77] Data 0.027 (0.028) Batch 0.071 (0.071) Remain 00:00:02 loss: 0.0642 data: -0.0048 Lr: 0.05519
2024-08-21 19:53:18.733 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][43/77] Data 0.032 (0.027) Batch 0.071 (0.071) Remain 00:00:02 loss: 0.0642 data: 0.0180 Lr: 0.05519
2024-08-21 19:53:18.799 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][44/77] Data 0.027 (0.028) Batch 0.067 (0.071) Remain 00:00:02 loss: 0.0874 data: 0.0079 Lr: 0.05357
2024-08-21 19:53:18.799 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][44/77] Data 0.027 (0.027) Batch 0.067 (0.071) Remain 00:00:02 loss: 0.0874 data: -0.0078 Lr: 0.05357
2024-08-21 19:53:18.872 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][45/77] Data 0.028 (0.028) Batch 0.073 (0.071) Remain 00:00:02 loss: 0.1280 data: 0.0060 Lr: 0.05195
2024-08-21 19:53:18.872 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][45/77] Data 0.034 (0.027) Batch 0.073 (0.071) Remain 00:00:02 loss: 0.1280 data: 0.0004 Lr: 0.05195
2024-08-21 19:53:18.941 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][46/77] Data 0.027 (0.028) Batch 0.069 (0.071) Remain 00:00:02 loss: 0.0821 data: 0.0093 Lr: 0.05032
2024-08-21 19:53:18.941 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][46/77] Data 0.030 (0.027) Batch 0.069 (0.071) Remain 00:00:02 loss: 0.0821 data: -0.0119 Lr: 0.05032
2024-08-21 19:53:19.005 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][47/77] Data 0.027 (0.028) Batch 0.065 (0.070) Remain 00:00:02 loss: 0.0982 data: 0.0069 Lr: 0.04870
2024-08-21 19:53:19.005 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][47/77] Data 0.027 (0.027) Batch 0.065 (0.070) Remain 00:00:02 loss: 0.0982 data: -0.0043 Lr: 0.04870
2024-08-21 19:53:19.070 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][48/77] Data 0.027 (0.028) Batch 0.065 (0.070) Remain 00:00:02 loss: 0.0683 data: -0.0116 Lr: 0.04708
2024-08-21 19:53:19.070 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][48/77] Data 0.027 (0.027) Batch 0.065 (0.070) Remain 00:00:02 loss: 0.0683 data: 0.0009 Lr: 0.04708
2024-08-21 19:53:19.135 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][49/77] Data 0.027 (0.028) Batch 0.065 (0.070) Remain 00:00:02 loss: 0.0905 data: 0.0085 Lr: 0.04545
2024-08-21 19:53:19.135 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][49/77] Data 0.027 (0.027) Batch 0.065 (0.070) Remain 00:00:02 loss: 0.0905 data: 0.0096 Lr: 0.04545
2024-08-21 19:53:19.199 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][50/77] Data 0.027 (0.028) Batch 0.065 (0.070) Remain 00:00:01 loss: 0.0733 data: -0.0132 Lr: 0.04383
2024-08-21 19:53:19.200 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][50/77] Data 0.027 (0.027) Batch 0.065 (0.070) Remain 00:00:01 loss: 0.0733 data: -0.0011 Lr: 0.04383
2024-08-21 19:53:19.264 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][51/77] Data 0.027 (0.028) Batch 0.065 (0.070) Remain 00:00:01 loss: 0.1146 data: -0.0028 Lr: 0.04221
2024-08-21 19:53:19.264 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][51/77] Data 0.027 (0.027) Batch 0.065 (0.070) Remain 00:00:01 loss: 0.1146 data: -0.0086 Lr: 0.04221
2024-08-21 19:53:19.329 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][52/77] Data 0.027 (0.028) Batch 0.065 (0.070) Remain 00:00:01 loss: 0.0730 data: 0.0056 Lr: 0.04058
2024-08-21 19:53:19.329 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][52/77] Data 0.027 (0.027) Batch 0.065 (0.070) Remain 00:00:01 loss: 0.0730 data: -0.0101 Lr: 0.04058
2024-08-21 19:53:19.394 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][53/77] Data 0.027 (0.028) Batch 0.065 (0.070) Remain 00:00:01 loss: 0.1100 data: 0.0171 Lr: 0.03896
2024-08-21 19:53:19.394 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][53/77] Data 0.027 (0.027) Batch 0.065 (0.070) Remain 00:00:01 loss: 0.1100 data: -0.0067 Lr: 0.03896
2024-08-21 19:53:19.472 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][54/77] Data 0.027 (0.028) Batch 0.078 (0.070) Remain 00:00:01 loss: 0.0827 data: -0.0228 Lr: 0.03734
2024-08-21 19:53:19.472 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][54/77] Data 0.027 (0.027) Batch 0.078 (0.070) Remain 00:00:01 loss: 0.0827 data: 0.0016 Lr: 0.03734
2024-08-21 19:53:19.539 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][55/77] Data 0.027 (0.028) Batch 0.067 (0.070) Remain 00:00:01 loss: 0.0864 data: -0.0035 Lr: 0.03571
2024-08-21 19:53:19.539 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][55/77] Data 0.027 (0.027) Batch 0.067 (0.070) Remain 00:00:01 loss: 0.0864 data: -0.0121 Lr: 0.03571
2024-08-21 19:53:19.604 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][56/77] Data 0.027 (0.028) Batch 0.065 (0.070) Remain 00:00:01 loss: 0.0407 data: 0.0114 Lr: 0.03409
2024-08-21 19:53:19.604 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][56/77] Data 0.027 (0.027) Batch 0.065 (0.070) Remain 00:00:01 loss: 0.0407 data: 0.0259 Lr: 0.03409
2024-08-21 19:53:19.670 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][57/77] Data 0.027 (0.028) Batch 0.065 (0.070) Remain 00:00:01 loss: 0.0745 data: -0.0014 Lr: 0.03247
2024-08-21 19:53:19.670 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][57/77] Data 0.027 (0.027) Batch 0.065 (0.070) Remain 00:00:01 loss: 0.0745 data: 0.0205 Lr: 0.03247
2024-08-21 19:53:19.735 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][58/77] Data 0.027 (0.028) Batch 0.065 (0.070) Remain 00:00:01 loss: 0.0920 data: -0.0027 Lr: 0.03084
2024-08-21 19:53:19.735 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][58/77] Data 0.027 (0.027) Batch 0.065 (0.070) Remain 00:00:01 loss: 0.0920 data: 0.0059 Lr: 0.03084
2024-08-21 19:53:19.800 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][59/77] Data 0.027 (0.028) Batch 0.065 (0.070) Remain 00:00:01 loss: 0.0819 data: 0.0132 Lr: 0.02922
2024-08-21 19:53:19.800 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][59/77] Data 0.027 (0.027) Batch 0.065 (0.070) Remain 00:00:01 loss: 0.0819 data: -0.0040 Lr: 0.02922
2024-08-21 19:53:19.866 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][60/77] Data 0.027 (0.028) Batch 0.066 (0.069) Remain 00:00:01 loss: 0.0944 data: -0.0033 Lr: 0.02760
2024-08-21 19:53:19.866 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][60/77] Data 0.027 (0.027) Batch 0.066 (0.069) Remain 00:00:01 loss: 0.0944 data: 0.0013 Lr: 0.02760
2024-08-21 19:53:19.931 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][61/77] Data 0.027 (0.028) Batch 0.065 (0.069) Remain 00:00:01 loss: 0.1111 data: -0.0037 Lr: 0.02597
2024-08-21 19:53:19.931 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_600
2024-08-21 19:53:19.931 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][61/77] Data 0.027 (0.027) Batch 0.065 (0.069) Remain 00:00:01 loss: 0.1111 data: -0.0081 Lr: 0.02597
2024-08-21 19:53:19.931 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_600
2024-08-21 19:53:19.958 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -582.36865234375
2024-08-21 19:53:19.959 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -582.36865234375
2024-08-21 19:53:19.959 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -281.17742919921875
2024-08-21 19:53:19.959 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -301.1912841796875
2024-08-21 19:53:20.028 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][62/77] Data 0.055 (0.028) Batch 0.097 (0.070) Remain 00:00:01 loss: 0.1209 data: 0.0063 Lr: 0.02435
2024-08-21 19:53:20.028 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][62/77] Data 0.055 (0.028) Batch 0.097 (0.070) Remain 00:00:01 loss: 0.1209 data: -0.0103 Lr: 0.02435
2024-08-21 19:53:20.098 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][63/77] Data 0.029 (0.028) Batch 0.070 (0.070) Remain 00:00:01 loss: 0.0839 data: 0.0070 Lr: 0.02273
2024-08-21 19:53:20.098 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][63/77] Data 0.028 (0.028) Batch 0.070 (0.070) Remain 00:00:01 loss: 0.0839 data: 0.0227 Lr: 0.02273
2024-08-21 19:53:20.166 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][64/77] Data 0.028 (0.028) Batch 0.067 (0.070) Remain 00:00:00 loss: 0.0999 data: 0.0008 Lr: 0.02110
2024-08-21 19:53:20.166 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][64/77] Data 0.028 (0.028) Batch 0.067 (0.070) Remain 00:00:00 loss: 0.0999 data: -0.0184 Lr: 0.02110
2024-08-21 19:53:20.231 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][65/77] Data 0.027 (0.028) Batch 0.066 (0.070) Remain 00:00:00 loss: 0.1116 data: 0.0134 Lr: 0.01948
2024-08-21 19:53:20.231 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][65/77] Data 0.027 (0.028) Batch 0.066 (0.070) Remain 00:00:00 loss: 0.1116 data: -0.0000 Lr: 0.01948
2024-08-21 19:53:20.306 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][66/77] Data 0.028 (0.028) Batch 0.075 (0.070) Remain 00:00:00 loss: 0.1193 data: -0.0271 Lr: 0.01786
2024-08-21 19:53:20.306 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][66/77] Data 0.027 (0.028) Batch 0.075 (0.070) Remain 00:00:00 loss: 0.1193 data: 0.0295 Lr: 0.01786
2024-08-21 19:53:20.373 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][67/77] Data 0.027 (0.028) Batch 0.067 (0.070) Remain 00:00:00 loss: 0.0628 data: 0.0089 Lr: 0.01623
2024-08-21 19:53:20.373 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][67/77] Data 0.027 (0.028) Batch 0.067 (0.070) Remain 00:00:00 loss: 0.0628 data: -0.0121 Lr: 0.01623
2024-08-21 19:53:20.443 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][68/77] Data 0.028 (0.028) Batch 0.070 (0.070) Remain 00:00:00 loss: 0.1181 data: 0.0155 Lr: 0.01461
2024-08-21 19:53:20.443 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][68/77] Data 0.028 (0.028) Batch 0.071 (0.070) Remain 00:00:00 loss: 0.1181 data: 0.0026 Lr: 0.01461
2024-08-21 19:53:20.514 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][69/77] Data 0.029 (0.028) Batch 0.070 (0.070) Remain 00:00:00 loss: 0.0372 data: -0.0251 Lr: 0.01299
2024-08-21 19:53:20.514 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][69/77] Data 0.028 (0.028) Batch 0.070 (0.070) Remain 00:00:00 loss: 0.0372 data: -0.0171 Lr: 0.01299
2024-08-21 19:53:20.584 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][70/77] Data 0.029 (0.028) Batch 0.070 (0.070) Remain 00:00:00 loss: 0.0785 data: -0.0143 Lr: 0.01136
2024-08-21 19:53:20.584 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][70/77] Data 0.028 (0.028) Batch 0.070 (0.070) Remain 00:00:00 loss: 0.0785 data: 0.0014 Lr: 0.01136
2024-08-21 19:53:20.654 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][71/77] Data 0.028 (0.028) Batch 0.070 (0.070) Remain 00:00:00 loss: 0.0458 data: -0.0263 Lr: 0.00974
2024-08-21 19:53:20.654 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][71/77] Data 0.028 (0.028) Batch 0.070 (0.070) Remain 00:00:00 loss: 0.0458 data: -0.0020 Lr: 0.00974
2024-08-21 19:53:20.724 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][72/77] Data 0.029 (0.028) Batch 0.070 (0.070) Remain 00:00:00 loss: 0.1089 data: 0.0012 Lr: 0.00812
2024-08-21 19:53:20.724 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][72/77] Data 0.028 (0.028) Batch 0.070 (0.070) Remain 00:00:00 loss: 0.1089 data: -0.0104 Lr: 0.00812
2024-08-21 19:53:20.794 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][73/77] Data 0.028 (0.028) Batch 0.070 (0.070) Remain 00:00:00 loss: 0.0840 data: -0.0014 Lr: 0.00649
2024-08-21 19:53:20.794 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][73/77] Data 0.028 (0.028) Batch 0.070 (0.070) Remain 00:00:00 loss: 0.0840 data: -0.0032 Lr: 0.00649
2024-08-21 19:53:20.864 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][74/77] Data 0.028 (0.028) Batch 0.070 (0.070) Remain 00:00:00 loss: 0.0875 data: 0.0047 Lr: 0.00487
2024-08-21 19:53:20.865 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][74/77] Data 0.028 (0.028) Batch 0.070 (0.070) Remain 00:00:00 loss: 0.0875 data: -0.0035 Lr: 0.00487
2024-08-21 19:53:20.934 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][75/77] Data 0.028 (0.028) Batch 0.070 (0.070) Remain 00:00:00 loss: 0.0517 data: -0.0082 Lr: 0.00325
2024-08-21 19:53:20.934 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][75/77] Data 0.028 (0.028) Batch 0.070 (0.070) Remain 00:00:00 loss: 0.0517 data: -0.0162 Lr: 0.00325
2024-08-21 19:53:20.999 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][76/77] Data 0.027 (0.028) Batch 0.065 (0.070) Remain 00:00:00 loss: 0.0703 data: 0.0136 Lr: 0.00162
2024-08-21 19:53:21.000 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][76/77] Data 0.027 (0.028) Batch 0.065 (0.070) Remain 00:00:00 loss: 0.0703 data: 0.0065 Lr: 0.00162
2024-08-21 19:53:21.041 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][77/77] Data 0.030 (0.028) Batch 0.042 (0.069) Remain 00:00:00 loss: 0.1636 data: -0.0050 Lr: 0.00000
2024-08-21 19:53:21.041 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][77/77] Data 0.030 (0.028) Batch 0.042 (0.069) Remain 00:00:00 loss: 0.1636 data: 0.0072 Lr: 0.00000
2024-08-21 19:53:21.041 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 19:53:21.041 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 19:53:25.297 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0301, Accuracy: 0.9896
2024-08-21 19:53:25.297 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0301, Accuracy: 0.9896
2024-08-21 19:53:25.298 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 19:53:25.298 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 19:53:25.298 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 19:53:25.298 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 19:53:25.298 | INFO     | trim.callbacks.evaluator:on_training_phase_end:49 - Best mIoU: 0.9896, epoch at  7
2024-08-21 19:53:25.298 | INFO     | trim.callbacks.evaluator:on_training_phase_end:49 - Best mIoU: 0.9896, epoch at  7
