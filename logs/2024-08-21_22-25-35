2024-08-21 22:25:35.079 | INFO     | __main__:configure_model:71 - => creating model ...
2024-08-21 22:25:35.094 | INFO     | __main__:configure_model:74 - Number of parameters: 1199882
2024-08-21 22:25:35.155 | INFO     | __main__:configure_model:71 - => creating model ...
2024-08-21 22:25:35.197 | INFO     | __main__:configure_model:74 - Number of parameters: 1199882
2024-08-21 22:25:37.344 | INFO     | trim.callbacks.misc:resume:179 - => Resuming from checkpoint: output/step_100
2024-08-21 22:25:37.384 | INFO     | trim.callbacks.misc:resume:184 - ### Model weight: -333.2380676269531
2024-08-21 22:25:37.384 | INFO     | trim.callbacks.misc:resume:185 - ### Optimizer weight: -169.3326416015625
2024-08-21 22:25:37.384 | INFO     | trim.callbacks.misc:resume:205 - => Skip epoch: 0
2024-08-21 22:25:38.047 | INFO     | trim.callbacks.misc:resume:179 - => Resuming from checkpoint: output/step_100
2024-08-21 22:25:38.836 | INFO     | trim.callbacks.misc:resume:184 - ### Model weight: -333.2380676269531
2024-08-21 22:25:38.836 | INFO     | trim.callbacks.misc:resume:185 - ### Optimizer weight: -163.90542602539062
2024-08-21 22:25:38.837 | INFO     | trim.callbacks.misc:resume:205 - => Skip epoch: 0
2024-08-21 22:25:38.911 | INFO     | trim.callbacks.misc:resume:207 - data: -0.010465910658240318
2024-08-21 22:25:38.928 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0022582923993468285
2024-08-21 22:25:38.939 | INFO     | trim.callbacks.misc:resume:207 - data: 0.016550030559301376
2024-08-21 22:25:38.959 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0019877648446708918
2024-08-21 22:25:38.968 | INFO     | trim.callbacks.misc:resume:207 - data: -0.012083113193511963
2024-08-21 22:25:38.988 | INFO     | trim.callbacks.misc:resume:207 - data: 0.006208587903529406
2024-08-21 22:25:38.996 | INFO     | trim.callbacks.misc:resume:207 - data: 0.005125232506543398
2024-08-21 22:25:39.017 | INFO     | trim.callbacks.misc:resume:207 - data: -0.004731396213173866
2024-08-21 22:25:39.024 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0165389571338892
2024-08-21 22:25:39.046 | INFO     | trim.callbacks.misc:resume:207 - data: 0.003405151888728142
2024-08-21 22:25:39.052 | INFO     | trim.callbacks.misc:resume:207 - data: -0.01452921237796545
2024-08-21 22:25:39.075 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0001479656930314377
2024-08-21 22:25:39.080 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0012582677882164717
2024-08-21 22:25:39.104 | INFO     | trim.callbacks.misc:resume:207 - data: -0.006574151571840048
2024-08-21 22:25:39.109 | INFO     | trim.callbacks.misc:resume:207 - data: 0.013121463358402252
2024-08-21 22:25:39.133 | INFO     | trim.callbacks.misc:resume:207 - data: 0.007438139524310827
2024-08-21 22:25:39.142 | INFO     | trim.callbacks.misc:resume:207 - data: 0.006436953321099281
2024-08-21 22:25:39.162 | INFO     | trim.callbacks.misc:resume:207 - data: 0.011390862986445427
2024-08-21 22:25:39.170 | INFO     | trim.callbacks.misc:resume:207 - data: -0.01039152778685093
2024-08-21 22:25:39.192 | INFO     | trim.callbacks.misc:resume:207 - data: -0.001181150902993977
2024-08-21 22:25:39.198 | INFO     | trim.callbacks.misc:resume:207 - data: -0.021304506808519363
2024-08-21 22:25:39.221 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0006022407906129956
2024-08-21 22:25:39.226 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0012370346812531352
2024-08-21 22:25:39.250 | INFO     | trim.callbacks.misc:resume:207 - data: -0.013356734067201614
2024-08-21 22:25:39.254 | INFO     | trim.callbacks.misc:resume:207 - data: -0.004466915037482977
2024-08-21 22:25:39.279 | INFO     | trim.callbacks.misc:resume:207 - data: 0.013606111519038677
2024-08-21 22:25:39.282 | INFO     | trim.callbacks.misc:resume:207 - data: 0.002524403855204582
2024-08-21 22:25:39.308 | INFO     | trim.callbacks.misc:resume:207 - data: 0.007530496921390295
2024-08-21 22:25:39.310 | INFO     | trim.callbacks.misc:resume:207 - data: -0.016346292570233345
2024-08-21 22:25:39.337 | INFO     | trim.callbacks.misc:resume:207 - data: -0.013398646377027035
2024-08-21 22:25:39.338 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0017635117983445525
2024-08-21 22:25:39.366 | INFO     | trim.callbacks.misc:resume:207 - data: 0.015550914220511913
2024-08-21 22:25:39.366 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0036392612382769585
2024-08-21 22:25:39.394 | INFO     | trim.callbacks.misc:resume:207 - data: -0.007989907637238503
2024-08-21 22:25:39.395 | INFO     | trim.callbacks.misc:resume:207 - data: -0.011133615858852863
2024-08-21 22:25:39.422 | INFO     | trim.callbacks.misc:resume:207 - data: -0.00439949007704854
2024-08-21 22:25:39.424 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0008696758886799216
2024-08-21 22:25:39.450 | INFO     | trim.callbacks.misc:resume:207 - data: 0.013678837567567825
2024-08-21 22:25:39.453 | INFO     | trim.callbacks.misc:resume:207 - data: -0.02323945425450802
2024-08-21 22:25:39.478 | INFO     | trim.callbacks.misc:resume:207 - data: 0.003253404051065445
2024-08-21 22:25:39.482 | INFO     | trim.callbacks.misc:resume:207 - data: -0.009103663265705109
2024-08-21 22:25:39.506 | INFO     | trim.callbacks.misc:resume:207 - data: -0.014360485598444939
2024-08-21 22:25:39.511 | INFO     | trim.callbacks.misc:resume:207 - data: 0.005427899770438671
2024-08-21 22:25:39.534 | INFO     | trim.callbacks.misc:resume:207 - data: -0.003831099718809128
2024-08-21 22:25:39.540 | INFO     | trim.callbacks.misc:resume:207 - data: 0.005197545979171991
2024-08-21 22:25:39.562 | INFO     | trim.callbacks.misc:resume:207 - data: 0.019381877034902573
2024-08-21 22:25:39.569 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0076192086562514305
2024-08-21 22:25:39.590 | INFO     | trim.callbacks.misc:resume:207 - data: -0.005598230287432671
2024-08-21 22:25:39.597 | INFO     | trim.callbacks.misc:resume:207 - data: -0.004473044071346521
2024-08-21 22:25:39.618 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0003653430612757802
2024-08-21 22:25:39.626 | INFO     | trim.callbacks.misc:resume:207 - data: 0.006265575531870127
2024-08-21 22:25:39.647 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0040033613331615925
2024-08-21 22:25:39.655 | INFO     | trim.callbacks.misc:resume:207 - data: -0.01321500726044178
2024-08-21 22:25:39.675 | INFO     | trim.callbacks.misc:resume:207 - data: -0.005245945882052183
2024-08-21 22:25:39.684 | INFO     | trim.callbacks.misc:resume:207 - data: 0.006776066962629557
2024-08-21 22:25:39.703 | INFO     | trim.callbacks.misc:resume:207 - data: -0.002411367604508996
2024-08-21 22:25:39.713 | INFO     | trim.callbacks.misc:resume:207 - data: -0.004721042700111866
2024-08-21 22:25:39.731 | INFO     | trim.callbacks.misc:resume:207 - data: 0.01229273434728384
2024-08-21 22:25:39.742 | INFO     | trim.callbacks.misc:resume:207 - data: 0.007709080819040537
2024-08-21 22:25:39.758 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0037546451203525066
2024-08-21 22:25:39.771 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0027331665623933077
2024-08-21 22:25:39.787 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0022452615667134523
2024-08-21 22:25:39.799 | INFO     | trim.callbacks.misc:resume:207 - data: 0.009746656753122807
2024-08-21 22:25:39.814 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0020636392291635275
2024-08-21 22:25:39.828 | INFO     | trim.callbacks.misc:resume:207 - data: 0.005136497784405947
2024-08-21 22:25:39.842 | INFO     | trim.callbacks.misc:resume:207 - data: 0.001234134892001748
2024-08-21 22:25:39.857 | INFO     | trim.callbacks.misc:resume:207 - data: -0.004589339252561331
2024-08-21 22:25:39.870 | INFO     | trim.callbacks.misc:resume:207 - data: -0.008358675055205822
2024-08-21 22:25:39.886 | INFO     | trim.callbacks.misc:resume:207 - data: -0.004590831231325865
2024-08-21 22:25:39.898 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0072958627715706825
2024-08-21 22:25:39.915 | INFO     | trim.callbacks.misc:resume:207 - data: 0.011632731184363365
2024-08-21 22:25:39.926 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0041428785771131516
2024-08-21 22:25:39.944 | INFO     | trim.callbacks.misc:resume:207 - data: -0.004550242330878973
2024-08-21 22:25:39.957 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0015681942459195852
2024-08-21 22:25:39.973 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0007751117227599025
2024-08-21 22:25:39.986 | INFO     | trim.callbacks.misc:resume:207 - data: -0.009420576505362988
2024-08-21 22:25:40.001 | INFO     | trim.callbacks.misc:resume:207 - data: 0.00461573526263237
2024-08-21 22:25:40.015 | INFO     | trim.callbacks.misc:resume:207 - data: 0.00017720468167681247
2024-08-21 22:25:40.030 | INFO     | trim.callbacks.misc:resume:207 - data: -0.002357443794608116
2024-08-21 22:25:40.044 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0068313139490783215
2024-08-21 22:25:40.059 | INFO     | trim.callbacks.misc:resume:207 - data: 0.021283524110913277
2024-08-21 22:25:40.072 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0068083154037594795
2024-08-21 22:25:40.087 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0009848130866885185
2024-08-21 22:25:40.101 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0065205292776227
2024-08-21 22:25:40.116 | INFO     | trim.callbacks.misc:resume:207 - data: 0.006694476585835218
2024-08-21 22:25:40.130 | INFO     | trim.callbacks.misc:resume:207 - data: -0.001014992711134255
2024-08-21 22:25:40.145 | INFO     | trim.callbacks.misc:resume:207 - data: 0.012823270633816719
2024-08-21 22:25:40.159 | INFO     | trim.callbacks.misc:resume:207 - data: 0.010308005847036839
2024-08-21 22:25:40.174 | INFO     | trim.callbacks.misc:resume:207 - data: -0.01765006221830845
2024-08-21 22:25:40.187 | INFO     | trim.callbacks.misc:resume:207 - data: 0.009162030182778835
2024-08-21 22:25:40.203 | INFO     | trim.callbacks.misc:resume:207 - data: -0.00993645191192627
2024-08-21 22:25:40.216 | INFO     | trim.callbacks.misc:resume:207 - data: 0.011270839720964432
2024-08-21 22:25:40.231 | INFO     | trim.callbacks.misc:resume:207 - data: -0.005323477555066347
2024-08-21 22:25:40.245 | INFO     | trim.callbacks.misc:resume:207 - data: -0.008299118839204311
2024-08-21 22:25:40.260 | INFO     | trim.callbacks.misc:resume:207 - data: -0.01598895527422428
2024-08-21 22:25:40.273 | INFO     | trim.callbacks.misc:resume:207 - data: -0.018460236489772797
2024-08-21 22:25:40.289 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0014924032147973776
2024-08-21 22:25:40.302 | INFO     | trim.callbacks.misc:resume:207 - data: 0.007342054508626461
2024-08-21 22:25:40.318 | INFO     | trim.callbacks.misc:resume:207 - data: 0.014011488296091557
2024-08-21 22:25:40.331 | INFO     | trim.callbacks.misc:resume:207 - data: 0.013315370306372643
2024-08-21 22:25:40.346 | INFO     | trim.callbacks.misc:resume:207 - data: 0.014639352448284626
2024-08-21 22:25:40.359 | INFO     | trim.callbacks.misc:resume:207 - data: -0.006340316031128168
2024-08-21 22:25:40.375 | INFO     | trim.callbacks.misc:resume:207 - data: -0.010670753195881844
2024-08-21 22:25:40.388 | INFO     | trim.callbacks.misc:resume:207 - data: 0.010151287540793419
2024-08-21 22:25:40.404 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0035998886451125145
2024-08-21 22:25:40.416 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0077559929341077805
2024-08-21 22:25:40.432 | INFO     | trim.callbacks.misc:resume:207 - data: -0.01663537323474884
2024-08-21 22:25:40.445 | INFO     | trim.callbacks.misc:resume:207 - data: -0.003010072745382786
2024-08-21 22:25:40.461 | INFO     | trim.callbacks.misc:resume:207 - data: 0.006026688031852245
2024-08-21 22:25:40.474 | INFO     | trim.callbacks.misc:resume:207 - data: 0.008713330142199993
2024-08-21 22:25:40.489 | INFO     | trim.callbacks.misc:resume:207 - data: -0.012870760634541512
2024-08-21 22:25:40.502 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0003205302054993808
2024-08-21 22:25:40.518 | INFO     | trim.callbacks.misc:resume:207 - data: -0.003439719323068857
2024-08-21 22:25:40.531 | INFO     | trim.callbacks.misc:resume:207 - data: -0.006622193846851587
2024-08-21 22:25:40.547 | INFO     | trim.callbacks.misc:resume:207 - data: 0.009802566841244698
2024-08-21 22:25:40.559 | INFO     | trim.callbacks.misc:resume:207 - data: -0.010254357941448689
2024-08-21 22:25:40.575 | INFO     | trim.callbacks.misc:resume:207 - data: 0.001976968953385949
2024-08-21 22:25:40.588 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0025456089060753584
2024-08-21 22:25:40.604 | INFO     | trim.callbacks.misc:resume:207 - data: -0.024705326184630394
2024-08-21 22:25:40.616 | INFO     | trim.callbacks.misc:resume:207 - data: 0.005350285209715366
2024-08-21 22:25:40.634 | INFO     | trim.callbacks.misc:resume:207 - data: -0.016568362712860107
2024-08-21 22:25:40.645 | INFO     | trim.callbacks.misc:resume:207 - data: -0.009928747080266476
2024-08-21 22:25:40.664 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0025535880122333765
2024-08-21 22:25:40.673 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0042135887779295444
2024-08-21 22:25:40.692 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0011394866742193699
2024-08-21 22:25:40.702 | INFO     | trim.callbacks.misc:resume:207 - data: 0.00042793602915480733
2024-08-21 22:25:40.721 | INFO     | trim.callbacks.misc:resume:207 - data: -0.002110604429617524
2024-08-21 22:25:40.730 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0013653398491442204
2024-08-21 22:25:40.750 | INFO     | trim.callbacks.misc:resume:207 - data: -0.006019842345267534
2024-08-21 22:25:40.759 | INFO     | trim.callbacks.misc:resume:207 - data: 0.008222968317568302
2024-08-21 22:25:40.779 | INFO     | trim.callbacks.misc:resume:207 - data: -0.02204568311572075
2024-08-21 22:25:40.787 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0106916269287467
2024-08-21 22:25:40.807 | INFO     | trim.callbacks.misc:resume:207 - data: 0.009460554458200932
2024-08-21 22:25:40.817 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0063099185936152935
2024-08-21 22:25:40.836 | INFO     | trim.callbacks.misc:resume:207 - data: 0.01225545909255743
2024-08-21 22:25:40.846 | INFO     | trim.callbacks.misc:resume:207 - data: -0.00032889729482121766
2024-08-21 22:25:40.865 | INFO     | trim.callbacks.misc:resume:207 - data: 0.00015508766227867454
2024-08-21 22:25:40.874 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0031650217715650797
2024-08-21 22:25:40.894 | INFO     | trim.callbacks.misc:resume:207 - data: -0.01189881470054388
2024-08-21 22:25:40.903 | INFO     | trim.callbacks.misc:resume:207 - data: -0.004711599089205265
2024-08-21 22:25:40.923 | INFO     | trim.callbacks.misc:resume:207 - data: -0.006250279489904642
2024-08-21 22:25:40.931 | INFO     | trim.callbacks.misc:resume:207 - data: 0.011563650332391262
2024-08-21 22:25:40.951 | INFO     | trim.callbacks.misc:resume:207 - data: -0.010005035437643528
2024-08-21 22:25:40.960 | INFO     | trim.callbacks.misc:resume:207 - data: -0.00416184589266777
2024-08-21 22:25:40.980 | INFO     | trim.callbacks.misc:resume:207 - data: 0.010936697013676167
2024-08-21 22:25:40.988 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0030085823964327574
2024-08-21 22:25:41.009 | INFO     | trim.callbacks.misc:resume:207 - data: -0.003492732997983694
2024-08-21 22:25:41.017 | INFO     | trim.callbacks.misc:resume:207 - data: 0.01886277087032795
2024-08-21 22:25:41.038 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0038414534647017717
2024-08-21 22:25:41.045 | INFO     | trim.callbacks.misc:resume:207 - data: -0.005145554896444082
2024-08-21 22:25:41.066 | INFO     | trim.callbacks.misc:resume:207 - data: 0.006278662942349911
2024-08-21 22:25:41.073 | INFO     | trim.callbacks.misc:resume:207 - data: -0.008792712353169918
2024-08-21 22:25:41.095 | INFO     | trim.callbacks.misc:resume:207 - data: 0.015260174870491028
2024-08-21 22:25:41.102 | INFO     | trim.callbacks.misc:resume:207 - data: -0.006539775989949703
2024-08-21 22:25:41.124 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0002908763417508453
2024-08-21 22:25:41.130 | INFO     | trim.callbacks.misc:resume:207 - data: 0.008932918310165405
2024-08-21 22:25:41.152 | INFO     | trim.callbacks.misc:resume:207 - data: 0.005173523910343647
2024-08-21 22:25:41.159 | INFO     | trim.callbacks.misc:resume:207 - data: 0.022707978263497353
2024-08-21 22:25:41.181 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0006490956875495613
2024-08-21 22:25:41.187 | INFO     | trim.callbacks.misc:resume:207 - data: -0.00316670723259449
2024-08-21 22:25:41.210 | INFO     | trim.callbacks.misc:resume:207 - data: -0.018077470362186432
2024-08-21 22:25:41.216 | INFO     | trim.callbacks.misc:resume:207 - data: -0.014502376317977905
2024-08-21 22:25:41.239 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0060407985001802444
2024-08-21 22:25:41.244 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0007786732749082148
2024-08-21 22:25:41.267 | INFO     | trim.callbacks.misc:resume:207 - data: 0.00900945533066988
2024-08-21 22:25:41.272 | INFO     | trim.callbacks.misc:resume:207 - data: -0.004163503181189299
2024-08-21 22:25:41.296 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0029972048941999674
2024-08-21 22:25:41.301 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0004124456609133631
2024-08-21 22:25:41.329 | INFO     | trim.callbacks.misc:resume:207 - data: 0.008457629941403866
2024-08-21 22:25:41.332 | INFO     | trim.callbacks.misc:resume:207 - data: -0.016228999942541122
2024-08-21 22:25:41.358 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0063209072686731815
2024-08-21 22:25:41.362 | INFO     | trim.callbacks.misc:resume:207 - data: 0.012409277260303497
2024-08-21 22:25:41.386 | INFO     | trim.callbacks.misc:resume:207 - data: -0.001413825899362564
2024-08-21 22:25:41.391 | INFO     | trim.callbacks.misc:resume:207 - data: -0.005324219353497028
2024-08-21 22:25:41.414 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0050204782746732235
2024-08-21 22:25:41.419 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0005643583135679364
2024-08-21 22:25:41.442 | INFO     | trim.callbacks.misc:resume:207 - data: 0.004109386820346117
2024-08-21 22:25:41.448 | INFO     | trim.callbacks.misc:resume:207 - data: 0.01132385153323412
2024-08-21 22:25:41.471 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0024459068663418293
2024-08-21 22:25:41.477 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0038793080020695925
2024-08-21 22:25:41.499 | INFO     | trim.callbacks.misc:resume:207 - data: 0.009676331654191017
2024-08-21 22:25:41.506 | INFO     | trim.callbacks.misc:resume:207 - data: -0.013450912199914455
2024-08-21 22:25:41.527 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0018270723521709442
2024-08-21 22:25:41.535 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0016734196105971932
2024-08-21 22:25:41.556 | INFO     | trim.callbacks.misc:resume:207 - data: 0.014852975495159626
2024-08-21 22:25:41.564 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0038268468342721462
2024-08-21 22:25:41.578 | INFO     | trim.callbacks.misc:resume:207 - data: 0.004753568209707737
2024-08-21 22:25:41.592 | INFO     | trim.callbacks.misc:resume:207 - data: -0.02467699535191059
2024-08-21 22:25:41.600 | INFO     | trim.callbacks.misc:resume:207 - data: 0.01162130106240511
2024-08-21 22:25:41.620 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0026484299451112747
2024-08-21 22:25:41.621 | INFO     | trim.callbacks.misc:resume:207 - data: -0.00959096197038889
2024-08-21 22:25:41.642 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0008132677176035941
2024-08-21 22:25:41.648 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0036450615152716637
2024-08-21 22:25:41.663 | INFO     | trim.callbacks.misc:resume:207 - data: -0.00475715659558773
2024-08-21 22:25:41.675 | INFO     | trim.callbacks.misc:resume:207 - data: 0.002644592896103859
2024-08-21 22:25:41.684 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0038738949224352837
2024-08-21 22:25:41.703 | INFO     | trim.callbacks.misc:resume:207 - data: -0.014735880307853222
2024-08-21 22:25:41.705 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0012922000605612993
2024-08-21 22:25:41.726 | INFO     | trim.callbacks.misc:resume:207 - data: 0.011266284622251987
2024-08-21 22:25:41.730 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0011548659531399608
2024-08-21 22:25:41.746 | INFO     | trim.callbacks.misc:resume:207 - data: -0.002165604615584016
2024-08-21 22:25:41.757 | INFO     | trim.callbacks.misc:resume:207 - data: 0.007140194997191429
2024-08-21 22:25:41.767 | INFO     | trim.callbacks.misc:resume:207 - data: -0.001759398845024407
2024-08-21 22:25:41.785 | INFO     | trim.callbacks.misc:resume:207 - data: 0.011268602684140205
2024-08-21 22:25:41.788 | INFO     | trim.callbacks.misc:resume:207 - data: 0.021444959565997124
2024-08-21 22:25:41.808 | INFO     | trim.callbacks.misc:resume:207 - data: 0.022582489997148514
2024-08-21 22:25:41.812 | INFO     | trim.callbacks.misc:resume:207 - data: -0.011279813945293427
2024-08-21 22:25:41.829 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0004761438467539847
2024-08-21 22:25:41.839 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0059347450733184814
2024-08-21 22:25:41.850 | INFO     | trim.callbacks.misc:resume:207 - data: -0.000970841443631798
2024-08-21 22:25:41.866 | INFO     | trim.callbacks.misc:resume:207 - data: -0.007647980470210314
2024-08-21 22:25:41.870 | INFO     | trim.callbacks.misc:resume:207 - data: 0.013864131644368172
2024-08-21 22:25:41.891 | INFO     | trim.callbacks.misc:resume:207 - data: 0.015674831345677376
2024-08-21 22:25:41.894 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0255317073315382
2024-08-21 22:25:41.911 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0009152612183243036
2024-08-21 22:25:41.921 | INFO     | trim.callbacks.misc:resume:207 - data: -0.015066958032548428
2024-08-21 22:25:41.932 | INFO     | trim.callbacks.misc:resume:207 - data: -0.005985549185425043
2024-08-21 22:25:41.948 | INFO     | trim.callbacks.misc:resume:207 - data: -0.015063892118632793
2024-08-21 22:25:41.954 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0034396927803754807
2024-08-21 22:25:41.975 | INFO     | trim.callbacks.misc:resume:207 - data: -0.01213198434561491
2024-08-21 22:25:41.975 | INFO     | trim.callbacks.misc:resume:207 - data: -0.015604617074131966
2024-08-21 22:25:41.997 | INFO     | trim.callbacks.misc:resume:207 - data: 0.004078572615981102
2024-08-21 22:25:42.003 | INFO     | trim.callbacks.misc:resume:207 - data: -0.004025008529424667
2024-08-21 22:25:42.018 | INFO     | trim.callbacks.misc:resume:207 - data: -0.001123251160606742
2024-08-21 22:25:42.031 | INFO     | trim.callbacks.misc:resume:207 - data: -0.004736614879220724
2024-08-21 22:25:42.038 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0005965250893495977
2024-08-21 22:25:42.058 | INFO     | trim.callbacks.misc:resume:207 - data: 0.01936282217502594
2024-08-21 22:25:42.059 | INFO     | trim.callbacks.misc:resume:207 - data: -0.02052324078977108
2024-08-21 22:25:42.079 | INFO     | trim.callbacks.misc:resume:207 - data: -0.013538050465285778
2024-08-21 22:25:42.085 | INFO     | trim.callbacks.misc:resume:207 - data: 0.001016537775285542
2024-08-21 22:25:42.100 | INFO     | trim.callbacks.misc:resume:207 - data: 0.011458370834589005
2024-08-21 22:25:42.112 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0027103880420327187
2024-08-21 22:25:42.121 | INFO     | trim.callbacks.misc:resume:207 - data: 0.009105123579502106
2024-08-21 22:25:42.139 | INFO     | trim.callbacks.misc:resume:207 - data: -0.004988919477909803
2024-08-21 22:25:42.141 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0003626094257924706
2024-08-21 22:25:42.162 | INFO     | trim.callbacks.misc:resume:207 - data: -0.010840225964784622
2024-08-21 22:25:42.167 | INFO     | trim.callbacks.misc:resume:207 - data: -0.007192156743258238
2024-08-21 22:25:42.182 | INFO     | trim.callbacks.misc:resume:207 - data: -0.015074660070240498
2024-08-21 22:25:42.194 | INFO     | trim.callbacks.misc:resume:207 - data: 0.00568252382799983
2024-08-21 22:25:42.203 | INFO     | trim.callbacks.misc:resume:207 - data: 0.011706202290952206
2024-08-21 22:25:42.221 | INFO     | trim.callbacks.misc:resume:207 - data: 0.020167862996459007
2024-08-21 22:25:42.223 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0256624948233366
2024-08-21 22:25:42.246 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0005396199994720519
2024-08-21 22:25:42.248 | INFO     | trim.callbacks.misc:resume:207 - data: 0.005183711647987366
2024-08-21 22:25:42.266 | INFO     | trim.callbacks.misc:resume:207 - data: -0.008083093911409378
2024-08-21 22:25:42.275 | INFO     | trim.callbacks.misc:resume:207 - data: -0.02278098091483116
2024-08-21 22:25:42.287 | INFO     | trim.callbacks.misc:resume:207 - data: -0.005856331903487444
2024-08-21 22:25:42.303 | INFO     | trim.callbacks.misc:resume:207 - data: -0.006360860541462898
2024-08-21 22:25:42.308 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0014080272521823645
2024-08-21 22:25:42.328 | INFO     | trim.callbacks.misc:resume:207 - data: 0.00157084537204355
2024-08-21 22:25:42.330 | INFO     | trim.callbacks.misc:resume:207 - data: 0.015326689928770065
2024-08-21 22:25:42.349 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0012228150153532624
2024-08-21 22:25:42.357 | INFO     | trim.callbacks.misc:resume:207 - data: 0.011801459826529026
2024-08-21 22:25:42.369 | INFO     | trim.callbacks.misc:resume:207 - data: 0.01516790222376585
2024-08-21 22:25:42.384 | INFO     | trim.callbacks.misc:resume:207 - data: 0.011018121615052223
2024-08-21 22:25:42.390 | INFO     | trim.callbacks.misc:resume:207 - data: 0.010970326140522957
2024-08-21 22:25:42.410 | INFO     | trim.callbacks.misc:resume:207 - data: -0.006284984759986401
2024-08-21 22:25:42.411 | INFO     | trim.callbacks.misc:resume:207 - data: 0.010700710117816925
2024-08-21 22:25:42.431 | INFO     | trim.callbacks.misc:resume:207 - data: -0.005897416267544031
2024-08-21 22:25:42.438 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0010320267174392939
2024-08-21 22:25:42.451 | INFO     | trim.callbacks.misc:resume:207 - data: -0.010902680456638336
2024-08-21 22:25:42.466 | INFO     | trim.callbacks.misc:resume:207 - data: 0.004609936382621527
2024-08-21 22:25:42.472 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0006056377314962447
2024-08-21 22:25:42.493 | INFO     | trim.callbacks.misc:resume:207 - data: 0.006942971609532833
2024-08-21 22:25:42.493 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0141229797154665
2024-08-21 22:25:42.513 | INFO     | trim.callbacks.misc:resume:207 - data: -0.006847577169537544
2024-08-21 22:25:42.520 | INFO     | trim.callbacks.misc:resume:207 - data: -0.005413929466158152
2024-08-21 22:25:42.534 | INFO     | trim.callbacks.misc:resume:207 - data: -0.005471082404255867
2024-08-21 22:25:42.547 | INFO     | trim.callbacks.misc:resume:207 - data: 0.02335994504392147
2024-08-21 22:25:42.554 | INFO     | trim.callbacks.misc:resume:207 - data: 0.007055041845887899
2024-08-21 22:25:42.574 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0003146493691019714
2024-08-21 22:25:42.575 | INFO     | trim.callbacks.misc:resume:207 - data: -0.022323086857795715
2024-08-21 22:25:42.595 | INFO     | trim.callbacks.misc:resume:207 - data: -0.003440880449488759
2024-08-21 22:25:42.601 | INFO     | trim.callbacks.misc:resume:207 - data: 0.004586081951856613
2024-08-21 22:25:42.616 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0016468844842165709
2024-08-21 22:25:42.628 | INFO     | trim.callbacks.misc:resume:207 - data: 0.002304153051227331
2024-08-21 22:25:42.636 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0024207253009080887
2024-08-21 22:25:42.655 | INFO     | trim.callbacks.misc:resume:207 - data: -0.01211955863982439
2024-08-21 22:25:42.657 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0013469790574163198
2024-08-21 22:25:42.677 | INFO     | trim.callbacks.misc:resume:207 - data: 0.015374318696558475
2024-08-21 22:25:42.682 | INFO     | trim.callbacks.misc:resume:207 - data: -0.029319623485207558
2024-08-21 22:25:42.698 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0027378874365240335
2024-08-21 22:25:42.711 | INFO     | trim.callbacks.misc:resume:207 - data: -0.015025043860077858
2024-08-21 22:25:42.719 | INFO     | trim.callbacks.misc:resume:207 - data: -0.007712504360824823
2024-08-21 22:25:42.738 | INFO     | trim.callbacks.misc:resume:207 - data: 0.013325477950274944
2024-08-21 22:25:42.739 | INFO     | trim.callbacks.misc:resume:207 - data: 0.006362239830195904
2024-08-21 22:25:42.760 | INFO     | trim.callbacks.misc:resume:207 - data: 0.001488510868512094
2024-08-21 22:25:42.765 | INFO     | trim.callbacks.misc:resume:207 - data: -0.007709854282438755
2024-08-21 22:25:42.782 | INFO     | trim.callbacks.misc:resume:207 - data: 0.00923334714025259
2024-08-21 22:25:42.783 | INFO     | trim.callbacks.misc:resume:207 - data: -0.001003230456262827
2024-08-21 22:25:42.783 | INFO     | trim.callbacks.misc:on_training_phase_start:70 - Namespace(block_size=None, checkpointing_steps='300', gamma=0.7, load_best_model=False, log_project='accl_test', log_tag='init_1', lr=1.0, lr_scheduler_type='linear', num_train_epochs=8, num_warmup_steps=0, output_dir='./output', per_device_eval_batch_size=1, per_device_train_batch_size=196, preprocessing_num_workers=None, report_to='all', resume_from_checkpoint='output/step_300', save_path='output/', seed=1, weight_decay=0.0, with_tracking=False)
2024-08-21 22:25:42.783 | INFO     | trim.engine.trainer:fit:125 - >>>>>>>>>>>>>>>> Start Training >>>>>>>>>>>>>>>>
2024-08-21 22:25:42.792 | INFO     | trim.callbacks.misc:resume:207 - data: 0.002092683920636773
2024-08-21 22:25:42.819 | INFO     | trim.callbacks.misc:resume:207 - data: 0.011831775307655334
2024-08-21 22:25:42.846 | INFO     | trim.callbacks.misc:resume:207 - data: 0.001067891251295805
2024-08-21 22:25:42.873 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0061202337965369225
2024-08-21 22:25:42.901 | INFO     | trim.callbacks.misc:resume:207 - data: -0.00423689279705286
2024-08-21 22:25:42.928 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0023472264874726534
2024-08-21 22:25:42.956 | INFO     | trim.callbacks.misc:resume:207 - data: -0.00969391968101263
2024-08-21 22:25:42.983 | INFO     | trim.callbacks.misc:resume:207 - data: -0.00897659920156002
2024-08-21 22:25:43.011 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0037812343798577785
2024-08-21 22:25:43.040 | INFO     | trim.callbacks.misc:resume:207 - data: 0.008107251487672329
2024-08-21 22:25:43.068 | INFO     | trim.callbacks.misc:resume:207 - data: 0.003289685118943453
2024-08-21 22:25:43.096 | INFO     | trim.callbacks.misc:resume:207 - data: -0.0019985332619398832
2024-08-21 22:25:43.123 | INFO     | trim.callbacks.misc:resume:207 - data: 0.008071551099419594
2024-08-21 22:25:43.151 | INFO     | trim.callbacks.misc:resume:207 - data: 0.004515176638960838
2024-08-21 22:25:43.178 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0005758722545579076
2024-08-21 22:25:43.205 | INFO     | trim.callbacks.misc:resume:207 - data: 0.007852048613131046
2024-08-21 22:25:43.235 | INFO     | trim.callbacks.misc:resume:207 - data: 0.0016291593201458454
2024-08-21 22:25:43.236 | INFO     | trim.callbacks.misc:resume:207 - data: -0.005936597008258104
2024-08-21 22:25:43.236 | INFO     | trim.callbacks.misc:on_training_phase_start:70 - Namespace(block_size=None, checkpointing_steps='300', gamma=0.7, load_best_model=False, log_project='accl_test', log_tag='init_1', lr=1.0, lr_scheduler_type='linear', num_train_epochs=8, num_warmup_steps=0, output_dir='./output', per_device_eval_batch_size=1, per_device_train_batch_size=196, preprocessing_num_workers=None, report_to='all', resume_from_checkpoint='output/step_300', save_path='output/', seed=1, weight_decay=0.0, with_tracking=False)
2024-08-21 22:25:43.236 | INFO     | trim.engine.trainer:fit:125 - >>>>>>>>>>>>>>>> Start Training >>>>>>>>>>>>>>>>
2024-08-21 22:25:43.694 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][24/77] Data 0.044 (0.044) Batch 0.893 (0.893) Remain 00:07:40 loss: 0.2858 data: 0.0036 Lr: 0.83604
2024-08-21 22:25:43.694 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][24/77] Data 0.058 (0.058) Batch 0.457 (0.457) Remain 00:03:55 loss: 0.2858 data: -0.0063 Lr: 0.83604
2024-08-21 22:25:43.763 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][25/77] Data 0.031 (0.031) Batch 0.069 (0.069) Remain 00:00:35 loss: 0.2697 data: 0.0165 Lr: 0.83442
2024-08-21 22:25:43.763 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][25/77] Data 0.039 (0.039) Batch 0.086 (0.086) Remain 00:00:44 loss: 0.2697 data: 0.0106 Lr: 0.83442
2024-08-21 22:25:43.827 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][26/77] Data 0.027 (0.029) Batch 0.064 (0.066) Remain 00:00:34 loss: 0.2188 data: -0.0164 Lr: 0.83279
2024-08-21 22:25:43.827 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][26/77] Data 0.021 (0.030) Batch 0.064 (0.075) Remain 00:00:38 loss: 0.2188 data: -0.0029 Lr: 0.83279
2024-08-21 22:25:43.890 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][27/77] Data 0.027 (0.028) Batch 0.063 (0.065) Remain 00:00:33 loss: 0.3187 data: 0.0028 Lr: 0.83117
2024-08-21 22:25:43.890 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][27/77] Data 0.021 (0.027) Batch 0.063 (0.071) Remain 00:00:36 loss: 0.3187 data: 0.0169 Lr: 0.83117
2024-08-21 22:25:43.954 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][28/77] Data 0.027 (0.028) Batch 0.064 (0.065) Remain 00:00:33 loss: 0.2331 data: -0.0073 Lr: 0.82955
2024-08-21 22:25:43.954 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][28/77] Data 0.021 (0.026) Batch 0.064 (0.069) Remain 00:00:35 loss: 0.2331 data: -0.0012 Lr: 0.82955
2024-08-21 22:25:44.017 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][29/77] Data 0.027 (0.028) Batch 0.063 (0.065) Remain 00:00:33 loss: 0.3149 data: -0.0012 Lr: 0.82792
2024-08-21 22:25:44.018 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][29/77] Data 0.021 (0.025) Batch 0.063 (0.068) Remain 00:00:34 loss: 0.3149 data: -0.0033 Lr: 0.82792
2024-08-21 22:25:44.082 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][30/77] Data 0.027 (0.028) Batch 0.065 (0.065) Remain 00:00:32 loss: 0.2111 data: -0.0090 Lr: 0.82630
2024-08-21 22:25:44.082 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][30/77] Data 0.023 (0.024) Batch 0.065 (0.068) Remain 00:00:34 loss: 0.2111 data: -0.0072 Lr: 0.82630
2024-08-21 22:25:44.144 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][31/77] Data 0.027 (0.028) Batch 0.062 (0.064) Remain 00:00:32 loss: 0.3079 data: 0.0136 Lr: 0.82468
2024-08-21 22:25:44.144 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][31/77] Data 0.023 (0.024) Batch 0.062 (0.067) Remain 00:00:34 loss: 0.3079 data: -0.0186 Lr: 0.82468
2024-08-21 22:25:44.210 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][32/77] Data 0.027 (0.027) Batch 0.066 (0.065) Remain 00:00:32 loss: 0.2183 data: 0.0071 Lr: 0.82305
2024-08-21 22:25:44.210 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][32/77] Data 0.022 (0.024) Batch 0.066 (0.067) Remain 00:00:33 loss: 0.2183 data: -0.0037 Lr: 0.82305
2024-08-21 22:25:44.276 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][33/77] Data 0.027 (0.027) Batch 0.066 (0.065) Remain 00:00:32 loss: 0.2403 data: 0.0149 Lr: 0.82143
2024-08-21 22:25:44.276 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][33/77] Data 0.027 (0.024) Batch 0.066 (0.067) Remain 00:00:33 loss: 0.2403 data: -0.0027 Lr: 0.82143
2024-08-21 22:25:44.342 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][34/77] Data 0.027 (0.027) Batch 0.066 (0.065) Remain 00:00:32 loss: 0.3028 data: 0.0117 Lr: 0.81981
2024-08-21 22:25:44.342 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][34/77] Data 0.027 (0.025) Batch 0.066 (0.067) Remain 00:00:33 loss: 0.3028 data: -0.0164 Lr: 0.81981
2024-08-21 22:25:44.410 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][35/77] Data 0.027 (0.027) Batch 0.068 (0.065) Remain 00:00:32 loss: 0.2077 data: 0.0046 Lr: 0.81818
2024-08-21 22:25:44.411 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][35/77] Data 0.028 (0.025) Batch 0.068 (0.067) Remain 00:00:33 loss: 0.2077 data: 0.0049 Lr: 0.81818
2024-08-21 22:25:44.481 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][36/77] Data 0.028 (0.027) Batch 0.071 (0.066) Remain 00:00:33 loss: 0.2814 data: -0.0107 Lr: 0.81656
2024-08-21 22:25:44.481 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][36/77] Data 0.028 (0.025) Batch 0.071 (0.067) Remain 00:00:33 loss: 0.2814 data: -0.0141 Lr: 0.81656
2024-08-21 22:25:44.552 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][37/77] Data 0.029 (0.028) Batch 0.071 (0.066) Remain 00:00:33 loss: 0.1938 data: -0.0088 Lr: 0.81494
2024-08-21 22:25:44.552 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][37/77] Data 0.029 (0.025) Batch 0.071 (0.067) Remain 00:00:33 loss: 0.1938 data: 0.0044 Lr: 0.81494
2024-08-21 22:25:44.625 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][38/77] Data 0.028 (0.028) Batch 0.073 (0.067) Remain 00:00:33 loss: 0.2396 data: 0.0169 Lr: 0.81331
2024-08-21 22:25:44.625 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][38/77] Data 0.029 (0.026) Batch 0.073 (0.068) Remain 00:00:34 loss: 0.2396 data: 0.0043 Lr: 0.81331
2024-08-21 22:25:44.698 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][39/77] Data 0.029 (0.028) Batch 0.073 (0.067) Remain 00:00:33 loss: 0.2582 data: 0.0070 Lr: 0.81169
2024-08-21 22:25:44.698 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][39/77] Data 0.029 (0.026) Batch 0.073 (0.068) Remain 00:00:34 loss: 0.2582 data: 0.0037 Lr: 0.81169
2024-08-21 22:25:44.770 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][40/77] Data 0.029 (0.028) Batch 0.072 (0.067) Remain 00:00:33 loss: 0.1748 data: 0.0044 Lr: 0.81006
2024-08-21 22:25:44.770 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][40/77] Data 0.029 (0.026) Batch 0.072 (0.068) Remain 00:00:34 loss: 0.1748 data: -0.0223 Lr: 0.81006
2024-08-21 22:25:44.844 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][41/77] Data 0.028 (0.028) Batch 0.074 (0.068) Remain 00:00:33 loss: 0.2578 data: 0.0002 Lr: 0.80844
2024-08-21 22:25:44.844 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][41/77] Data 0.029 (0.026) Batch 0.074 (0.069) Remain 00:00:34 loss: 0.2578 data: 0.0003 Lr: 0.80844
2024-08-21 22:25:44.914 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][42/77] Data 0.030 (0.028) Batch 0.070 (0.068) Remain 00:00:33 loss: 0.2609 data: -0.0108 Lr: 0.80682
2024-08-21 22:25:44.914 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][42/77] Data 0.023 (0.026) Batch 0.070 (0.069) Remain 00:00:34 loss: 0.2609 data: -0.0043 Lr: 0.80682
2024-08-21 22:25:44.980 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][43/77] Data 0.028 (0.028) Batch 0.066 (0.068) Remain 00:00:33 loss: 0.4019 data: -0.0178 Lr: 0.80519
2024-08-21 22:25:44.980 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][43/77] Data 0.022 (0.026) Batch 0.066 (0.069) Remain 00:00:34 loss: 0.4019 data: -0.0002 Lr: 0.80519
2024-08-21 22:25:45.045 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][44/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:33 loss: 0.4527 data: -0.0085 Lr: 0.80357
2024-08-21 22:25:45.045 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][44/77] Data 0.022 (0.026) Batch 0.066 (0.068) Remain 00:00:33 loss: 0.4527 data: 0.0022 Lr: 0.80357
2024-08-21 22:25:45.110 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][45/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:33 loss: 0.2723 data: -0.0076 Lr: 0.80195
2024-08-21 22:25:45.110 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][45/77] Data 0.022 (0.025) Batch 0.065 (0.068) Remain 00:00:33 loss: 0.2723 data: -0.0029 Lr: 0.80195
2024-08-21 22:25:45.185 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][46/77] Data 0.027 (0.028) Batch 0.074 (0.068) Remain 00:00:33 loss: 0.2986 data: -0.0116 Lr: 0.80032
2024-08-21 22:25:45.185 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][46/77] Data 0.022 (0.025) Batch 0.075 (0.069) Remain 00:00:33 loss: 0.2986 data: 0.0039 Lr: 0.80032
2024-08-21 22:25:45.259 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][47/77] Data 0.027 (0.028) Batch 0.074 (0.068) Remain 00:00:33 loss: 0.2841 data: -0.0083 Lr: 0.79870
2024-08-21 22:25:45.259 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][47/77] Data 0.036 (0.026) Batch 0.074 (0.069) Remain 00:00:33 loss: 0.2841 data: 0.0109 Lr: 0.79870
2024-08-21 22:25:45.325 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][48/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:33 loss: 0.1764 data: -0.0018 Lr: 0.79708
2024-08-21 22:25:45.325 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][48/77] Data 0.029 (0.026) Batch 0.066 (0.069) Remain 00:00:33 loss: 0.1764 data: 0.0121 Lr: 0.79708
2024-08-21 22:25:45.390 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][49/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:33 loss: 0.3041 data: -0.0057 Lr: 0.79545
2024-08-21 22:25:45.390 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][49/77] Data 0.023 (0.026) Batch 0.065 (0.069) Remain 00:00:33 loss: 0.3041 data: -0.0056 Lr: 0.79545
2024-08-21 22:25:45.455 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][50/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:33 loss: 0.2695 data: -0.0059 Lr: 0.79383
2024-08-21 22:25:45.455 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][50/77] Data 0.023 (0.026) Batch 0.065 (0.068) Remain 00:00:33 loss: 0.2695 data: -0.0177 Lr: 0.79383
2024-08-21 22:25:45.520 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][51/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:33 loss: 0.1977 data: 0.0019 Lr: 0.79221
2024-08-21 22:25:45.520 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][51/77] Data 0.022 (0.026) Batch 0.065 (0.068) Remain 00:00:33 loss: 0.1977 data: 0.0120 Lr: 0.79221
2024-08-21 22:25:45.587 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][52/77] Data 0.027 (0.028) Batch 0.067 (0.068) Remain 00:00:32 loss: 0.1861 data: -0.0249 Lr: 0.79058
2024-08-21 22:25:45.587 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][52/77] Data 0.022 (0.025) Batch 0.067 (0.068) Remain 00:00:33 loss: 0.1861 data: 0.0017 Lr: 0.79058
2024-08-21 22:25:45.653 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][53/77] Data 0.028 (0.028) Batch 0.066 (0.068) Remain 00:00:32 loss: 0.2033 data: -0.0014 Lr: 0.78896
2024-08-21 22:25:45.653 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][53/77] Data 0.022 (0.025) Batch 0.066 (0.068) Remain 00:00:33 loss: 0.2033 data: 0.0034 Lr: 0.78896
2024-08-21 22:25:45.719 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][54/77] Data 0.028 (0.028) Batch 0.066 (0.067) Remain 00:00:32 loss: 0.2667 data: 0.0218 Lr: 0.78734
2024-08-21 22:25:45.719 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][54/77] Data 0.022 (0.025) Batch 0.066 (0.068) Remain 00:00:33 loss: 0.2667 data: 0.0069 Lr: 0.78734
2024-08-21 22:25:45.784 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][55/77] Data 0.028 (0.028) Batch 0.066 (0.067) Remain 00:00:32 loss: 0.1959 data: 0.0029 Lr: 0.78571
2024-08-21 22:25:45.784 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][55/77] Data 0.022 (0.025) Batch 0.066 (0.068) Remain 00:00:32 loss: 0.1959 data: 0.0152 Lr: 0.78571
2024-08-21 22:25:45.850 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][56/77] Data 0.028 (0.028) Batch 0.066 (0.067) Remain 00:00:32 loss: 0.2772 data: 0.0070 Lr: 0.78409
2024-08-21 22:25:45.850 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][56/77] Data 0.023 (0.025) Batch 0.066 (0.068) Remain 00:00:32 loss: 0.2772 data: -0.0028 Lr: 0.78409
2024-08-21 22:25:45.915 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][57/77] Data 0.028 (0.028) Batch 0.065 (0.067) Remain 00:00:32 loss: 0.2550 data: 0.0044 Lr: 0.78247
2024-08-21 22:25:45.915 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][57/77] Data 0.022 (0.025) Batch 0.065 (0.068) Remain 00:00:32 loss: 0.2550 data: -0.0048 Lr: 0.78247
2024-08-21 22:25:45.981 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][58/77] Data 0.028 (0.028) Batch 0.066 (0.067) Remain 00:00:32 loss: 0.1554 data: 0.0036 Lr: 0.78084
2024-08-21 22:25:45.981 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][58/77] Data 0.022 (0.025) Batch 0.065 (0.068) Remain 00:00:32 loss: 0.1554 data: 0.0017 Lr: 0.78084
2024-08-21 22:25:46.046 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][59/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:32 loss: 0.2714 data: -0.0058 Lr: 0.77922
2024-08-21 22:25:46.046 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][59/77] Data 0.022 (0.025) Batch 0.065 (0.068) Remain 00:00:32 loss: 0.2714 data: 0.0104 Lr: 0.77922
2024-08-21 22:25:46.111 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][60/77] Data 0.028 (0.028) Batch 0.065 (0.067) Remain 00:00:32 loss: 0.1998 data: 0.0062 Lr: 0.77760
2024-08-21 22:25:46.111 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][60/77] Data 0.022 (0.025) Batch 0.065 (0.068) Remain 00:00:32 loss: 0.1998 data: -0.0078 Lr: 0.77760
2024-08-21 22:25:46.176 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][61/77] Data 0.027 (0.028) Batch 0.066 (0.067) Remain 00:00:32 loss: 0.2022 data: -0.0027 Lr: 0.77597
2024-08-21 22:25:46.176 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][61/77] Data 0.022 (0.025) Batch 0.066 (0.068) Remain 00:00:32 loss: 0.2022 data: 0.0060 Lr: 0.77597
2024-08-21 22:25:46.242 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][62/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:32 loss: 0.1744 data: 0.0132 Lr: 0.77435
2024-08-21 22:25:46.242 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][62/77] Data 0.021 (0.024) Batch 0.065 (0.068) Remain 00:00:32 loss: 0.1744 data: -0.0130 Lr: 0.77435
2024-08-21 22:25:46.307 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][63/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:31 loss: 0.2041 data: 0.0036 Lr: 0.77273
2024-08-21 22:25:46.307 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][63/77] Data 0.022 (0.024) Batch 0.065 (0.067) Remain 00:00:32 loss: 0.2041 data: 0.0162 Lr: 0.77273
2024-08-21 22:25:46.372 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][64/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:31 loss: 0.1735 data: 0.0171 Lr: 0.77110
2024-08-21 22:25:46.372 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][64/77] Data 0.022 (0.024) Batch 0.065 (0.067) Remain 00:00:32 loss: 0.1735 data: 0.0150 Lr: 0.77110
2024-08-21 22:25:46.437 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][65/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:31 loss: 0.2626 data: -0.0064 Lr: 0.76948
2024-08-21 22:25:46.437 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][65/77] Data 0.023 (0.024) Batch 0.065 (0.067) Remain 00:00:31 loss: 0.2626 data: 0.0033 Lr: 0.76948
2024-08-21 22:25:46.502 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][66/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:31 loss: 0.2081 data: 0.0051 Lr: 0.76786
2024-08-21 22:25:46.502 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][66/77] Data 0.025 (0.024) Batch 0.065 (0.067) Remain 00:00:31 loss: 0.2081 data: 0.0126 Lr: 0.76786
2024-08-21 22:25:46.569 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][67/77] Data 0.021 (0.024) Batch 0.067 (0.067) Remain 00:00:31 loss: 0.1938 data: 0.0009 Lr: 0.76623
2024-08-21 22:25:46.569 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][67/77] Data 0.027 (0.028) Batch 0.067 (0.067) Remain 00:00:31 loss: 0.1938 data: -0.0006 Lr: 0.76623
2024-08-21 22:25:46.634 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][68/77] Data 0.028 (0.028) Batch 0.065 (0.067) Remain 00:00:31 loss: 0.2109 data: -0.0102 Lr: 0.76461
2024-08-21 22:25:46.634 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][68/77] Data 0.022 (0.024) Batch 0.065 (0.067) Remain 00:00:31 loss: 0.2109 data: -0.0143 Lr: 0.76461
2024-08-21 22:25:46.699 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][69/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:31 loss: 0.2081 data: -0.0094 Lr: 0.76299
2024-08-21 22:25:46.699 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][69/77] Data 0.022 (0.024) Batch 0.065 (0.067) Remain 00:00:31 loss: 0.2081 data: 0.0056 Lr: 0.76299
2024-08-21 22:25:46.764 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][70/77] Data 0.027 (0.028) Batch 0.064 (0.067) Remain 00:00:31 loss: 0.1933 data: 0.0183 Lr: 0.76136
2024-08-21 22:25:46.764 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][70/77] Data 0.022 (0.024) Batch 0.064 (0.067) Remain 00:00:31 loss: 0.1933 data: -0.0179 Lr: 0.76136
2024-08-21 22:25:46.829 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][71/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:31 loss: 0.1376 data: 0.0079 Lr: 0.75974
2024-08-21 22:25:46.829 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][71/77] Data 0.021 (0.024) Batch 0.065 (0.067) Remain 00:00:31 loss: 0.1376 data: 0.0003 Lr: 0.75974
2024-08-21 22:25:46.894 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][72/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:31 loss: 0.1204 data: -0.0032 Lr: 0.75812
2024-08-21 22:25:46.894 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][72/77] Data 0.023 (0.024) Batch 0.065 (0.067) Remain 00:00:31 loss: 0.1204 data: 0.0072 Lr: 0.75812
2024-08-21 22:25:46.968 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][73/77] Data 0.027 (0.028) Batch 0.074 (0.067) Remain 00:00:31 loss: 0.1542 data: -0.0001 Lr: 0.75649
2024-08-21 22:25:46.968 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_150
2024-08-21 22:25:46.968 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][73/77] Data 0.022 (0.024) Batch 0.074 (0.067) Remain 00:00:31 loss: 0.1542 data: -0.0136 Lr: 0.75649
2024-08-21 22:25:46.968 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_150
2024-08-21 22:25:46.999 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -393.9523010253906
2024-08-21 22:25:46.999 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -188.40049743652344
2024-08-21 22:25:47.002 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -393.9523010253906
2024-08-21 22:25:47.003 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -205.55181884765625
2024-08-21 22:25:47.066 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][74/77] Data 0.059 (0.028) Batch 0.098 (0.067) Remain 00:00:31 loss: 0.3094 data: 0.0015 Lr: 0.75487
2024-08-21 22:25:47.066 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][74/77] Data 0.058 (0.025) Batch 0.098 (0.068) Remain 00:00:31 loss: 0.3094 data: -0.0041 Lr: 0.75487
2024-08-21 22:25:47.130 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][75/77] Data 0.027 (0.028) Batch 0.064 (0.067) Remain 00:00:31 loss: 0.2534 data: 0.0104 Lr: 0.75325
2024-08-21 22:25:47.130 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][75/77] Data 0.022 (0.025) Batch 0.064 (0.068) Remain 00:00:31 loss: 0.2534 data: -0.0017 Lr: 0.75325
2024-08-21 22:25:47.195 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][76/77] Data 0.027 (0.028) Batch 0.064 (0.067) Remain 00:00:31 loss: 0.1001 data: -0.0117 Lr: 0.75162
2024-08-21 22:25:47.195 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][76/77] Data 0.022 (0.025) Batch 0.064 (0.068) Remain 00:00:31 loss: 0.1001 data: 0.0095 Lr: 0.75162
2024-08-21 22:25:47.235 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][77/77] Data 0.030 (0.028) Batch 0.041 (0.067) Remain 00:00:30 loss: 0.2523 data: -0.0051 Lr: 0.75000
2024-08-21 22:25:47.235 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][77/77] Data 0.024 (0.025) Batch 0.041 (0.067) Remain 00:00:31 loss: 0.2523 data: -0.0003 Lr: 0.75000
2024-08-21 22:25:47.236 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 22:25:47.236 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 22:25:50.871 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0563, Accuracy: 0.9821
2024-08-21 22:25:50.872 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0563, Accuracy: 0.9821
2024-08-21 22:25:50.872 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 22:25:50.872 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 22:25:50.876 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 22:25:50.876 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 22:25:50.958 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][1/77] Data 0.044 (0.044) Batch 0.082 (0.082) Remain 00:00:37 loss: 0.2107 data: -0.0045 Lr: 0.74838
2024-08-21 22:25:50.959 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][1/77] Data 0.044 (0.044) Batch 0.082 (0.082) Remain 00:00:38 loss: 0.2107 data: -0.0011 Lr: 0.74838
2024-08-21 22:25:51.023 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][2/77] Data 0.023 (0.023) Batch 0.065 (0.065) Remain 00:00:29 loss: 0.2823 data: 0.0096 Lr: 0.74675
2024-08-21 22:25:51.023 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][2/77] Data 0.029 (0.029) Batch 0.065 (0.065) Remain 00:00:29 loss: 0.2823 data: 0.0084 Lr: 0.74675
2024-08-21 22:25:51.089 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][3/77] Data 0.022 (0.023) Batch 0.066 (0.065) Remain 00:00:30 loss: 0.2101 data: -0.0070 Lr: 0.74513
2024-08-21 22:25:51.089 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][3/77] Data 0.027 (0.028) Batch 0.065 (0.065) Remain 00:00:29 loss: 0.2101 data: 0.0068 Lr: 0.74513
2024-08-21 22:25:51.155 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][4/77] Data 0.021 (0.022) Batch 0.066 (0.065) Remain 00:00:30 loss: 0.2044 data: 0.0159 Lr: 0.74351
2024-08-21 22:25:51.155 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][4/77] Data 0.027 (0.028) Batch 0.066 (0.065) Remain 00:00:30 loss: 0.2044 data: 0.0033 Lr: 0.74351
2024-08-21 22:25:51.227 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][5/77] Data 0.028 (0.023) Batch 0.073 (0.067) Remain 00:00:30 loss: 0.1806 data: -0.0087 Lr: 0.74188
2024-08-21 22:25:51.227 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][5/77] Data 0.027 (0.028) Batch 0.073 (0.067) Remain 00:00:30 loss: 0.1806 data: 0.0059 Lr: 0.74188
2024-08-21 22:25:51.293 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][6/77] Data 0.028 (0.024) Batch 0.066 (0.067) Remain 00:00:30 loss: 0.3011 data: 0.0066 Lr: 0.74026
2024-08-21 22:25:51.293 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][6/77] Data 0.027 (0.028) Batch 0.066 (0.067) Remain 00:00:30 loss: 0.3011 data: 0.0003 Lr: 0.74026
2024-08-21 22:25:51.359 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][7/77] Data 0.028 (0.025) Batch 0.066 (0.067) Remain 00:00:30 loss: 0.2604 data: 0.0009 Lr: 0.73864
2024-08-21 22:25:51.359 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][7/77] Data 0.027 (0.028) Batch 0.066 (0.067) Remain 00:00:30 loss: 0.2604 data: -0.0049 Lr: 0.73864
2024-08-21 22:25:51.426 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][8/77] Data 0.028 (0.025) Batch 0.067 (0.067) Remain 00:00:30 loss: 0.2427 data: 0.0145 Lr: 0.73701
2024-08-21 22:25:51.426 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][8/77] Data 0.028 (0.028) Batch 0.067 (0.067) Remain 00:00:30 loss: 0.2427 data: 0.0171 Lr: 0.73701
2024-08-21 22:25:51.492 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][9/77] Data 0.028 (0.026) Batch 0.066 (0.067) Remain 00:00:30 loss: 0.1861 data: -0.0157 Lr: 0.73539
2024-08-21 22:25:51.492 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][9/77] Data 0.027 (0.028) Batch 0.066 (0.067) Remain 00:00:30 loss: 0.1861 data: 0.0071 Lr: 0.73539
2024-08-21 22:25:51.559 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][10/77] Data 0.028 (0.026) Batch 0.067 (0.067) Remain 00:00:30 loss: 0.2109 data: -0.0009 Lr: 0.73377
2024-08-21 22:25:51.559 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][10/77] Data 0.028 (0.028) Batch 0.067 (0.067) Remain 00:00:30 loss: 0.2109 data: -0.0004 Lr: 0.73377
2024-08-21 22:25:51.625 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][11/77] Data 0.027 (0.026) Batch 0.066 (0.067) Remain 00:00:30 loss: 0.2342 data: 0.0062 Lr: 0.73214
2024-08-21 22:25:51.625 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][11/77] Data 0.028 (0.028) Batch 0.066 (0.067) Remain 00:00:30 loss: 0.2342 data: -0.0147 Lr: 0.73214
2024-08-21 22:25:51.693 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][12/77] Data 0.028 (0.026) Batch 0.069 (0.067) Remain 00:00:30 loss: 0.1784 data: 0.0201 Lr: 0.73052
2024-08-21 22:25:51.694 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][12/77] Data 0.028 (0.028) Batch 0.069 (0.067) Remain 00:00:30 loss: 0.1784 data: 0.0268 Lr: 0.73052
2024-08-21 22:25:51.765 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][13/77] Data 0.029 (0.026) Batch 0.071 (0.067) Remain 00:00:30 loss: 0.1758 data: 0.0123 Lr: 0.72890
2024-08-21 22:25:51.765 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][13/77] Data 0.028 (0.028) Batch 0.071 (0.067) Remain 00:00:30 loss: 0.1758 data: -0.0012 Lr: 0.72890
2024-08-21 22:25:51.833 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][14/77] Data 0.029 (0.027) Batch 0.068 (0.067) Remain 00:00:30 loss: 0.2359 data: -0.0102 Lr: 0.72727
2024-08-21 22:25:51.833 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][14/77] Data 0.029 (0.028) Batch 0.068 (0.067) Remain 00:00:30 loss: 0.2359 data: -0.0023 Lr: 0.72727
2024-08-21 22:25:51.901 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][15/77] Data 0.029 (0.027) Batch 0.068 (0.067) Remain 00:00:30 loss: 0.1784 data: -0.0138 Lr: 0.72565
2024-08-21 22:25:51.901 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][15/77] Data 0.027 (0.028) Batch 0.068 (0.067) Remain 00:00:30 loss: 0.1784 data: -0.0008 Lr: 0.72565
2024-08-21 22:25:51.967 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][16/77] Data 0.028 (0.027) Batch 0.066 (0.067) Remain 00:00:30 loss: 0.2232 data: 0.0008 Lr: 0.72403
2024-08-21 22:25:51.967 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][16/77] Data 0.027 (0.028) Batch 0.066 (0.067) Remain 00:00:30 loss: 0.2232 data: -0.0084 Lr: 0.72403
2024-08-21 22:25:52.034 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][17/77] Data 0.028 (0.027) Batch 0.067 (0.067) Remain 00:00:29 loss: 0.1644 data: 0.0065 Lr: 0.72240
2024-08-21 22:25:52.034 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][17/77] Data 0.028 (0.028) Batch 0.067 (0.067) Remain 00:00:29 loss: 0.1644 data: 0.0021 Lr: 0.72240
2024-08-21 22:25:52.102 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][18/77] Data 0.027 (0.027) Batch 0.068 (0.067) Remain 00:00:29 loss: 0.2250 data: 0.0004 Lr: 0.72078
2024-08-21 22:25:52.102 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][18/77] Data 0.029 (0.028) Batch 0.068 (0.067) Remain 00:00:29 loss: 0.2250 data: -0.0179 Lr: 0.72078
2024-08-21 22:25:52.168 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][19/77] Data 0.027 (0.027) Batch 0.066 (0.067) Remain 00:00:29 loss: 0.2599 data: -0.0039 Lr: 0.71916
2024-08-21 22:25:52.168 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][19/77] Data 0.027 (0.028) Batch 0.066 (0.067) Remain 00:00:29 loss: 0.2599 data: 0.0078 Lr: 0.71916
2024-08-21 22:25:52.233 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][20/77] Data 0.027 (0.027) Batch 0.066 (0.067) Remain 00:00:29 loss: 0.1632 data: -0.0017 Lr: 0.71753
2024-08-21 22:25:52.233 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][20/77] Data 0.028 (0.028) Batch 0.066 (0.067) Remain 00:00:29 loss: 0.1632 data: 0.0066 Lr: 0.71753
2024-08-21 22:25:52.299 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][21/77] Data 0.027 (0.027) Batch 0.066 (0.067) Remain 00:00:29 loss: 0.1879 data: -0.0026 Lr: 0.71591
2024-08-21 22:25:52.299 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][21/77] Data 0.028 (0.028) Batch 0.066 (0.067) Remain 00:00:29 loss: 0.1879 data: 0.0077 Lr: 0.71591
2024-08-21 22:25:52.368 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][22/77] Data 0.030 (0.027) Batch 0.069 (0.067) Remain 00:00:29 loss: 0.1470 data: -0.0002 Lr: 0.71429
2024-08-21 22:25:52.368 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][22/77] Data 0.028 (0.028) Batch 0.069 (0.067) Remain 00:00:29 loss: 0.1470 data: -0.0077 Lr: 0.71429
2024-08-21 22:25:52.438 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][23/77] Data 0.028 (0.027) Batch 0.070 (0.067) Remain 00:00:29 loss: 0.1915 data: -0.0122 Lr: 0.71266
2024-08-21 22:25:52.438 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][23/77] Data 0.028 (0.028) Batch 0.070 (0.067) Remain 00:00:29 loss: 0.1915 data: 0.0054 Lr: 0.71266
2024-08-21 22:25:52.509 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][24/77] Data 0.028 (0.027) Batch 0.071 (0.067) Remain 00:00:29 loss: 0.2027 data: 0.0237 Lr: 0.71104
2024-08-21 22:25:52.509 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][24/77] Data 0.029 (0.028) Batch 0.071 (0.067) Remain 00:00:29 loss: 0.2027 data: 0.0091 Lr: 0.71104
2024-08-21 22:25:52.576 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][25/77] Data 0.027 (0.027) Batch 0.068 (0.067) Remain 00:00:29 loss: 0.2321 data: -0.0124 Lr: 0.70942
2024-08-21 22:25:52.576 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][25/77] Data 0.028 (0.028) Batch 0.068 (0.067) Remain 00:00:29 loss: 0.2321 data: 0.0053 Lr: 0.70942
2024-08-21 22:25:52.645 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][26/77] Data 0.028 (0.027) Batch 0.069 (0.067) Remain 00:00:29 loss: 0.1683 data: 0.0091 Lr: 0.70779
2024-08-21 22:25:52.646 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][26/77] Data 0.028 (0.028) Batch 0.069 (0.067) Remain 00:00:29 loss: 0.1683 data: 0.0113 Lr: 0.70779
2024-08-21 22:25:52.715 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][27/77] Data 0.028 (0.027) Batch 0.070 (0.068) Remain 00:00:29 loss: 0.2188 data: -0.0085 Lr: 0.70617
2024-08-21 22:25:52.715 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][27/77] Data 0.028 (0.028) Batch 0.070 (0.068) Remain 00:00:29 loss: 0.2188 data: -0.0077 Lr: 0.70617
2024-08-21 22:25:52.786 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][28/77] Data 0.029 (0.027) Batch 0.070 (0.068) Remain 00:00:29 loss: 0.1970 data: -0.0011 Lr: 0.70455
2024-08-21 22:25:52.786 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][28/77] Data 0.029 (0.028) Batch 0.070 (0.068) Remain 00:00:29 loss: 0.1970 data: 0.0013 Lr: 0.70455
2024-08-21 22:25:52.851 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][29/77] Data 0.027 (0.027) Batch 0.065 (0.068) Remain 00:00:29 loss: 0.1990 data: 0.0135 Lr: 0.70292
2024-08-21 22:25:52.851 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][29/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:29 loss: 0.1990 data: -0.0093 Lr: 0.70292
2024-08-21 22:25:52.917 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][30/77] Data 0.027 (0.027) Batch 0.066 (0.068) Remain 00:00:29 loss: 0.1589 data: 0.0096 Lr: 0.70130
2024-08-21 22:25:52.917 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][30/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:29 loss: 0.1589 data: -0.0269 Lr: 0.70130
2024-08-21 22:25:52.993 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][31/77] Data 0.027 (0.027) Batch 0.076 (0.068) Remain 00:00:29 loss: 0.1930 data: 0.0054 Lr: 0.69968
2024-08-21 22:25:52.994 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][31/77] Data 0.027 (0.028) Batch 0.076 (0.068) Remain 00:00:29 loss: 0.1930 data: 0.0053 Lr: 0.69968
2024-08-21 22:25:53.059 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][32/77] Data 0.028 (0.027) Batch 0.066 (0.068) Remain 00:00:29 loss: 0.1771 data: -0.0337 Lr: 0.69805
2024-08-21 22:25:53.059 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][32/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:29 loss: 0.1771 data: -0.0045 Lr: 0.69805
2024-08-21 22:25:53.126 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][33/77] Data 0.028 (0.027) Batch 0.066 (0.068) Remain 00:00:29 loss: 0.2450 data: -0.0012 Lr: 0.69643
2024-08-21 22:25:53.126 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][33/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:29 loss: 0.2450 data: 0.0139 Lr: 0.69643
2024-08-21 22:25:53.194 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][34/77] Data 0.027 (0.027) Batch 0.068 (0.068) Remain 00:00:29 loss: 0.1901 data: 0.0190 Lr: 0.69481
2024-08-21 22:25:53.194 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][34/77] Data 0.029 (0.028) Batch 0.068 (0.068) Remain 00:00:29 loss: 0.1901 data: 0.0132 Lr: 0.69481
2024-08-21 22:25:53.261 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][35/77] Data 0.028 (0.027) Batch 0.067 (0.068) Remain 00:00:28 loss: 0.1869 data: -0.0044 Lr: 0.69318
2024-08-21 22:25:53.261 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][35/77] Data 0.028 (0.028) Batch 0.067 (0.068) Remain 00:00:28 loss: 0.1869 data: 0.0115 Lr: 0.69318
2024-08-21 22:25:53.328 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][36/77] Data 0.027 (0.028) Batch 0.067 (0.068) Remain 00:00:28 loss: 0.1736 data: -0.0134 Lr: 0.69156
2024-08-21 22:25:53.328 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][36/77] Data 0.027 (0.027) Batch 0.067 (0.068) Remain 00:00:28 loss: 0.1736 data: -0.0074 Lr: 0.69156
2024-08-21 22:25:53.392 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][37/77] Data 0.025 (0.027) Batch 0.063 (0.068) Remain 00:00:28 loss: 0.2019 data: -0.0128 Lr: 0.68994
2024-08-21 22:25:53.392 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][37/77] Data 0.027 (0.028) Batch 0.064 (0.068) Remain 00:00:28 loss: 0.2019 data: -0.0070 Lr: 0.68994
2024-08-21 22:25:53.452 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][38/77] Data 0.022 (0.027) Batch 0.061 (0.067) Remain 00:00:28 loss: 0.2536 data: 0.0128 Lr: 0.68831
2024-08-21 22:25:53.452 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][38/77] Data 0.027 (0.028) Batch 0.061 (0.067) Remain 00:00:28 loss: 0.2536 data: 0.0175 Lr: 0.68831
2024-08-21 22:25:53.509 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][39/77] Data 0.023 (0.027) Batch 0.057 (0.067) Remain 00:00:28 loss: 0.2233 data: 0.0293 Lr: 0.68669
2024-08-21 22:25:53.509 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][39/77] Data 0.022 (0.028) Batch 0.057 (0.067) Remain 00:00:28 loss: 0.2233 data: 0.0159 Lr: 0.68669
2024-08-21 22:25:53.564 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][40/77] Data 0.023 (0.027) Batch 0.055 (0.067) Remain 00:00:28 loss: 0.2450 data: -0.0067 Lr: 0.68506
2024-08-21 22:25:53.564 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][40/77] Data 0.023 (0.027) Batch 0.055 (0.067) Remain 00:00:28 loss: 0.2450 data: -0.0013 Lr: 0.68506
2024-08-21 22:25:53.620 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][41/77] Data 0.023 (0.027) Batch 0.055 (0.067) Remain 00:00:28 loss: 0.1964 data: -0.0067 Lr: 0.68344
2024-08-21 22:25:53.620 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][41/77] Data 0.023 (0.027) Batch 0.055 (0.067) Remain 00:00:28 loss: 0.1964 data: -0.0098 Lr: 0.68344
2024-08-21 22:25:53.676 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][42/77] Data 0.023 (0.027) Batch 0.057 (0.066) Remain 00:00:27 loss: 0.2412 data: 0.0099 Lr: 0.68182
2024-08-21 22:25:53.676 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][42/77] Data 0.023 (0.027) Batch 0.057 (0.066) Remain 00:00:27 loss: 0.2412 data: -0.0034 Lr: 0.68182
2024-08-21 22:25:53.732 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][43/77] Data 0.022 (0.027) Batch 0.055 (0.066) Remain 00:00:27 loss: 0.1330 data: -0.0086 Lr: 0.68019
2024-08-21 22:25:53.732 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][43/77] Data 0.023 (0.027) Batch 0.055 (0.066) Remain 00:00:27 loss: 0.1330 data: -0.0002 Lr: 0.68019
2024-08-21 22:25:53.788 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][44/77] Data 0.023 (0.027) Batch 0.056 (0.066) Remain 00:00:27 loss: 0.2352 data: 0.0017 Lr: 0.67857
2024-08-21 22:25:53.788 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][44/77] Data 0.023 (0.027) Batch 0.056 (0.066) Remain 00:00:27 loss: 0.2352 data: 0.0015 Lr: 0.67857
2024-08-21 22:25:53.851 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][45/77] Data 0.022 (0.026) Batch 0.063 (0.066) Remain 00:00:27 loss: 0.1543 data: -0.0113 Lr: 0.67695
2024-08-21 22:25:53.851 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][45/77] Data 0.023 (0.027) Batch 0.063 (0.066) Remain 00:00:27 loss: 0.1543 data: -0.0039 Lr: 0.67695
2024-08-21 22:25:53.915 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][46/77] Data 0.027 (0.026) Batch 0.064 (0.066) Remain 00:00:27 loss: 0.2102 data: -0.0087 Lr: 0.67532
2024-08-21 22:25:53.915 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][46/77] Data 0.021 (0.027) Batch 0.064 (0.066) Remain 00:00:27 loss: 0.2102 data: -0.0039 Lr: 0.67532
2024-08-21 22:25:53.915 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_200
2024-08-21 22:25:53.916 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_200
2024-08-21 22:25:53.944 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -475.5408630371094
2024-08-21 22:25:53.944 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -475.5408630371094
2024-08-21 22:25:53.944 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -253.029052734375
2024-08-21 22:25:53.944 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -222.51181030273438
2024-08-21 22:25:54.010 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][47/77] Data 0.056 (0.027) Batch 0.095 (0.066) Remain 00:00:27 loss: 0.1650 data: 0.0070 Lr: 0.67370
2024-08-21 22:25:54.010 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][47/77] Data 0.050 (0.027) Batch 0.095 (0.066) Remain 00:00:27 loss: 0.1650 data: 0.0089 Lr: 0.67370
2024-08-21 22:25:54.076 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][48/77] Data 0.027 (0.027) Batch 0.066 (0.066) Remain 00:00:27 loss: 0.1767 data: -0.0072 Lr: 0.67208
2024-08-21 22:25:54.076 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][48/77] Data 0.023 (0.027) Batch 0.066 (0.066) Remain 00:00:27 loss: 0.1767 data: -0.0044 Lr: 0.67208
2024-08-21 22:25:54.147 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][49/77] Data 0.028 (0.027) Batch 0.071 (0.066) Remain 00:00:27 loss: 0.1702 data: 0.0042 Lr: 0.67045
2024-08-21 22:25:54.148 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][49/77] Data 0.022 (0.027) Batch 0.071 (0.066) Remain 00:00:27 loss: 0.1702 data: 0.0119 Lr: 0.67045
2024-08-21 22:25:54.212 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][50/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:27 loss: 0.1572 data: -0.0272 Lr: 0.66883
2024-08-21 22:25:54.212 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][50/77] Data 0.021 (0.027) Batch 0.065 (0.066) Remain 00:00:27 loss: 0.1572 data: -0.0082 Lr: 0.66883
2024-08-21 22:25:54.275 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][51/77] Data 0.027 (0.027) Batch 0.063 (0.066) Remain 00:00:27 loss: 0.1499 data: -0.0063 Lr: 0.66721
2024-08-21 22:25:54.275 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][51/77] Data 0.023 (0.027) Batch 0.063 (0.066) Remain 00:00:27 loss: 0.1499 data: 0.0005 Lr: 0.66721
2024-08-21 22:25:54.338 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][52/77] Data 0.027 (0.027) Batch 0.064 (0.066) Remain 00:00:27 loss: 0.1313 data: -0.0056 Lr: 0.66558
2024-08-21 22:25:54.338 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][52/77] Data 0.022 (0.027) Batch 0.064 (0.066) Remain 00:00:27 loss: 0.1313 data: 0.0022 Lr: 0.66558
2024-08-21 22:25:54.400 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][53/77] Data 0.027 (0.027) Batch 0.062 (0.066) Remain 00:00:27 loss: 0.2138 data: 0.0066 Lr: 0.66396
2024-08-21 22:25:54.400 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][53/77] Data 0.021 (0.027) Batch 0.062 (0.066) Remain 00:00:27 loss: 0.2138 data: 0.0006 Lr: 0.66396
2024-08-21 22:25:54.465 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][54/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:27 loss: 0.1629 data: 0.0115 Lr: 0.66234
2024-08-21 22:25:54.465 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][54/77] Data 0.022 (0.027) Batch 0.065 (0.066) Remain 00:00:27 loss: 0.1629 data: 0.0042 Lr: 0.66234
2024-08-21 22:25:54.530 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][55/77] Data 0.021 (0.027) Batch 0.064 (0.066) Remain 00:00:26 loss: 0.1104 data: 0.0062 Lr: 0.66071
2024-08-21 22:25:54.530 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][55/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:26 loss: 0.1104 data: 0.0112 Lr: 0.66071
2024-08-21 22:25:54.596 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][56/77] Data 0.028 (0.027) Batch 0.066 (0.066) Remain 00:00:26 loss: 0.2505 data: 0.0039 Lr: 0.65909
2024-08-21 22:25:54.596 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][56/77] Data 0.027 (0.027) Batch 0.066 (0.066) Remain 00:00:26 loss: 0.2505 data: -0.0098 Lr: 0.65909
2024-08-21 22:25:54.660 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][57/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:26 loss: 0.1393 data: -0.0106 Lr: 0.65747
2024-08-21 22:25:54.660 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][57/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:26 loss: 0.1393 data: -0.0118 Lr: 0.65747
2024-08-21 22:25:54.725 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][58/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:26 loss: 0.1816 data: -0.0061 Lr: 0.65584
2024-08-21 22:25:54.725 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][58/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:26 loss: 0.1816 data: -0.0058 Lr: 0.65584
2024-08-21 22:25:54.791 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][59/77] Data 0.027 (0.027) Batch 0.066 (0.066) Remain 00:00:26 loss: 0.1829 data: 0.0112 Lr: 0.65422
2024-08-21 22:25:54.791 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][59/77] Data 0.027 (0.027) Batch 0.066 (0.066) Remain 00:00:26 loss: 0.1829 data: 0.0194 Lr: 0.65422
2024-08-21 22:25:54.856 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][60/77] Data 0.027 (0.027) Batch 0.066 (0.066) Remain 00:00:26 loss: 0.1219 data: -0.0083 Lr: 0.65260
2024-08-21 22:25:54.856 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][60/77] Data 0.027 (0.027) Batch 0.066 (0.066) Remain 00:00:26 loss: 0.1219 data: -0.0125 Lr: 0.65260
2024-08-21 22:25:54.923 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][61/77] Data 0.027 (0.027) Batch 0.067 (0.066) Remain 00:00:26 loss: 0.1026 data: 0.0105 Lr: 0.65097
2024-08-21 22:25:54.923 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][61/77] Data 0.028 (0.027) Batch 0.067 (0.066) Remain 00:00:26 loss: 0.1026 data: -0.0034 Lr: 0.65097
2024-08-21 22:25:54.992 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][62/77] Data 0.028 (0.027) Batch 0.069 (0.066) Remain 00:00:26 loss: 0.3017 data: 0.0028 Lr: 0.64935
2024-08-21 22:25:54.993 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][62/77] Data 0.028 (0.027) Batch 0.069 (0.066) Remain 00:00:26 loss: 0.3017 data: 0.0012 Lr: 0.64935
2024-08-21 22:25:55.062 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][63/77] Data 0.028 (0.027) Batch 0.070 (0.066) Remain 00:00:26 loss: 0.1439 data: 0.0114 Lr: 0.64773
2024-08-21 22:25:55.062 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][63/77] Data 0.028 (0.027) Batch 0.070 (0.066) Remain 00:00:26 loss: 0.1439 data: -0.0043 Lr: 0.64773
2024-08-21 22:25:55.131 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][64/77] Data 0.028 (0.027) Batch 0.069 (0.066) Remain 00:00:26 loss: 0.1950 data: -0.0030 Lr: 0.64610
2024-08-21 22:25:55.132 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][64/77] Data 0.028 (0.027) Batch 0.069 (0.066) Remain 00:00:26 loss: 0.1950 data: 0.0092 Lr: 0.64610
2024-08-21 22:25:55.200 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][65/77] Data 0.028 (0.027) Batch 0.068 (0.066) Remain 00:00:26 loss: 0.1481 data: 0.0200 Lr: 0.64448
2024-08-21 22:25:55.200 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][65/77] Data 0.028 (0.027) Batch 0.068 (0.066) Remain 00:00:26 loss: 0.1481 data: 0.0101 Lr: 0.64448
2024-08-21 22:25:55.266 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][66/77] Data 0.028 (0.027) Batch 0.066 (0.066) Remain 00:00:26 loss: 0.1708 data: -0.0093 Lr: 0.64286
2024-08-21 22:25:55.266 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][66/77] Data 0.027 (0.027) Batch 0.066 (0.066) Remain 00:00:26 loss: 0.1708 data: 0.0158 Lr: 0.64286
2024-08-21 22:25:55.331 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][67/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:26 loss: 0.1717 data: -0.0030 Lr: 0.64123
2024-08-21 22:25:55.331 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][67/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:26 loss: 0.1717 data: -0.0095 Lr: 0.64123
2024-08-21 22:25:55.395 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][68/77] Data 0.027 (0.027) Batch 0.064 (0.066) Remain 00:00:26 loss: 0.1176 data: -0.0148 Lr: 0.63961
2024-08-21 22:25:55.395 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][68/77] Data 0.027 (0.027) Batch 0.064 (0.066) Remain 00:00:26 loss: 0.1176 data: -0.0122 Lr: 0.63961
2024-08-21 22:25:55.460 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][69/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:26 loss: 0.1610 data: -0.0094 Lr: 0.63799
2024-08-21 22:25:55.460 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][69/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:26 loss: 0.1610 data: -0.0098 Lr: 0.63799
2024-08-21 22:25:55.525 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][70/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:26 loss: 0.1315 data: -0.0026 Lr: 0.63636
2024-08-21 22:25:55.525 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][70/77] Data 0.028 (0.027) Batch 0.065 (0.066) Remain 00:00:26 loss: 0.1315 data: -0.0017 Lr: 0.63636
2024-08-21 22:25:55.590 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][71/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:25 loss: 0.1535 data: 0.0111 Lr: 0.63474
2024-08-21 22:25:55.590 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][71/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:25 loss: 0.1535 data: -0.0179 Lr: 0.63474
2024-08-21 22:25:55.663 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][72/77] Data 0.027 (0.027) Batch 0.073 (0.066) Remain 00:00:25 loss: 0.1682 data: 0.0086 Lr: 0.63312
2024-08-21 22:25:55.663 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][72/77] Data 0.028 (0.027) Batch 0.073 (0.066) Remain 00:00:25 loss: 0.1682 data: -0.0067 Lr: 0.63312
2024-08-21 22:25:55.739 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][73/77] Data 0.031 (0.027) Batch 0.076 (0.066) Remain 00:00:25 loss: 0.1353 data: 0.0017 Lr: 0.63149
2024-08-21 22:25:55.739 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][73/77] Data 0.027 (0.027) Batch 0.076 (0.066) Remain 00:00:25 loss: 0.1353 data: -0.0150 Lr: 0.63149
2024-08-21 22:25:55.811 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][74/77] Data 0.031 (0.027) Batch 0.073 (0.066) Remain 00:00:25 loss: 0.1715 data: -0.0004 Lr: 0.62987
2024-08-21 22:25:55.811 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][74/77] Data 0.027 (0.027) Batch 0.073 (0.066) Remain 00:00:25 loss: 0.1715 data: 0.0011 Lr: 0.62987
2024-08-21 22:25:55.927 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][75/77] Data 0.044 (0.027) Batch 0.116 (0.067) Remain 00:00:26 loss: 0.1643 data: -0.0026 Lr: 0.62825
2024-08-21 22:25:55.928 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][75/77] Data 0.036 (0.027) Batch 0.116 (0.067) Remain 00:00:26 loss: 0.1643 data: 0.0077 Lr: 0.62825
2024-08-21 22:25:55.997 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][76/77] Data 0.032 (0.028) Batch 0.069 (0.067) Remain 00:00:25 loss: 0.1376 data: -0.0095 Lr: 0.62662
2024-08-21 22:25:55.997 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][76/77] Data 0.027 (0.027) Batch 0.069 (0.067) Remain 00:00:25 loss: 0.1376 data: 0.0026 Lr: 0.62662
2024-08-21 22:25:56.040 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][77/77] Data 0.030 (0.028) Batch 0.043 (0.067) Remain 00:00:25 loss: 0.2094 data: -0.0129 Lr: 0.62500
2024-08-21 22:25:56.040 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][77/77] Data 0.030 (0.027) Batch 0.043 (0.067) Remain 00:00:25 loss: 0.2094 data: 0.0030 Lr: 0.62500
2024-08-21 22:25:56.040 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 22:25:56.040 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 22:25:59.988 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0486, Accuracy: 0.9843
2024-08-21 22:25:59.988 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0486, Accuracy: 0.9843
2024-08-21 22:25:59.988 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 22:25:59.988 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 22:25:59.989 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 22:25:59.989 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 22:26:00.069 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][1/77] Data 0.045 (0.045) Batch 0.081 (0.081) Remain 00:00:31 loss: 0.1434 data: -0.0047 Lr: 0.62338
2024-08-21 22:26:00.070 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][1/77] Data 0.045 (0.045) Batch 0.081 (0.081) Remain 00:00:31 loss: 0.1434 data: 0.0073 Lr: 0.62338
2024-08-21 22:26:00.125 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][2/77] Data 0.024 (0.024) Batch 0.056 (0.056) Remain 00:00:21 loss: 0.1587 data: -0.0086 Lr: 0.62175
2024-08-21 22:26:00.125 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][2/77] Data 0.023 (0.023) Batch 0.056 (0.056) Remain 00:00:21 loss: 0.1587 data: -0.0142 Lr: 0.62175
2024-08-21 22:26:00.183 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][3/77] Data 0.023 (0.023) Batch 0.057 (0.057) Remain 00:00:21 loss: 0.1448 data: -0.0071 Lr: 0.62013
2024-08-21 22:26:00.183 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][3/77] Data 0.023 (0.023) Batch 0.058 (0.057) Remain 00:00:21 loss: 0.1448 data: 0.0005 Lr: 0.62013
2024-08-21 22:26:00.248 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][4/77] Data 0.028 (0.025) Batch 0.065 (0.060) Remain 00:00:22 loss: 0.2313 data: -0.0244 Lr: 0.61851
2024-08-21 22:26:00.248 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][4/77] Data 0.023 (0.023) Batch 0.065 (0.060) Remain 00:00:22 loss: 0.2313 data: -0.0053 Lr: 0.61851
2024-08-21 22:26:00.314 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][5/77] Data 0.028 (0.026) Batch 0.066 (0.061) Remain 00:00:23 loss: 0.0985 data: 0.0004 Lr: 0.61688
2024-08-21 22:26:00.314 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][5/77] Data 0.023 (0.023) Batch 0.066 (0.061) Remain 00:00:23 loss: 0.0985 data: 0.0133 Lr: 0.61688
2024-08-21 22:26:00.379 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][6/77] Data 0.027 (0.026) Batch 0.065 (0.062) Remain 00:00:23 loss: 0.1920 data: 0.0166 Lr: 0.61526
2024-08-21 22:26:00.379 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][6/77] Data 0.023 (0.023) Batch 0.065 (0.062) Remain 00:00:23 loss: 0.1920 data: 0.0016 Lr: 0.61526
2024-08-21 22:26:00.445 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][7/77] Data 0.027 (0.026) Batch 0.066 (0.063) Remain 00:00:23 loss: 0.1572 data: -0.0050 Lr: 0.61364
2024-08-21 22:26:00.445 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][7/77] Data 0.022 (0.023) Batch 0.066 (0.063) Remain 00:00:23 loss: 0.1572 data: -0.0014 Lr: 0.61364
2024-08-21 22:26:00.510 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][8/77] Data 0.027 (0.026) Batch 0.065 (0.063) Remain 00:00:23 loss: 0.1722 data: -0.0056 Lr: 0.61201
2024-08-21 22:26:00.510 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][8/77] Data 0.021 (0.023) Batch 0.065 (0.063) Remain 00:00:23 loss: 0.1722 data: -0.0114 Lr: 0.61201
2024-08-21 22:26:00.580 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][9/77] Data 0.028 (0.027) Batch 0.070 (0.064) Remain 00:00:24 loss: 0.1131 data: 0.0029 Lr: 0.61039
2024-08-21 22:26:00.580 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][9/77] Data 0.022 (0.022) Batch 0.070 (0.064) Remain 00:00:24 loss: 0.1131 data: 0.0002 Lr: 0.61039
2024-08-21 22:26:00.652 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][10/77] Data 0.033 (0.027) Batch 0.072 (0.065) Remain 00:00:24 loss: 0.1186 data: 0.0060 Lr: 0.60877
2024-08-21 22:26:00.652 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][10/77] Data 0.021 (0.022) Batch 0.072 (0.065) Remain 00:00:24 loss: 0.1186 data: -0.0066 Lr: 0.60877
2024-08-21 22:26:00.718 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][11/77] Data 0.028 (0.027) Batch 0.066 (0.065) Remain 00:00:24 loss: 0.2154 data: 0.0084 Lr: 0.60714
2024-08-21 22:26:00.718 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][11/77] Data 0.021 (0.022) Batch 0.066 (0.065) Remain 00:00:24 loss: 0.2154 data: 0.0268 Lr: 0.60714
2024-08-21 22:26:00.784 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][12/77] Data 0.027 (0.027) Batch 0.066 (0.065) Remain 00:00:24 loss: 0.1222 data: 0.0143 Lr: 0.60552
2024-08-21 22:26:00.784 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][12/77] Data 0.023 (0.022) Batch 0.066 (0.065) Remain 00:00:24 loss: 0.1222 data: -0.0109 Lr: 0.60552
2024-08-21 22:26:00.850 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][13/77] Data 0.028 (0.027) Batch 0.066 (0.065) Remain 00:00:24 loss: 0.1540 data: 0.0112 Lr: 0.60390
2024-08-21 22:26:00.850 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][13/77] Data 0.022 (0.022) Batch 0.066 (0.065) Remain 00:00:24 loss: 0.1540 data: -0.0053 Lr: 0.60390
2024-08-21 22:26:00.913 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][14/77] Data 0.028 (0.027) Batch 0.063 (0.065) Remain 00:00:24 loss: 0.1585 data: 0.0104 Lr: 0.60227
2024-08-21 22:26:00.914 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][14/77] Data 0.022 (0.022) Batch 0.063 (0.065) Remain 00:00:24 loss: 0.1585 data: -0.0045 Lr: 0.60227
2024-08-21 22:26:00.978 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][15/77] Data 0.028 (0.027) Batch 0.065 (0.065) Remain 00:00:24 loss: 0.1526 data: -0.0038 Lr: 0.60065
2024-08-21 22:26:00.978 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][15/77] Data 0.023 (0.022) Batch 0.065 (0.065) Remain 00:00:24 loss: 0.1526 data: -0.0167 Lr: 0.60065
2024-08-21 22:26:01.043 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][16/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:24 loss: 0.1369 data: -0.0055 Lr: 0.59903
2024-08-21 22:26:01.043 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][16/77] Data 0.022 (0.022) Batch 0.065 (0.065) Remain 00:00:24 loss: 0.1369 data: 0.0192 Lr: 0.59903
2024-08-21 22:26:01.116 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][17/77] Data 0.022 (0.022) Batch 0.072 (0.065) Remain 00:00:24 loss: 0.1479 data: -0.0025 Lr: 0.59740
2024-08-21 22:26:01.116 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][17/77] Data 0.027 (0.027) Batch 0.072 (0.065) Remain 00:00:24 loss: 0.1479 data: -0.0110 Lr: 0.59740
2024-08-21 22:26:01.199 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][18/77] Data 0.021 (0.022) Batch 0.084 (0.066) Remain 00:00:24 loss: 0.1236 data: 0.0059 Lr: 0.59578
2024-08-21 22:26:01.199 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][18/77] Data 0.037 (0.028) Batch 0.084 (0.066) Remain 00:00:24 loss: 0.1236 data: 0.0192 Lr: 0.59578
2024-08-21 22:26:01.279 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][19/77] Data 0.037 (0.028) Batch 0.080 (0.067) Remain 00:00:24 loss: 0.1158 data: 0.0032 Lr: 0.59416
2024-08-21 22:26:01.279 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][19/77] Data 0.021 (0.022) Batch 0.080 (0.067) Remain 00:00:24 loss: 0.1158 data: 0.0162 Lr: 0.59416
2024-08-21 22:26:01.279 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_250
2024-08-21 22:26:01.279 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_250
2024-08-21 22:26:01.304 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -496.0804443359375
2024-08-21 22:26:01.304 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -247.06309509277344
2024-08-21 22:26:01.305 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -496.0804443359375
2024-08-21 22:26:01.305 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -249.0173797607422
2024-08-21 22:26:01.372 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][20/77] Data 0.054 (0.030) Batch 0.093 (0.069) Remain 00:00:25 loss: 0.1299 data: 0.0024 Lr: 0.59253
2024-08-21 22:26:01.372 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][20/77] Data 0.046 (0.023) Batch 0.093 (0.069) Remain 00:00:25 loss: 0.1299 data: 0.0002 Lr: 0.59253
2024-08-21 22:26:01.438 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][21/77] Data 0.027 (0.030) Batch 0.065 (0.068) Remain 00:00:24 loss: 0.1526 data: -0.0058 Lr: 0.59091
2024-08-21 22:26:01.438 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][21/77] Data 0.022 (0.023) Batch 0.065 (0.068) Remain 00:00:24 loss: 0.1526 data: -0.0005 Lr: 0.59091
2024-08-21 22:26:01.502 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][22/77] Data 0.027 (0.030) Batch 0.065 (0.068) Remain 00:00:24 loss: 0.1205 data: 0.0166 Lr: 0.58929
2024-08-21 22:26:01.503 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][22/77] Data 0.022 (0.023) Batch 0.065 (0.068) Remain 00:00:24 loss: 0.1205 data: -0.0062 Lr: 0.58929
2024-08-21 22:26:01.574 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][23/77] Data 0.029 (0.030) Batch 0.071 (0.068) Remain 00:00:24 loss: 0.1125 data: 0.0021 Lr: 0.58766
2024-08-21 22:26:01.574 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][23/77] Data 0.021 (0.023) Batch 0.071 (0.068) Remain 00:00:24 loss: 0.1125 data: -0.0117 Lr: 0.58766
2024-08-21 22:26:01.647 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][24/77] Data 0.032 (0.030) Batch 0.074 (0.069) Remain 00:00:24 loss: 0.0835 data: -0.0047 Lr: 0.58604
2024-08-21 22:26:01.647 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][24/77] Data 0.021 (0.023) Batch 0.074 (0.069) Remain 00:00:24 loss: 0.0835 data: -0.0134 Lr: 0.58604
2024-08-21 22:26:01.721 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][25/77] Data 0.031 (0.030) Batch 0.074 (0.069) Remain 00:00:24 loss: 0.1311 data: -0.0039 Lr: 0.58442
2024-08-21 22:26:01.721 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][25/77] Data 0.021 (0.023) Batch 0.074 (0.069) Remain 00:00:24 loss: 0.1311 data: -0.0034 Lr: 0.58442
2024-08-21 22:26:01.796 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][26/77] Data 0.032 (0.030) Batch 0.075 (0.069) Remain 00:00:24 loss: 0.1240 data: 0.0041 Lr: 0.58279
2024-08-21 22:26:01.796 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][26/77] Data 0.021 (0.023) Batch 0.075 (0.069) Remain 00:00:24 loss: 0.1240 data: -0.0055 Lr: 0.58279
2024-08-21 22:26:01.872 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][27/77] Data 0.032 (0.030) Batch 0.076 (0.069) Remain 00:00:24 loss: 0.1754 data: -0.0054 Lr: 0.58117
2024-08-21 22:26:01.872 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][27/77] Data 0.021 (0.023) Batch 0.076 (0.069) Remain 00:00:24 loss: 0.1754 data: -0.0092 Lr: 0.58117
2024-08-21 22:26:01.952 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][28/77] Data 0.021 (0.023) Batch 0.079 (0.070) Remain 00:00:24 loss: 0.2012 data: -0.0027 Lr: 0.57955
2024-08-21 22:26:01.952 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][28/77] Data 0.032 (0.030) Batch 0.080 (0.070) Remain 00:00:24 loss: 0.2012 data: 0.0072 Lr: 0.57955
2024-08-21 22:26:02.008 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][29/77] Data 0.023 (0.030) Batch 0.056 (0.069) Remain 00:00:24 loss: 0.1287 data: -0.0074 Lr: 0.57792
2024-08-21 22:26:02.008 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][29/77] Data 0.022 (0.023) Batch 0.056 (0.069) Remain 00:00:24 loss: 0.1287 data: 0.0011 Lr: 0.57792
2024-08-21 22:26:02.064 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][30/77] Data 0.022 (0.029) Batch 0.056 (0.069) Remain 00:00:24 loss: 0.2373 data: 0.0148 Lr: 0.57630
2024-08-21 22:26:02.064 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][30/77] Data 0.023 (0.023) Batch 0.056 (0.069) Remain 00:00:24 loss: 0.2373 data: -0.0176 Lr: 0.57630
2024-08-21 22:26:02.120 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][31/77] Data 0.022 (0.029) Batch 0.056 (0.068) Remain 00:00:24 loss: 0.0588 data: -0.0003 Lr: 0.57468
2024-08-21 22:26:02.120 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][31/77] Data 0.024 (0.023) Batch 0.056 (0.068) Remain 00:00:24 loss: 0.0588 data: 0.0021 Lr: 0.57468
2024-08-21 22:26:02.176 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][32/77] Data 0.023 (0.029) Batch 0.056 (0.068) Remain 00:00:24 loss: 0.0871 data: -0.0023 Lr: 0.57305
2024-08-21 22:26:02.176 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][32/77] Data 0.022 (0.023) Batch 0.056 (0.068) Remain 00:00:24 loss: 0.0871 data: -0.0045 Lr: 0.57305
2024-08-21 22:26:02.229 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][33/77] Data 0.022 (0.029) Batch 0.053 (0.067) Remain 00:00:23 loss: 0.1276 data: -0.0035 Lr: 0.57143
2024-08-21 22:26:02.229 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][33/77] Data 0.022 (0.023) Batch 0.053 (0.067) Remain 00:00:23 loss: 0.1276 data: -0.0185 Lr: 0.57143
2024-08-21 22:26:02.281 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][34/77] Data 0.022 (0.029) Batch 0.052 (0.067) Remain 00:00:23 loss: 0.0901 data: -0.0073 Lr: 0.56981
2024-08-21 22:26:02.281 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][34/77] Data 0.022 (0.023) Batch 0.052 (0.067) Remain 00:00:23 loss: 0.0901 data: 0.0006 Lr: 0.56981
2024-08-21 22:26:02.335 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][35/77] Data 0.023 (0.028) Batch 0.055 (0.067) Remain 00:00:23 loss: 0.1068 data: 0.0062 Lr: 0.56818
2024-08-21 22:26:02.335 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][35/77] Data 0.022 (0.023) Batch 0.055 (0.067) Remain 00:00:23 loss: 0.1068 data: -0.0006 Lr: 0.56818
2024-08-21 22:26:02.388 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][36/77] Data 0.022 (0.028) Batch 0.053 (0.066) Remain 00:00:23 loss: 0.1094 data: -0.0185 Lr: 0.56656
2024-08-21 22:26:02.388 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][36/77] Data 0.022 (0.023) Batch 0.053 (0.066) Remain 00:00:23 loss: 0.1094 data: 0.0126 Lr: 0.56656
2024-08-21 22:26:02.444 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][37/77] Data 0.022 (0.028) Batch 0.055 (0.066) Remain 00:00:23 loss: 0.1363 data: 0.0076 Lr: 0.56494
2024-08-21 22:26:02.444 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][37/77] Data 0.022 (0.023) Batch 0.055 (0.066) Remain 00:00:23 loss: 0.1363 data: 0.0157 Lr: 0.56494
2024-08-21 22:26:02.495 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][38/77] Data 0.022 (0.028) Batch 0.052 (0.066) Remain 00:00:22 loss: 0.1554 data: 0.0169 Lr: 0.56331
2024-08-21 22:26:02.495 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][38/77] Data 0.021 (0.023) Batch 0.052 (0.066) Remain 00:00:22 loss: 0.1554 data: -0.0074 Lr: 0.56331
2024-08-21 22:26:02.549 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][39/77] Data 0.022 (0.028) Batch 0.054 (0.065) Remain 00:00:22 loss: 0.1537 data: 0.0041 Lr: 0.56169
2024-08-21 22:26:02.549 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][39/77] Data 0.021 (0.023) Batch 0.054 (0.065) Remain 00:00:22 loss: 0.1537 data: -0.0038 Lr: 0.56169
2024-08-21 22:26:02.603 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][40/77] Data 0.023 (0.028) Batch 0.053 (0.065) Remain 00:00:22 loss: 0.0960 data: -0.0162 Lr: 0.56006
2024-08-21 22:26:02.603 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][40/77] Data 0.021 (0.022) Batch 0.053 (0.065) Remain 00:00:22 loss: 0.0960 data: 0.0020 Lr: 0.56006
2024-08-21 22:26:02.655 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][41/77] Data 0.023 (0.027) Batch 0.053 (0.065) Remain 00:00:22 loss: 0.2661 data: -0.0118 Lr: 0.55844
2024-08-21 22:26:02.655 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][41/77] Data 0.022 (0.022) Batch 0.053 (0.065) Remain 00:00:22 loss: 0.2661 data: -0.0029 Lr: 0.55844
2024-08-21 22:26:02.709 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][42/77] Data 0.022 (0.027) Batch 0.054 (0.064) Remain 00:00:22 loss: 0.1841 data: 0.0077 Lr: 0.55682
2024-08-21 22:26:02.710 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][42/77] Data 0.022 (0.022) Batch 0.054 (0.064) Remain 00:00:22 loss: 0.1841 data: -0.0087 Lr: 0.55682
2024-08-21 22:26:02.765 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][43/77] Data 0.022 (0.027) Batch 0.055 (0.064) Remain 00:00:22 loss: 0.1256 data: 0.0034 Lr: 0.55519
2024-08-21 22:26:02.765 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][43/77] Data 0.022 (0.022) Batch 0.055 (0.064) Remain 00:00:22 loss: 0.1256 data: -0.0074 Lr: 0.55519
2024-08-21 22:26:02.819 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][44/77] Data 0.022 (0.027) Batch 0.055 (0.064) Remain 00:00:21 loss: 0.1482 data: -0.0086 Lr: 0.55357
2024-08-21 22:26:02.820 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][44/77] Data 0.025 (0.023) Batch 0.055 (0.064) Remain 00:00:21 loss: 0.1482 data: -0.0160 Lr: 0.55357
2024-08-21 22:26:02.874 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][45/77] Data 0.022 (0.027) Batch 0.055 (0.064) Remain 00:00:21 loss: 0.1248 data: 0.0038 Lr: 0.55195
2024-08-21 22:26:02.874 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][45/77] Data 0.022 (0.023) Batch 0.055 (0.064) Remain 00:00:21 loss: 0.1248 data: -0.0082 Lr: 0.55195
2024-08-21 22:26:02.931 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][46/77] Data 0.023 (0.027) Batch 0.056 (0.064) Remain 00:00:21 loss: 0.1889 data: -0.0025 Lr: 0.55032
2024-08-21 22:26:02.931 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][46/77] Data 0.021 (0.022) Batch 0.056 (0.064) Remain 00:00:21 loss: 0.1889 data: -0.0056 Lr: 0.55032
2024-08-21 22:26:02.985 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][47/77] Data 0.022 (0.027) Batch 0.054 (0.063) Remain 00:00:21 loss: 0.1761 data: 0.0173 Lr: 0.54870
2024-08-21 22:26:02.985 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][47/77] Data 0.022 (0.022) Batch 0.054 (0.063) Remain 00:00:21 loss: 0.1761 data: 0.0061 Lr: 0.54870
2024-08-21 22:26:03.040 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][48/77] Data 0.022 (0.027) Batch 0.055 (0.063) Remain 00:00:21 loss: 0.1540 data: -0.0020 Lr: 0.54708
2024-08-21 22:26:03.040 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][48/77] Data 0.021 (0.022) Batch 0.055 (0.063) Remain 00:00:21 loss: 0.1540 data: 0.0014 Lr: 0.54708
2024-08-21 22:26:03.095 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][49/77] Data 0.023 (0.027) Batch 0.056 (0.063) Remain 00:00:21 loss: 0.1151 data: -0.0034 Lr: 0.54545
2024-08-21 22:26:03.096 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][49/77] Data 0.022 (0.022) Batch 0.056 (0.063) Remain 00:00:21 loss: 0.1151 data: -0.0062 Lr: 0.54545
2024-08-21 22:26:03.152 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][50/77] Data 0.022 (0.027) Batch 0.056 (0.063) Remain 00:00:21 loss: 0.1674 data: -0.0050 Lr: 0.54383
2024-08-21 22:26:03.152 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][50/77] Data 0.022 (0.022) Batch 0.056 (0.063) Remain 00:00:21 loss: 0.1674 data: -0.0072 Lr: 0.54383
2024-08-21 22:26:03.225 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][51/77] Data 0.030 (0.027) Batch 0.073 (0.063) Remain 00:00:21 loss: 0.2378 data: -0.0151 Lr: 0.54221
2024-08-21 22:26:03.225 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][51/77] Data 0.022 (0.022) Batch 0.073 (0.063) Remain 00:00:21 loss: 0.2378 data: -0.0041 Lr: 0.54221
2024-08-21 22:26:03.290 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][52/77] Data 0.027 (0.027) Batch 0.065 (0.063) Remain 00:00:21 loss: 0.1498 data: 0.0062 Lr: 0.54058
2024-08-21 22:26:03.290 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][52/77] Data 0.021 (0.022) Batch 0.065 (0.063) Remain 00:00:21 loss: 0.1498 data: -0.0049 Lr: 0.54058
2024-08-21 22:26:03.355 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][53/77] Data 0.027 (0.027) Batch 0.065 (0.063) Remain 00:00:21 loss: 0.1655 data: -0.0032 Lr: 0.53896
2024-08-21 22:26:03.355 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][53/77] Data 0.022 (0.022) Batch 0.065 (0.063) Remain 00:00:21 loss: 0.1655 data: 0.0042 Lr: 0.53896
2024-08-21 22:26:03.418 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][54/77] Data 0.027 (0.027) Batch 0.063 (0.063) Remain 00:00:20 loss: 0.1077 data: 0.0097 Lr: 0.53734
2024-08-21 22:26:03.418 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][54/77] Data 0.021 (0.022) Batch 0.063 (0.063) Remain 00:00:20 loss: 0.1077 data: 0.0007 Lr: 0.53734
2024-08-21 22:26:03.481 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][55/77] Data 0.027 (0.027) Batch 0.063 (0.063) Remain 00:00:20 loss: 0.1042 data: -0.0165 Lr: 0.53571
2024-08-21 22:26:03.481 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][55/77] Data 0.021 (0.022) Batch 0.063 (0.063) Remain 00:00:20 loss: 0.1042 data: -0.0001 Lr: 0.53571
2024-08-21 22:26:03.546 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][56/77] Data 0.027 (0.027) Batch 0.065 (0.063) Remain 00:00:20 loss: 0.0695 data: -0.0279 Lr: 0.53409
2024-08-21 22:26:03.546 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][56/77] Data 0.021 (0.022) Batch 0.065 (0.063) Remain 00:00:20 loss: 0.0695 data: 0.0034 Lr: 0.53409
2024-08-21 22:26:03.611 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][57/77] Data 0.027 (0.027) Batch 0.066 (0.063) Remain 00:00:20 loss: 0.0841 data: -0.0152 Lr: 0.53247
2024-08-21 22:26:03.612 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][57/77] Data 0.022 (0.022) Batch 0.066 (0.063) Remain 00:00:20 loss: 0.0841 data: -0.0163 Lr: 0.53247
2024-08-21 22:26:03.676 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][58/77] Data 0.027 (0.027) Batch 0.064 (0.063) Remain 00:00:20 loss: 0.1493 data: -0.0062 Lr: 0.53084
2024-08-21 22:26:03.676 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][58/77] Data 0.022 (0.022) Batch 0.064 (0.063) Remain 00:00:20 loss: 0.1493 data: 0.0013 Lr: 0.53084
2024-08-21 22:26:03.740 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][59/77] Data 0.027 (0.027) Batch 0.064 (0.063) Remain 00:00:20 loss: 0.2542 data: -0.0070 Lr: 0.52922
2024-08-21 22:26:03.740 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][59/77] Data 0.023 (0.022) Batch 0.064 (0.063) Remain 00:00:20 loss: 0.2542 data: 0.0049 Lr: 0.52922
2024-08-21 22:26:03.804 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][60/77] Data 0.027 (0.027) Batch 0.064 (0.063) Remain 00:00:20 loss: 0.1579 data: -0.0038 Lr: 0.52760
2024-08-21 22:26:03.804 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][60/77] Data 0.022 (0.022) Batch 0.064 (0.063) Remain 00:00:20 loss: 0.1579 data: 0.0050 Lr: 0.52760
2024-08-21 22:26:03.867 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][61/77] Data 0.027 (0.027) Batch 0.063 (0.063) Remain 00:00:20 loss: 0.1805 data: 0.0051 Lr: 0.52597
2024-08-21 22:26:03.867 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][61/77] Data 0.022 (0.022) Batch 0.063 (0.063) Remain 00:00:20 loss: 0.1805 data: 0.0004 Lr: 0.52597
2024-08-21 22:26:03.932 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][62/77] Data 0.027 (0.027) Batch 0.065 (0.063) Remain 00:00:20 loss: 0.1287 data: -0.0009 Lr: 0.52435
2024-08-21 22:26:03.932 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][62/77] Data 0.022 (0.022) Batch 0.065 (0.063) Remain 00:00:20 loss: 0.1287 data: 0.0161 Lr: 0.52435
2024-08-21 22:26:03.996 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][63/77] Data 0.027 (0.027) Batch 0.064 (0.063) Remain 00:00:20 loss: 0.0480 data: -0.0041 Lr: 0.52273
2024-08-21 22:26:03.996 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][63/77] Data 0.022 (0.022) Batch 0.064 (0.063) Remain 00:00:20 loss: 0.0480 data: 0.0031 Lr: 0.52273
2024-08-21 22:26:04.060 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][64/77] Data 0.027 (0.027) Batch 0.065 (0.063) Remain 00:00:20 loss: 0.0777 data: -0.0126 Lr: 0.52110
2024-08-21 22:26:04.061 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][64/77] Data 0.022 (0.022) Batch 0.065 (0.063) Remain 00:00:20 loss: 0.0777 data: 0.0043 Lr: 0.52110
2024-08-21 22:26:04.126 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][65/77] Data 0.027 (0.027) Batch 0.066 (0.063) Remain 00:00:20 loss: 0.0652 data: -0.0044 Lr: 0.51948
2024-08-21 22:26:04.127 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][65/77] Data 0.022 (0.022) Batch 0.066 (0.063) Remain 00:00:20 loss: 0.0652 data: 0.0093 Lr: 0.51948
2024-08-21 22:26:04.183 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][66/77] Data 0.022 (0.027) Batch 0.057 (0.063) Remain 00:00:20 loss: 0.0536 data: -0.0004 Lr: 0.51786
2024-08-21 22:26:04.183 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][66/77] Data 0.022 (0.022) Batch 0.057 (0.063) Remain 00:00:20 loss: 0.0536 data: -0.0055 Lr: 0.51786
2024-08-21 22:26:04.240 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][67/77] Data 0.023 (0.027) Batch 0.057 (0.063) Remain 00:00:20 loss: 0.0794 data: -0.0107 Lr: 0.51623
2024-08-21 22:26:04.240 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][67/77] Data 0.022 (0.022) Batch 0.057 (0.063) Remain 00:00:20 loss: 0.0794 data: -0.0045 Lr: 0.51623
2024-08-21 22:26:04.294 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][68/77] Data 0.023 (0.026) Batch 0.054 (0.063) Remain 00:00:20 loss: 0.1061 data: 0.0190 Lr: 0.51461
2024-08-21 22:26:04.294 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][68/77] Data 0.022 (0.022) Batch 0.054 (0.063) Remain 00:00:20 loss: 0.1061 data: -0.0070 Lr: 0.51461
2024-08-21 22:26:04.348 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][69/77] Data 0.022 (0.026) Batch 0.054 (0.063) Remain 00:00:19 loss: 0.1470 data: 0.0135 Lr: 0.51299
2024-08-21 22:26:04.348 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_300
2024-08-21 22:26:04.348 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][69/77] Data 0.022 (0.022) Batch 0.054 (0.063) Remain 00:00:19 loss: 0.1470 data: -0.0104 Lr: 0.51299
2024-08-21 22:26:04.348 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_300
2024-08-21 22:26:04.372 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -534.2822875976562
2024-08-21 22:26:04.372 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -534.2822875976562
2024-08-21 22:26:04.372 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -262.61199951171875
2024-08-21 22:26:04.372 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -271.6702880859375
2024-08-21 22:26:04.429 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][70/77] Data 0.046 (0.027) Batch 0.080 (0.063) Remain 00:00:19 loss: 0.1335 data: 0.0258 Lr: 0.51136
2024-08-21 22:26:04.429 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][70/77] Data 0.045 (0.023) Batch 0.080 (0.063) Remain 00:00:19 loss: 0.1335 data: -0.0014 Lr: 0.51136
2024-08-21 22:26:04.484 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][71/77] Data 0.023 (0.027) Batch 0.056 (0.063) Remain 00:00:19 loss: 0.0892 data: 0.0188 Lr: 0.50974
2024-08-21 22:26:04.484 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][71/77] Data 0.022 (0.023) Batch 0.056 (0.063) Remain 00:00:19 loss: 0.0892 data: -0.0099 Lr: 0.50974
2024-08-21 22:26:04.538 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][72/77] Data 0.023 (0.027) Batch 0.054 (0.063) Remain 00:00:19 loss: 0.1639 data: -0.0225 Lr: 0.50812
2024-08-21 22:26:04.538 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][72/77] Data 0.022 (0.023) Batch 0.054 (0.063) Remain 00:00:19 loss: 0.1639 data: 0.0022 Lr: 0.50812
2024-08-21 22:26:04.592 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][73/77] Data 0.023 (0.027) Batch 0.053 (0.063) Remain 00:00:19 loss: 0.1310 data: 0.0146 Lr: 0.50649
2024-08-21 22:26:04.592 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][73/77] Data 0.022 (0.023) Batch 0.053 (0.063) Remain 00:00:19 loss: 0.1310 data: 0.0028 Lr: 0.50649
2024-08-21 22:26:04.647 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][74/77] Data 0.023 (0.026) Batch 0.055 (0.063) Remain 00:00:19 loss: 0.1121 data: 0.0089 Lr: 0.50487
2024-08-21 22:26:04.647 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][74/77] Data 0.023 (0.023) Batch 0.055 (0.063) Remain 00:00:19 loss: 0.1121 data: 0.0203 Lr: 0.50487
2024-08-21 22:26:04.702 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][75/77] Data 0.022 (0.023) Batch 0.054 (0.063) Remain 00:00:19 loss: 0.1512 data: 0.0153 Lr: 0.50325
2024-08-21 22:26:04.702 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][75/77] Data 0.022 (0.026) Batch 0.055 (0.063) Remain 00:00:19 loss: 0.1512 data: 0.0108 Lr: 0.50325
2024-08-21 22:26:04.766 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][76/77] Data 0.029 (0.026) Batch 0.064 (0.063) Remain 00:00:19 loss: 0.1218 data: -0.0068 Lr: 0.50162
2024-08-21 22:26:04.766 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][76/77] Data 0.022 (0.023) Batch 0.065 (0.063) Remain 00:00:19 loss: 0.1218 data: -0.0105 Lr: 0.50162
2024-08-21 22:26:04.807 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][77/77] Data 0.030 (0.027) Batch 0.041 (0.062) Remain 00:00:19 loss: 0.1031 data: 0.0080 Lr: 0.50000
2024-08-21 22:26:04.807 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][77/77] Data 0.023 (0.023) Batch 0.041 (0.062) Remain 00:00:19 loss: 0.1031 data: 0.0143 Lr: 0.50000
2024-08-21 22:26:04.807 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 22:26:04.807 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 22:26:08.515 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0384, Accuracy: 0.9865
2024-08-21 22:26:08.515 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0384, Accuracy: 0.9865
2024-08-21 22:26:08.515 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 22:26:08.515 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 22:26:08.515 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 22:26:08.516 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 22:26:08.612 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][1/77] Data 0.056 (0.056) Batch 0.096 (0.096) Remain 00:00:29 loss: 0.1658 data: 0.0083 Lr: 0.49838
2024-08-21 22:26:08.612 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][1/77] Data 0.044 (0.044) Batch 0.096 (0.096) Remain 00:00:29 loss: 0.1658 data: 0.0052 Lr: 0.49838
2024-08-21 22:26:08.677 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][2/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:19 loss: 0.1151 data: 0.0093 Lr: 0.49675
2024-08-21 22:26:08.677 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][2/77] Data 0.021 (0.021) Batch 0.065 (0.065) Remain 00:00:19 loss: 0.1151 data: 0.0029 Lr: 0.49675
2024-08-21 22:26:08.746 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][3/77] Data 0.027 (0.027) Batch 0.069 (0.067) Remain 00:00:20 loss: 0.1291 data: -0.0161 Lr: 0.49513
2024-08-21 22:26:08.746 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][3/77] Data 0.022 (0.021) Batch 0.069 (0.067) Remain 00:00:20 loss: 0.1291 data: -0.0005 Lr: 0.49513
2024-08-21 22:26:08.810 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][4/77] Data 0.027 (0.027) Batch 0.064 (0.066) Remain 00:00:20 loss: 0.1344 data: 0.0085 Lr: 0.49351
2024-08-21 22:26:08.810 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][4/77] Data 0.021 (0.021) Batch 0.064 (0.066) Remain 00:00:20 loss: 0.1344 data: -0.0001 Lr: 0.49351
2024-08-21 22:26:08.872 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][5/77] Data 0.027 (0.027) Batch 0.062 (0.065) Remain 00:00:19 loss: 0.1415 data: -0.0002 Lr: 0.49188
2024-08-21 22:26:08.872 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][5/77] Data 0.021 (0.021) Batch 0.062 (0.065) Remain 00:00:19 loss: 0.1415 data: 0.0131 Lr: 0.49188
2024-08-21 22:26:08.935 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][6/77] Data 0.027 (0.027) Batch 0.064 (0.065) Remain 00:00:19 loss: 0.1329 data: -0.0010 Lr: 0.49026
2024-08-21 22:26:08.936 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][6/77] Data 0.021 (0.021) Batch 0.064 (0.065) Remain 00:00:19 loss: 0.1329 data: 0.0141 Lr: 0.49026
2024-08-21 22:26:09.000 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][7/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:19 loss: 0.1477 data: -0.0143 Lr: 0.48864
2024-08-21 22:26:09.000 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][7/77] Data 0.022 (0.021) Batch 0.065 (0.065) Remain 00:00:19 loss: 0.1477 data: -0.0092 Lr: 0.48864
2024-08-21 22:26:09.065 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][8/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:19 loss: 0.0949 data: 0.0009 Lr: 0.48701
2024-08-21 22:26:09.065 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][8/77] Data 0.021 (0.021) Batch 0.065 (0.065) Remain 00:00:19 loss: 0.0949 data: -0.0091 Lr: 0.48701
2024-08-21 22:26:09.132 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][9/77] Data 0.021 (0.021) Batch 0.067 (0.065) Remain 00:00:19 loss: 0.0945 data: 0.0132 Lr: 0.48539
2024-08-21 22:26:09.132 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][9/77] Data 0.027 (0.027) Batch 0.067 (0.065) Remain 00:00:19 loss: 0.0945 data: 0.0181 Lr: 0.48539
2024-08-21 22:26:09.200 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][10/77] Data 0.031 (0.028) Batch 0.069 (0.065) Remain 00:00:19 loss: 0.0847 data: -0.0050 Lr: 0.48377
2024-08-21 22:26:09.201 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][10/77] Data 0.022 (0.021) Batch 0.069 (0.065) Remain 00:00:19 loss: 0.0847 data: 0.0072 Lr: 0.48377
2024-08-21 22:26:09.264 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][11/77] Data 0.027 (0.028) Batch 0.063 (0.065) Remain 00:00:19 loss: 0.1087 data: 0.0128 Lr: 0.48214
2024-08-21 22:26:09.264 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][11/77] Data 0.022 (0.021) Batch 0.063 (0.065) Remain 00:00:19 loss: 0.1087 data: 0.0006 Lr: 0.48214
2024-08-21 22:26:09.330 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][12/77] Data 0.028 (0.028) Batch 0.066 (0.065) Remain 00:00:19 loss: 0.1161 data: 0.0025 Lr: 0.48052
2024-08-21 22:26:09.330 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][12/77] Data 0.021 (0.021) Batch 0.066 (0.065) Remain 00:00:19 loss: 0.1161 data: -0.0087 Lr: 0.48052
2024-08-21 22:26:09.394 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][13/77] Data 0.027 (0.028) Batch 0.064 (0.065) Remain 00:00:19 loss: 0.0677 data: 0.0144 Lr: 0.47890
2024-08-21 22:26:09.394 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][13/77] Data 0.021 (0.021) Batch 0.064 (0.065) Remain 00:00:19 loss: 0.0677 data: 0.0046 Lr: 0.47890
2024-08-21 22:26:09.458 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][14/77] Data 0.027 (0.028) Batch 0.064 (0.065) Remain 00:00:19 loss: 0.1154 data: -0.0103 Lr: 0.47727
2024-08-21 22:26:09.458 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][14/77] Data 0.021 (0.021) Batch 0.064 (0.065) Remain 00:00:19 loss: 0.1154 data: 0.0026 Lr: 0.47727
2024-08-21 22:26:09.522 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][15/77] Data 0.027 (0.028) Batch 0.064 (0.065) Remain 00:00:19 loss: 0.0741 data: 0.0008 Lr: 0.47565
2024-08-21 22:26:09.523 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][15/77] Data 0.021 (0.021) Batch 0.064 (0.065) Remain 00:00:19 loss: 0.0741 data: -0.0049 Lr: 0.47565
2024-08-21 22:26:09.586 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][16/77] Data 0.027 (0.027) Batch 0.064 (0.065) Remain 00:00:19 loss: 0.1352 data: -0.0114 Lr: 0.47403
2024-08-21 22:26:09.586 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][16/77] Data 0.021 (0.021) Batch 0.064 (0.065) Remain 00:00:19 loss: 0.1352 data: 0.0060 Lr: 0.47403
2024-08-21 22:26:09.650 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][17/77] Data 0.027 (0.027) Batch 0.064 (0.065) Remain 00:00:18 loss: 0.1030 data: -0.0005 Lr: 0.47240
2024-08-21 22:26:09.650 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][17/77] Data 0.021 (0.021) Batch 0.064 (0.065) Remain 00:00:18 loss: 0.1030 data: -0.0153 Lr: 0.47240
2024-08-21 22:26:09.714 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][18/77] Data 0.027 (0.027) Batch 0.064 (0.065) Remain 00:00:18 loss: 0.1112 data: 0.0003 Lr: 0.47078
2024-08-21 22:26:09.714 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][18/77] Data 0.021 (0.021) Batch 0.064 (0.065) Remain 00:00:18 loss: 0.1112 data: -0.0132 Lr: 0.47078
2024-08-21 22:26:09.778 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][19/77] Data 0.027 (0.027) Batch 0.064 (0.065) Remain 00:00:18 loss: 0.1197 data: 0.0128 Lr: 0.46916
2024-08-21 22:26:09.778 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][19/77] Data 0.021 (0.021) Batch 0.064 (0.065) Remain 00:00:18 loss: 0.1197 data: 0.0028 Lr: 0.46916
2024-08-21 22:26:09.841 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][20/77] Data 0.027 (0.027) Batch 0.063 (0.065) Remain 00:00:18 loss: 0.1510 data: -0.0035 Lr: 0.46753
2024-08-21 22:26:09.841 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][20/77] Data 0.021 (0.021) Batch 0.063 (0.065) Remain 00:00:18 loss: 0.1510 data: -0.0049 Lr: 0.46753
2024-08-21 22:26:09.905 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][21/77] Data 0.027 (0.027) Batch 0.064 (0.065) Remain 00:00:18 loss: 0.1274 data: 0.0170 Lr: 0.46591
2024-08-21 22:26:09.905 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][21/77] Data 0.021 (0.021) Batch 0.064 (0.065) Remain 00:00:18 loss: 0.1274 data: 0.0059 Lr: 0.46591
2024-08-21 22:26:09.968 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][22/77] Data 0.027 (0.027) Batch 0.064 (0.065) Remain 00:00:18 loss: 0.2249 data: -0.0123 Lr: 0.46429
2024-08-21 22:26:09.968 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][22/77] Data 0.022 (0.021) Batch 0.064 (0.065) Remain 00:00:18 loss: 0.2249 data: -0.0073 Lr: 0.46429
2024-08-21 22:26:10.032 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][23/77] Data 0.027 (0.027) Batch 0.064 (0.065) Remain 00:00:18 loss: 0.1440 data: 0.0074 Lr: 0.46266
2024-08-21 22:26:10.032 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][23/77] Data 0.022 (0.021) Batch 0.064 (0.065) Remain 00:00:18 loss: 0.1440 data: -0.0111 Lr: 0.46266
2024-08-21 22:26:10.096 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][24/77] Data 0.027 (0.027) Batch 0.064 (0.065) Remain 00:00:18 loss: 0.1046 data: 0.0008 Lr: 0.46104
2024-08-21 22:26:10.096 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][24/77] Data 0.021 (0.021) Batch 0.064 (0.065) Remain 00:00:18 loss: 0.1046 data: 0.0007 Lr: 0.46104
2024-08-21 22:26:10.160 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][25/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:18 loss: 0.2121 data: 0.0049 Lr: 0.45942
2024-08-21 22:26:10.160 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][25/77] Data 0.021 (0.021) Batch 0.064 (0.064) Remain 00:00:18 loss: 0.2121 data: 0.0033 Lr: 0.45942
2024-08-21 22:26:10.223 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][26/77] Data 0.027 (0.027) Batch 0.063 (0.064) Remain 00:00:18 loss: 0.1102 data: 0.0082 Lr: 0.45779
2024-08-21 22:26:10.223 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][26/77] Data 0.022 (0.021) Batch 0.063 (0.064) Remain 00:00:18 loss: 0.1102 data: 0.0042 Lr: 0.45779
2024-08-21 22:26:10.286 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][27/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:18 loss: 0.1571 data: -0.0030 Lr: 0.45617
2024-08-21 22:26:10.286 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][27/77] Data 0.021 (0.021) Batch 0.064 (0.064) Remain 00:00:18 loss: 0.1571 data: 0.0168 Lr: 0.45617
2024-08-21 22:26:10.350 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][28/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:18 loss: 0.1478 data: 0.0003 Lr: 0.45455
2024-08-21 22:26:10.350 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][28/77] Data 0.021 (0.021) Batch 0.064 (0.064) Remain 00:00:18 loss: 0.1478 data: -0.0069 Lr: 0.45455
2024-08-21 22:26:10.414 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][29/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:18 loss: 0.1863 data: 0.0033 Lr: 0.45292
2024-08-21 22:26:10.414 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][29/77] Data 0.021 (0.021) Batch 0.064 (0.064) Remain 00:00:18 loss: 0.1863 data: 0.0206 Lr: 0.45292
2024-08-21 22:26:10.477 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][30/77] Data 0.021 (0.021) Batch 0.063 (0.064) Remain 00:00:17 loss: 0.1179 data: -0.0056 Lr: 0.45130
2024-08-21 22:26:10.477 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][30/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:17 loss: 0.1179 data: -0.0038 Lr: 0.45130
2024-08-21 22:26:10.544 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][31/77] Data 0.026 (0.027) Batch 0.066 (0.064) Remain 00:00:17 loss: 0.1313 data: -0.0020 Lr: 0.44968
2024-08-21 22:26:10.544 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][31/77] Data 0.021 (0.021) Batch 0.067 (0.064) Remain 00:00:17 loss: 0.1313 data: -0.0062 Lr: 0.44968
2024-08-21 22:26:10.609 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][32/77] Data 0.021 (0.021) Batch 0.065 (0.064) Remain 00:00:17 loss: 0.1416 data: -0.0094 Lr: 0.44805
2024-08-21 22:26:10.609 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][32/77] Data 0.027 (0.027) Batch 0.065 (0.064) Remain 00:00:17 loss: 0.1416 data: 0.0069 Lr: 0.44805
2024-08-21 22:26:10.681 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][33/77] Data 0.021 (0.021) Batch 0.072 (0.065) Remain 00:00:17 loss: 0.1455 data: -0.0007 Lr: 0.44643
2024-08-21 22:26:10.682 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][33/77] Data 0.032 (0.027) Batch 0.073 (0.065) Remain 00:00:17 loss: 0.1455 data: 0.0004 Lr: 0.44643
2024-08-21 22:26:10.748 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][34/77] Data 0.029 (0.027) Batch 0.067 (0.065) Remain 00:00:17 loss: 0.1329 data: -0.0012 Lr: 0.44481
2024-08-21 22:26:10.748 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][34/77] Data 0.021 (0.021) Batch 0.067 (0.065) Remain 00:00:17 loss: 0.1329 data: 0.0023 Lr: 0.44481
2024-08-21 22:26:10.812 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][35/77] Data 0.027 (0.027) Batch 0.064 (0.065) Remain 00:00:17 loss: 0.0972 data: -0.0101 Lr: 0.44318
2024-08-21 22:26:10.812 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][35/77] Data 0.022 (0.021) Batch 0.064 (0.065) Remain 00:00:17 loss: 0.0972 data: 0.0042 Lr: 0.44318
2024-08-21 22:26:10.881 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][36/77] Data 0.031 (0.027) Batch 0.068 (0.065) Remain 00:00:17 loss: 0.0755 data: -0.0069 Lr: 0.44156
2024-08-21 22:26:10.881 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][36/77] Data 0.023 (0.021) Batch 0.068 (0.065) Remain 00:00:17 loss: 0.0755 data: -0.0051 Lr: 0.44156
2024-08-21 22:26:10.952 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][37/77] Data 0.021 (0.021) Batch 0.071 (0.065) Remain 00:00:17 loss: 0.2141 data: -0.0017 Lr: 0.43994
2024-08-21 22:26:10.952 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][37/77] Data 0.027 (0.027) Batch 0.072 (0.065) Remain 00:00:17 loss: 0.2141 data: -0.0185 Lr: 0.43994
2024-08-21 22:26:11.026 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][38/77] Data 0.030 (0.028) Batch 0.074 (0.065) Remain 00:00:17 loss: 0.0798 data: -0.0115 Lr: 0.43831
2024-08-21 22:26:11.026 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][38/77] Data 0.021 (0.021) Batch 0.075 (0.065) Remain 00:00:17 loss: 0.0798 data: -0.0026 Lr: 0.43831
2024-08-21 22:26:11.092 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][39/77] Data 0.029 (0.028) Batch 0.065 (0.065) Remain 00:00:17 loss: 0.0781 data: 0.0002 Lr: 0.43669
2024-08-21 22:26:11.092 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][39/77] Data 0.021 (0.021) Batch 0.065 (0.065) Remain 00:00:17 loss: 0.0781 data: 0.0017 Lr: 0.43669
2024-08-21 22:26:11.156 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][40/77] Data 0.027 (0.028) Batch 0.064 (0.065) Remain 00:00:17 loss: 0.1188 data: 0.0048 Lr: 0.43506
2024-08-21 22:26:11.156 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][40/77] Data 0.022 (0.021) Batch 0.064 (0.065) Remain 00:00:17 loss: 0.1188 data: 0.0082 Lr: 0.43506
2024-08-21 22:26:11.220 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][41/77] Data 0.027 (0.028) Batch 0.064 (0.065) Remain 00:00:17 loss: 0.1566 data: -0.0055 Lr: 0.43344
2024-08-21 22:26:11.220 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][41/77] Data 0.022 (0.021) Batch 0.064 (0.065) Remain 00:00:17 loss: 0.1566 data: 0.0003 Lr: 0.43344
2024-08-21 22:26:11.284 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][42/77] Data 0.027 (0.028) Batch 0.064 (0.065) Remain 00:00:17 loss: 0.1835 data: -0.0115 Lr: 0.43182
2024-08-21 22:26:11.284 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][42/77] Data 0.022 (0.021) Batch 0.064 (0.065) Remain 00:00:17 loss: 0.1835 data: -0.0019 Lr: 0.43182
2024-08-21 22:26:11.284 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_350
2024-08-21 22:26:11.284 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_350
2024-08-21 22:26:11.306 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -551.7533569335938
2024-08-21 22:26:11.306 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -281.30084228515625
2024-08-21 22:26:11.306 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -551.7533569335938
2024-08-21 22:26:11.306 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -270.45257568359375
2024-08-21 22:26:11.371 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][43/77] Data 0.050 (0.028) Batch 0.087 (0.066) Remain 00:00:17 loss: 0.1377 data: 0.0048 Lr: 0.43019
2024-08-21 22:26:11.371 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][43/77] Data 0.043 (0.022) Batch 0.087 (0.066) Remain 00:00:17 loss: 0.1377 data: 0.0038 Lr: 0.43019
2024-08-21 22:26:11.436 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][44/77] Data 0.027 (0.028) Batch 0.065 (0.066) Remain 00:00:17 loss: 0.0799 data: -0.0127 Lr: 0.42857
2024-08-21 22:26:11.437 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][44/77] Data 0.022 (0.022) Batch 0.065 (0.066) Remain 00:00:17 loss: 0.0799 data: -0.0067 Lr: 0.42857
2024-08-21 22:26:11.501 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][45/77] Data 0.027 (0.028) Batch 0.065 (0.066) Remain 00:00:17 loss: 0.1538 data: 0.0013 Lr: 0.42695
2024-08-21 22:26:11.501 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][45/77] Data 0.023 (0.022) Batch 0.065 (0.066) Remain 00:00:17 loss: 0.1538 data: 0.0039 Lr: 0.42695
2024-08-21 22:26:11.565 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][46/77] Data 0.027 (0.028) Batch 0.064 (0.066) Remain 00:00:17 loss: 0.1058 data: -0.0181 Lr: 0.42532
2024-08-21 22:26:11.565 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][46/77] Data 0.021 (0.022) Batch 0.064 (0.066) Remain 00:00:17 loss: 0.1058 data: 0.0130 Lr: 0.42532
2024-08-21 22:26:11.636 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][47/77] Data 0.027 (0.028) Batch 0.071 (0.066) Remain 00:00:17 loss: 0.1383 data: -0.0068 Lr: 0.42370
2024-08-21 22:26:11.637 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][47/77] Data 0.022 (0.022) Batch 0.071 (0.066) Remain 00:00:17 loss: 0.1383 data: -0.0153 Lr: 0.42370
2024-08-21 22:26:11.720 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][48/77] Data 0.038 (0.028) Batch 0.084 (0.066) Remain 00:00:17 loss: 0.1230 data: -0.0043 Lr: 0.42208
2024-08-21 22:26:11.720 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][48/77] Data 0.037 (0.022) Batch 0.084 (0.066) Remain 00:00:17 loss: 0.1230 data: 0.0115 Lr: 0.42208
2024-08-21 22:26:11.794 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][49/77] Data 0.036 (0.028) Batch 0.074 (0.066) Remain 00:00:17 loss: 0.1189 data: -0.0033 Lr: 0.42045
2024-08-21 22:26:11.794 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][49/77] Data 0.021 (0.022) Batch 0.074 (0.066) Remain 00:00:17 loss: 0.1189 data: 0.0087 Lr: 0.42045
2024-08-21 22:26:11.860 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][50/77] Data 0.027 (0.028) Batch 0.066 (0.066) Remain 00:00:17 loss: 0.1229 data: 0.0039 Lr: 0.41883
2024-08-21 22:26:11.860 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][50/77] Data 0.020 (0.022) Batch 0.066 (0.066) Remain 00:00:17 loss: 0.1229 data: 0.0019 Lr: 0.41883
2024-08-21 22:26:11.924 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][51/77] Data 0.027 (0.028) Batch 0.064 (0.066) Remain 00:00:17 loss: 0.1353 data: 0.0043 Lr: 0.41721
2024-08-21 22:26:11.925 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][51/77] Data 0.021 (0.022) Batch 0.064 (0.066) Remain 00:00:17 loss: 0.1353 data: -0.0014 Lr: 0.41721
2024-08-21 22:26:11.989 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][52/77] Data 0.027 (0.028) Batch 0.064 (0.066) Remain 00:00:17 loss: 0.1119 data: 0.0039 Lr: 0.41558
2024-08-21 22:26:11.989 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][52/77] Data 0.020 (0.022) Batch 0.064 (0.066) Remain 00:00:17 loss: 0.1119 data: 0.0043 Lr: 0.41558
2024-08-21 22:26:12.053 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][53/77] Data 0.027 (0.028) Batch 0.064 (0.066) Remain 00:00:16 loss: 0.1621 data: 0.0042 Lr: 0.41396
2024-08-21 22:26:12.053 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][53/77] Data 0.022 (0.022) Batch 0.064 (0.066) Remain 00:00:16 loss: 0.1621 data: 0.0098 Lr: 0.41396
2024-08-21 22:26:12.121 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][54/77] Data 0.027 (0.028) Batch 0.068 (0.066) Remain 00:00:16 loss: 0.1062 data: 0.0206 Lr: 0.41234
2024-08-21 22:26:12.121 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][54/77] Data 0.021 (0.022) Batch 0.068 (0.066) Remain 00:00:16 loss: 0.1062 data: -0.0146 Lr: 0.41234
2024-08-21 22:26:12.202 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][55/77] Data 0.021 (0.022) Batch 0.080 (0.066) Remain 00:00:16 loss: 0.1143 data: 0.0108 Lr: 0.41071
2024-08-21 22:26:12.202 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][55/77] Data 0.036 (0.028) Batch 0.080 (0.066) Remain 00:00:16 loss: 0.1143 data: -0.0055 Lr: 0.41071
2024-08-21 22:26:12.286 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][56/77] Data 0.021 (0.022) Batch 0.085 (0.067) Remain 00:00:16 loss: 0.1117 data: -0.0007 Lr: 0.40909
2024-08-21 22:26:12.286 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][56/77] Data 0.036 (0.029) Batch 0.085 (0.067) Remain 00:00:16 loss: 0.1117 data: 0.0123 Lr: 0.40909
2024-08-21 22:26:12.355 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][57/77] Data 0.021 (0.022) Batch 0.069 (0.067) Remain 00:00:16 loss: 0.1887 data: 0.0060 Lr: 0.40747
2024-08-21 22:26:12.355 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][57/77] Data 0.030 (0.029) Batch 0.069 (0.067) Remain 00:00:16 loss: 0.1887 data: 0.0129 Lr: 0.40747
2024-08-21 22:26:12.429 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][58/77] Data 0.036 (0.029) Batch 0.073 (0.067) Remain 00:00:16 loss: 0.1176 data: 0.0061 Lr: 0.40584
2024-08-21 22:26:12.429 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][58/77] Data 0.021 (0.022) Batch 0.074 (0.067) Remain 00:00:16 loss: 0.1176 data: -0.0133 Lr: 0.40584
2024-08-21 22:26:12.493 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][59/77] Data 0.029 (0.029) Batch 0.065 (0.067) Remain 00:00:16 loss: 0.1418 data: 0.0014 Lr: 0.40422
2024-08-21 22:26:12.494 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][59/77] Data 0.021 (0.022) Batch 0.065 (0.067) Remain 00:00:16 loss: 0.1418 data: 0.0114 Lr: 0.40422
2024-08-21 22:26:12.557 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][60/77] Data 0.029 (0.029) Batch 0.064 (0.067) Remain 00:00:16 loss: 0.0772 data: -0.0059 Lr: 0.40260
2024-08-21 22:26:12.557 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][60/77] Data 0.021 (0.022) Batch 0.064 (0.067) Remain 00:00:16 loss: 0.0772 data: -0.0132 Lr: 0.40260
2024-08-21 22:26:12.620 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][61/77] Data 0.027 (0.029) Batch 0.063 (0.067) Remain 00:00:16 loss: 0.0972 data: -0.0065 Lr: 0.40097
2024-08-21 22:26:12.620 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][61/77] Data 0.021 (0.022) Batch 0.063 (0.067) Remain 00:00:16 loss: 0.0972 data: 0.0008 Lr: 0.40097
2024-08-21 22:26:12.684 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][62/77] Data 0.027 (0.029) Batch 0.064 (0.067) Remain 00:00:16 loss: 0.0720 data: -0.0120 Lr: 0.39935
2024-08-21 22:26:12.684 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][62/77] Data 0.021 (0.022) Batch 0.064 (0.067) Remain 00:00:16 loss: 0.0720 data: 0.0184 Lr: 0.39935
2024-08-21 22:26:12.746 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][63/77] Data 0.027 (0.029) Batch 0.062 (0.067) Remain 00:00:16 loss: 0.1101 data: 0.0034 Lr: 0.39773
2024-08-21 22:26:12.746 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][63/77] Data 0.021 (0.022) Batch 0.062 (0.067) Remain 00:00:16 loss: 0.1101 data: 0.0043 Lr: 0.39773
2024-08-21 22:26:12.810 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][64/77] Data 0.027 (0.029) Batch 0.065 (0.067) Remain 00:00:16 loss: 0.1110 data: -0.0036 Lr: 0.39610
2024-08-21 22:26:12.811 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][64/77] Data 0.022 (0.022) Batch 0.065 (0.067) Remain 00:00:16 loss: 0.1110 data: 0.0031 Lr: 0.39610
2024-08-21 22:26:12.884 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][65/77] Data 0.022 (0.022) Batch 0.073 (0.067) Remain 00:00:16 loss: 0.1256 data: 0.0020 Lr: 0.39448
2024-08-21 22:26:12.884 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][65/77] Data 0.033 (0.029) Batch 0.074 (0.067) Remain 00:00:16 loss: 0.1256 data: 0.0011 Lr: 0.39448
2024-08-21 22:26:12.959 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][66/77] Data 0.021 (0.022) Batch 0.075 (0.067) Remain 00:00:16 loss: 0.1137 data: 0.0113 Lr: 0.39286
2024-08-21 22:26:12.959 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][66/77] Data 0.034 (0.029) Batch 0.075 (0.067) Remain 00:00:16 loss: 0.1137 data: -0.0039 Lr: 0.39286
2024-08-21 22:26:13.037 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][67/77] Data 0.021 (0.022) Batch 0.078 (0.067) Remain 00:00:16 loss: 0.1086 data: -0.0025 Lr: 0.39123
2024-08-21 22:26:13.037 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][67/77] Data 0.033 (0.029) Batch 0.078 (0.067) Remain 00:00:16 loss: 0.1086 data: 0.0163 Lr: 0.39123
2024-08-21 22:26:13.108 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][68/77] Data 0.030 (0.029) Batch 0.071 (0.067) Remain 00:00:16 loss: 0.0771 data: -0.0100 Lr: 0.38961
2024-08-21 22:26:13.108 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][68/77] Data 0.021 (0.022) Batch 0.071 (0.067) Remain 00:00:16 loss: 0.0771 data: 0.0062 Lr: 0.38961
2024-08-21 22:26:13.189 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][69/77] Data 0.021 (0.022) Batch 0.081 (0.067) Remain 00:00:16 loss: 0.1107 data: -0.0260 Lr: 0.38799
2024-08-21 22:26:13.189 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][69/77] Data 0.034 (0.029) Batch 0.081 (0.067) Remain 00:00:16 loss: 0.1107 data: -0.0002 Lr: 0.38799
2024-08-21 22:26:13.268 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][70/77] Data 0.034 (0.029) Batch 0.079 (0.067) Remain 00:00:16 loss: 0.1000 data: -0.0036 Lr: 0.38636
2024-08-21 22:26:13.268 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][70/77] Data 0.021 (0.022) Batch 0.079 (0.067) Remain 00:00:16 loss: 0.1000 data: -0.0002 Lr: 0.38636
2024-08-21 22:26:13.376 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][71/77] Data 0.039 (0.029) Batch 0.108 (0.068) Remain 00:00:16 loss: 0.1291 data: 0.0014 Lr: 0.38474
2024-08-21 22:26:13.376 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][71/77] Data 0.021 (0.022) Batch 0.108 (0.068) Remain 00:00:16 loss: 0.1291 data: 0.0028 Lr: 0.38474
2024-08-21 22:26:13.442 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][72/77] Data 0.027 (0.029) Batch 0.066 (0.068) Remain 00:00:16 loss: 0.1064 data: 0.0001 Lr: 0.38312
2024-08-21 22:26:13.442 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][72/77] Data 0.027 (0.022) Batch 0.066 (0.068) Remain 00:00:16 loss: 0.1064 data: 0.0056 Lr: 0.38312
2024-08-21 22:26:13.505 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][73/77] Data 0.027 (0.029) Batch 0.063 (0.068) Remain 00:00:16 loss: 0.1287 data: -0.0077 Lr: 0.38149
2024-08-21 22:26:13.505 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][73/77] Data 0.026 (0.022) Batch 0.064 (0.068) Remain 00:00:16 loss: 0.1287 data: -0.0081 Lr: 0.38149
2024-08-21 22:26:13.574 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][74/77] Data 0.027 (0.029) Batch 0.069 (0.068) Remain 00:00:15 loss: 0.1416 data: 0.0020 Lr: 0.37987
2024-08-21 22:26:13.574 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][74/77] Data 0.028 (0.022) Batch 0.069 (0.068) Remain 00:00:15 loss: 0.1416 data: -0.0110 Lr: 0.37987
2024-08-21 22:26:13.642 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][75/77] Data 0.027 (0.029) Batch 0.068 (0.068) Remain 00:00:15 loss: 0.1375 data: 0.0091 Lr: 0.37825
2024-08-21 22:26:13.642 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][75/77] Data 0.027 (0.022) Batch 0.068 (0.068) Remain 00:00:15 loss: 0.1375 data: 0.0016 Lr: 0.37825
2024-08-21 22:26:13.708 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][76/77] Data 0.027 (0.029) Batch 0.066 (0.068) Remain 00:00:15 loss: 0.1171 data: 0.0090 Lr: 0.37662
2024-08-21 22:26:13.708 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][76/77] Data 0.028 (0.022) Batch 0.066 (0.068) Remain 00:00:15 loss: 0.1171 data: -0.0059 Lr: 0.37662
2024-08-21 22:26:13.747 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][77/77] Data 0.029 (0.029) Batch 0.039 (0.068) Remain 00:00:15 loss: 0.0939 data: 0.0059 Lr: 0.37500
2024-08-21 22:26:13.748 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 22:26:13.748 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][77/77] Data 0.024 (0.022) Batch 0.039 (0.068) Remain 00:00:15 loss: 0.0939 data: -0.0222 Lr: 0.37500
2024-08-21 22:26:13.748 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 22:26:17.678 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0348, Accuracy: 0.9882
2024-08-21 22:26:17.678 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 22:26:17.678 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 22:26:17.678 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0348, Accuracy: 0.9882
2024-08-21 22:26:17.678 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 22:26:17.679 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 22:26:17.774 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][1/77] Data 0.043 (0.043) Batch 0.096 (0.096) Remain 00:00:22 loss: 0.0726 data: -0.0048 Lr: 0.37338
2024-08-21 22:26:17.774 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][1/77] Data 0.057 (0.057) Batch 0.095 (0.095) Remain 00:00:21 loss: 0.0726 data: 0.0207 Lr: 0.37338
2024-08-21 22:26:17.840 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][2/77] Data 0.022 (0.022) Batch 0.066 (0.066) Remain 00:00:15 loss: 0.0917 data: 0.0003 Lr: 0.37175
2024-08-21 22:26:17.840 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][2/77] Data 0.027 (0.027) Batch 0.066 (0.066) Remain 00:00:15 loss: 0.0917 data: 0.0018 Lr: 0.37175
2024-08-21 22:26:17.906 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][3/77] Data 0.022 (0.022) Batch 0.065 (0.066) Remain 00:00:15 loss: 0.1028 data: -0.0084 Lr: 0.37013
2024-08-21 22:26:17.906 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][3/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:15 loss: 0.1028 data: -0.0102 Lr: 0.37013
2024-08-21 22:26:17.973 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][4/77] Data 0.028 (0.024) Batch 0.068 (0.066) Remain 00:00:15 loss: 0.0803 data: 0.0040 Lr: 0.36851
2024-08-21 22:26:17.974 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][4/77] Data 0.027 (0.027) Batch 0.068 (0.066) Remain 00:00:15 loss: 0.0803 data: 0.0095 Lr: 0.36851
2024-08-21 22:26:18.042 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][5/77] Data 0.027 (0.025) Batch 0.068 (0.067) Remain 00:00:15 loss: 0.0631 data: 0.0098 Lr: 0.36688
2024-08-21 22:26:18.042 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][5/77] Data 0.029 (0.028) Batch 0.068 (0.067) Remain 00:00:15 loss: 0.0631 data: 0.0200 Lr: 0.36688
2024-08-21 22:26:18.111 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][6/77] Data 0.029 (0.025) Batch 0.069 (0.067) Remain 00:00:15 loss: 0.1153 data: -0.0010 Lr: 0.36526
2024-08-21 22:26:18.111 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][6/77] Data 0.028 (0.028) Batch 0.069 (0.067) Remain 00:00:15 loss: 0.1153 data: -0.0033 Lr: 0.36526
2024-08-21 22:26:18.178 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][7/77] Data 0.030 (0.026) Batch 0.067 (0.067) Remain 00:00:15 loss: 0.0929 data: -0.0009 Lr: 0.36364
2024-08-21 22:26:18.178 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][7/77] Data 0.027 (0.028) Batch 0.067 (0.067) Remain 00:00:15 loss: 0.0929 data: -0.0046 Lr: 0.36364
2024-08-21 22:26:18.246 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][8/77] Data 0.029 (0.027) Batch 0.068 (0.067) Remain 00:00:15 loss: 0.1019 data: 0.0052 Lr: 0.36201
2024-08-21 22:26:18.246 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][8/77] Data 0.028 (0.028) Batch 0.068 (0.067) Remain 00:00:15 loss: 0.1019 data: -0.0108 Lr: 0.36201
2024-08-21 22:26:18.323 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][9/77] Data 0.035 (0.028) Batch 0.077 (0.069) Remain 00:00:15 loss: 0.1078 data: -0.0215 Lr: 0.36039
2024-08-21 22:26:18.323 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][9/77] Data 0.027 (0.028) Batch 0.077 (0.069) Remain 00:00:15 loss: 0.1078 data: -0.0174 Lr: 0.36039
2024-08-21 22:26:18.400 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][10/77] Data 0.037 (0.029) Batch 0.077 (0.070) Remain 00:00:15 loss: 0.1112 data: -0.0005 Lr: 0.35877
2024-08-21 22:26:18.400 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][10/77] Data 0.027 (0.028) Batch 0.077 (0.070) Remain 00:00:15 loss: 0.1112 data: 0.0161 Lr: 0.35877
2024-08-21 22:26:18.475 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][11/77] Data 0.027 (0.028) Batch 0.075 (0.070) Remain 00:00:15 loss: 0.1441 data: 0.0047 Lr: 0.35714
2024-08-21 22:26:18.475 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][11/77] Data 0.027 (0.028) Batch 0.075 (0.070) Remain 00:00:15 loss: 0.1441 data: -0.0004 Lr: 0.35714
2024-08-21 22:26:18.554 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][12/77] Data 0.037 (0.029) Batch 0.079 (0.071) Remain 00:00:15 loss: 0.1762 data: 0.0077 Lr: 0.35552
2024-08-21 22:26:18.555 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][12/77] Data 0.027 (0.028) Batch 0.079 (0.071) Remain 00:00:15 loss: 0.1762 data: -0.0003 Lr: 0.35552
2024-08-21 22:26:18.619 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][13/77] Data 0.027 (0.029) Batch 0.064 (0.070) Remain 00:00:15 loss: 0.1311 data: -0.0017 Lr: 0.35390
2024-08-21 22:26:18.619 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][13/77] Data 0.027 (0.028) Batch 0.064 (0.070) Remain 00:00:15 loss: 0.1311 data: -0.0061 Lr: 0.35390
2024-08-21 22:26:18.684 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][14/77] Data 0.027 (0.029) Batch 0.065 (0.070) Remain 00:00:15 loss: 0.1111 data: -0.0142 Lr: 0.35227
2024-08-21 22:26:18.684 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][14/77] Data 0.027 (0.028) Batch 0.065 (0.070) Remain 00:00:15 loss: 0.1111 data: 0.0031 Lr: 0.35227
2024-08-21 22:26:18.749 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][15/77] Data 0.027 (0.029) Batch 0.065 (0.070) Remain 00:00:15 loss: 0.0688 data: 0.0090 Lr: 0.35065
2024-08-21 22:26:18.749 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_400
2024-08-21 22:26:18.749 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][15/77] Data 0.027 (0.028) Batch 0.065 (0.070) Remain 00:00:15 loss: 0.0688 data: 0.0112 Lr: 0.35065
2024-08-21 22:26:18.749 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_400
2024-08-21 22:26:18.769 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -563.8003540039062
2024-08-21 22:26:18.770 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -274.00146484375
2024-08-21 22:26:18.770 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -563.8003540039062
2024-08-21 22:26:18.770 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -289.7989196777344
2024-08-21 22:26:18.836 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][16/77] Data 0.048 (0.030) Batch 0.088 (0.071) Remain 00:00:15 loss: 0.0731 data: -0.0122 Lr: 0.34903
2024-08-21 22:26:18.836 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][16/77] Data 0.049 (0.029) Batch 0.088 (0.071) Remain 00:00:15 loss: 0.0731 data: -0.0009 Lr: 0.34903
2024-08-21 22:26:18.901 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][17/77] Data 0.027 (0.030) Batch 0.065 (0.070) Remain 00:00:15 loss: 0.0728 data: 0.0150 Lr: 0.34740
2024-08-21 22:26:18.901 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][17/77] Data 0.027 (0.029) Batch 0.065 (0.070) Remain 00:00:15 loss: 0.0728 data: 0.0024 Lr: 0.34740
2024-08-21 22:26:18.966 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][18/77] Data 0.027 (0.030) Batch 0.065 (0.070) Remain 00:00:15 loss: 0.1243 data: -0.0023 Lr: 0.34578
2024-08-21 22:26:18.967 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][18/77] Data 0.027 (0.029) Batch 0.065 (0.070) Remain 00:00:15 loss: 0.1243 data: 0.0074 Lr: 0.34578
2024-08-21 22:26:19.031 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][19/77] Data 0.027 (0.030) Batch 0.065 (0.070) Remain 00:00:14 loss: 0.1778 data: -0.0049 Lr: 0.34416
2024-08-21 22:26:19.031 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][19/77] Data 0.027 (0.029) Batch 0.065 (0.070) Remain 00:00:14 loss: 0.1778 data: 0.0019 Lr: 0.34416
2024-08-21 22:26:19.097 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][20/77] Data 0.027 (0.029) Batch 0.065 (0.070) Remain 00:00:14 loss: 0.0841 data: 0.0180 Lr: 0.34253
2024-08-21 22:26:19.097 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][20/77] Data 0.027 (0.029) Batch 0.065 (0.070) Remain 00:00:14 loss: 0.0841 data: 0.0016 Lr: 0.34253
2024-08-21 22:26:19.163 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][21/77] Data 0.027 (0.029) Batch 0.066 (0.069) Remain 00:00:14 loss: 0.0661 data: 0.0004 Lr: 0.34091
2024-08-21 22:26:19.163 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][21/77] Data 0.027 (0.029) Batch 0.066 (0.069) Remain 00:00:14 loss: 0.0661 data: -0.0171 Lr: 0.34091
2024-08-21 22:26:19.229 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][22/77] Data 0.027 (0.029) Batch 0.067 (0.069) Remain 00:00:14 loss: 0.1302 data: 0.0093 Lr: 0.33929
2024-08-21 22:26:19.229 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][22/77] Data 0.028 (0.028) Batch 0.067 (0.069) Remain 00:00:14 loss: 0.1302 data: 0.0281 Lr: 0.33929
2024-08-21 22:26:19.296 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][23/77] Data 0.027 (0.029) Batch 0.066 (0.069) Remain 00:00:14 loss: 0.1229 data: -0.0026 Lr: 0.33766
2024-08-21 22:26:19.296 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][23/77] Data 0.027 (0.028) Batch 0.066 (0.069) Remain 00:00:14 loss: 0.1229 data: -0.0006 Lr: 0.33766
2024-08-21 22:26:19.361 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][24/77] Data 0.027 (0.029) Batch 0.066 (0.069) Remain 00:00:14 loss: 0.1172 data: -0.0075 Lr: 0.33604
2024-08-21 22:26:19.361 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][24/77] Data 0.029 (0.028) Batch 0.066 (0.069) Remain 00:00:14 loss: 0.1172 data: -0.0042 Lr: 0.33604
2024-08-21 22:26:19.427 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][25/77] Data 0.027 (0.029) Batch 0.066 (0.069) Remain 00:00:14 loss: 0.1076 data: -0.0091 Lr: 0.33442
2024-08-21 22:26:19.427 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][25/77] Data 0.028 (0.028) Batch 0.066 (0.069) Remain 00:00:14 loss: 0.1076 data: -0.0039 Lr: 0.33442
2024-08-21 22:26:19.491 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][26/77] Data 0.027 (0.029) Batch 0.064 (0.069) Remain 00:00:14 loss: 0.1556 data: 0.0128 Lr: 0.33279
2024-08-21 22:26:19.492 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][26/77] Data 0.027 (0.028) Batch 0.064 (0.069) Remain 00:00:14 loss: 0.1556 data: 0.0125 Lr: 0.33279
2024-08-21 22:26:19.556 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][27/77] Data 0.027 (0.029) Batch 0.065 (0.069) Remain 00:00:14 loss: 0.0839 data: 0.0105 Lr: 0.33117
2024-08-21 22:26:19.556 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][27/77] Data 0.027 (0.028) Batch 0.065 (0.069) Remain 00:00:14 loss: 0.0839 data: -0.0005 Lr: 0.33117
2024-08-21 22:26:19.621 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][28/77] Data 0.027 (0.029) Batch 0.065 (0.068) Remain 00:00:13 loss: 0.1242 data: 0.0011 Lr: 0.32955
2024-08-21 22:26:19.621 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][28/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:13 loss: 0.1242 data: 0.0065 Lr: 0.32955
2024-08-21 22:26:19.687 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][29/77] Data 0.027 (0.029) Batch 0.065 (0.068) Remain 00:00:13 loss: 0.0676 data: 0.0052 Lr: 0.32792
2024-08-21 22:26:19.687 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][29/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:13 loss: 0.0676 data: 0.0013 Lr: 0.32792
2024-08-21 22:26:19.754 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][30/77] Data 0.028 (0.029) Batch 0.068 (0.068) Remain 00:00:13 loss: 0.0935 data: -0.0017 Lr: 0.32630
2024-08-21 22:26:19.754 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][30/77] Data 0.027 (0.028) Batch 0.068 (0.068) Remain 00:00:13 loss: 0.0935 data: 0.0014 Lr: 0.32630
2024-08-21 22:26:19.820 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][31/77] Data 0.027 (0.029) Batch 0.066 (0.068) Remain 00:00:13 loss: 0.0935 data: 0.0064 Lr: 0.32468
2024-08-21 22:26:19.821 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][31/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:13 loss: 0.0935 data: 0.0081 Lr: 0.32468
2024-08-21 22:26:19.889 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][32/77] Data 0.027 (0.029) Batch 0.068 (0.068) Remain 00:00:13 loss: 0.0904 data: 0.0188 Lr: 0.32305
2024-08-21 22:26:19.889 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][32/77] Data 0.028 (0.028) Batch 0.068 (0.068) Remain 00:00:13 loss: 0.0904 data: 0.0002 Lr: 0.32305
2024-08-21 22:26:19.955 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][33/77] Data 0.028 (0.028) Batch 0.066 (0.068) Remain 00:00:13 loss: 0.1054 data: 0.0007 Lr: 0.32143
2024-08-21 22:26:19.955 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][33/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:13 loss: 0.1054 data: 0.0149 Lr: 0.32143
2024-08-21 22:26:20.021 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][34/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:13 loss: 0.1034 data: -0.0118 Lr: 0.31981
2024-08-21 22:26:20.021 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][34/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:13 loss: 0.1034 data: 0.0067 Lr: 0.31981
2024-08-21 22:26:20.088 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][35/77] Data 0.028 (0.028) Batch 0.068 (0.068) Remain 00:00:13 loss: 0.0758 data: 0.0114 Lr: 0.31818
2024-08-21 22:26:20.088 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][35/77] Data 0.027 (0.028) Batch 0.068 (0.068) Remain 00:00:13 loss: 0.0758 data: -0.0016 Lr: 0.31818
2024-08-21 22:26:20.157 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][36/77] Data 0.028 (0.028) Batch 0.069 (0.068) Remain 00:00:13 loss: 0.0556 data: -0.0139 Lr: 0.31656
2024-08-21 22:26:20.158 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][36/77] Data 0.027 (0.028) Batch 0.069 (0.068) Remain 00:00:13 loss: 0.0556 data: 0.0071 Lr: 0.31656
2024-08-21 22:26:20.227 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][37/77] Data 0.029 (0.028) Batch 0.069 (0.068) Remain 00:00:13 loss: 0.0365 data: 0.0010 Lr: 0.31494
2024-08-21 22:26:20.227 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][37/77] Data 0.028 (0.028) Batch 0.069 (0.068) Remain 00:00:13 loss: 0.0365 data: 0.0075 Lr: 0.31494
2024-08-21 22:26:20.293 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][38/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:13 loss: 0.0748 data: -0.0011 Lr: 0.31331
2024-08-21 22:26:20.293 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][38/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:13 loss: 0.0748 data: -0.0155 Lr: 0.31331
2024-08-21 22:26:20.364 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][39/77] Data 0.028 (0.028) Batch 0.071 (0.068) Remain 00:00:13 loss: 0.1201 data: -0.0121 Lr: 0.31169
2024-08-21 22:26:20.364 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][39/77] Data 0.028 (0.028) Batch 0.072 (0.068) Remain 00:00:13 loss: 0.1201 data: 0.0119 Lr: 0.31169
2024-08-21 22:26:20.436 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][40/77] Data 0.029 (0.028) Batch 0.072 (0.068) Remain 00:00:13 loss: 0.1331 data: -0.0064 Lr: 0.31006
2024-08-21 22:26:20.436 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][40/77] Data 0.029 (0.028) Batch 0.072 (0.068) Remain 00:00:13 loss: 0.1331 data: 0.0154 Lr: 0.31006
2024-08-21 22:26:20.507 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][41/77] Data 0.028 (0.028) Batch 0.071 (0.068) Remain 00:00:13 loss: 0.1245 data: 0.0134 Lr: 0.30844
2024-08-21 22:26:20.507 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][41/77] Data 0.028 (0.028) Batch 0.071 (0.068) Remain 00:00:13 loss: 0.1245 data: -0.0015 Lr: 0.30844
2024-08-21 22:26:20.574 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][42/77] Data 0.028 (0.028) Batch 0.068 (0.068) Remain 00:00:12 loss: 0.0729 data: 0.0074 Lr: 0.30682
2024-08-21 22:26:20.575 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][42/77] Data 0.028 (0.028) Batch 0.068 (0.068) Remain 00:00:12 loss: 0.0729 data: 0.0047 Lr: 0.30682
2024-08-21 22:26:20.640 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][43/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:12 loss: 0.0943 data: 0.0085 Lr: 0.30519
2024-08-21 22:26:20.640 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][43/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:12 loss: 0.0943 data: -0.0006 Lr: 0.30519
2024-08-21 22:26:20.705 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][44/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:12 loss: 0.1219 data: 0.0050 Lr: 0.30357
2024-08-21 22:26:20.705 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][44/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:12 loss: 0.1219 data: 0.0176 Lr: 0.30357
2024-08-21 22:26:20.776 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][45/77] Data 0.027 (0.028) Batch 0.070 (0.068) Remain 00:00:12 loss: 0.0678 data: 0.0038 Lr: 0.30195
2024-08-21 22:26:20.776 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][45/77] Data 0.027 (0.028) Batch 0.070 (0.068) Remain 00:00:12 loss: 0.0678 data: 0.0177 Lr: 0.30195
2024-08-21 22:26:20.847 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][46/77] Data 0.027 (0.028) Batch 0.071 (0.068) Remain 00:00:12 loss: 0.1162 data: -0.0016 Lr: 0.30032
2024-08-21 22:26:20.847 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][46/77] Data 0.027 (0.028) Batch 0.071 (0.068) Remain 00:00:12 loss: 0.1162 data: -0.0026 Lr: 0.30032
2024-08-21 22:26:20.918 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][47/77] Data 0.027 (0.028) Batch 0.071 (0.068) Remain 00:00:12 loss: 0.1479 data: -0.0064 Lr: 0.29870
2024-08-21 22:26:20.918 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][47/77] Data 0.035 (0.028) Batch 0.071 (0.068) Remain 00:00:12 loss: 0.1479 data: 0.0036 Lr: 0.29870
2024-08-21 22:26:20.983 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][48/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:12 loss: 0.0460 data: -0.0077 Lr: 0.29708
2024-08-21 22:26:20.983 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][48/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:12 loss: 0.0460 data: -0.0030 Lr: 0.29708
2024-08-21 22:26:21.048 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][49/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:12 loss: 0.0727 data: 0.0010 Lr: 0.29545
2024-08-21 22:26:21.048 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][49/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:12 loss: 0.0727 data: -0.0064 Lr: 0.29545
2024-08-21 22:26:21.115 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][50/77] Data 0.027 (0.028) Batch 0.067 (0.068) Remain 00:00:12 loss: 0.1121 data: 0.0031 Lr: 0.29383
2024-08-21 22:26:21.115 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][50/77] Data 0.029 (0.028) Batch 0.067 (0.068) Remain 00:00:12 loss: 0.1121 data: -0.0073 Lr: 0.29383
2024-08-21 22:26:21.179 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][51/77] Data 0.027 (0.028) Batch 0.064 (0.068) Remain 00:00:12 loss: 0.1700 data: 0.0146 Lr: 0.29221
2024-08-21 22:26:21.179 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][51/77] Data 0.027 (0.028) Batch 0.064 (0.068) Remain 00:00:12 loss: 0.1700 data: 0.0001 Lr: 0.29221
2024-08-21 22:26:21.250 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][52/77] Data 0.029 (0.028) Batch 0.071 (0.068) Remain 00:00:12 loss: 0.1332 data: -0.0058 Lr: 0.29058
2024-08-21 22:26:21.251 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][52/77] Data 0.029 (0.028) Batch 0.071 (0.068) Remain 00:00:12 loss: 0.1332 data: -0.0161 Lr: 0.29058
2024-08-21 22:26:21.315 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][53/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:12 loss: 0.1928 data: -0.0125 Lr: 0.28896
2024-08-21 22:26:21.315 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][53/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:12 loss: 0.1928 data: 0.0044 Lr: 0.28896
2024-08-21 22:26:21.379 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][54/77] Data 0.027 (0.028) Batch 0.064 (0.068) Remain 00:00:12 loss: 0.0541 data: -0.0028 Lr: 0.28734
2024-08-21 22:26:21.379 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][54/77] Data 0.027 (0.028) Batch 0.064 (0.068) Remain 00:00:12 loss: 0.0541 data: 0.0090 Lr: 0.28734
2024-08-21 22:26:21.443 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][55/77] Data 0.027 (0.028) Batch 0.064 (0.068) Remain 00:00:12 loss: 0.1133 data: -0.0214 Lr: 0.28571
2024-08-21 22:26:21.443 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][55/77] Data 0.027 (0.028) Batch 0.064 (0.068) Remain 00:00:12 loss: 0.1133 data: -0.0014 Lr: 0.28571
2024-08-21 22:26:21.527 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][56/77] Data 0.027 (0.028) Batch 0.084 (0.068) Remain 00:00:12 loss: 0.0480 data: -0.0066 Lr: 0.28409
2024-08-21 22:26:21.527 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][56/77] Data 0.027 (0.028) Batch 0.084 (0.068) Remain 00:00:12 loss: 0.0480 data: -0.0109 Lr: 0.28409
2024-08-21 22:26:21.620 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][57/77] Data 0.046 (0.028) Batch 0.093 (0.069) Remain 00:00:12 loss: 0.1022 data: -0.0083 Lr: 0.28247
2024-08-21 22:26:21.620 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][57/77] Data 0.046 (0.028) Batch 0.093 (0.069) Remain 00:00:12 loss: 0.1022 data: -0.0063 Lr: 0.28247
2024-08-21 22:26:21.689 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][58/77] Data 0.027 (0.028) Batch 0.069 (0.069) Remain 00:00:11 loss: 0.0926 data: -0.0136 Lr: 0.28084
2024-08-21 22:26:21.690 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][58/77] Data 0.027 (0.028) Batch 0.069 (0.069) Remain 00:00:11 loss: 0.0926 data: -0.0010 Lr: 0.28084
2024-08-21 22:26:21.755 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][59/77] Data 0.027 (0.028) Batch 0.066 (0.069) Remain 00:00:11 loss: 0.1126 data: -0.0025 Lr: 0.27922
2024-08-21 22:26:21.755 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][59/77] Data 0.027 (0.028) Batch 0.066 (0.069) Remain 00:00:11 loss: 0.1126 data: 0.0090 Lr: 0.27922
2024-08-21 22:26:21.821 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][60/77] Data 0.027 (0.028) Batch 0.065 (0.069) Remain 00:00:11 loss: 0.0875 data: -0.0015 Lr: 0.27760
2024-08-21 22:26:21.821 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][60/77] Data 0.027 (0.028) Batch 0.065 (0.069) Remain 00:00:11 loss: 0.0875 data: -0.0037 Lr: 0.27760
2024-08-21 22:26:21.886 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][61/77] Data 0.027 (0.028) Batch 0.065 (0.069) Remain 00:00:11 loss: 0.0755 data: 0.0039 Lr: 0.27597
2024-08-21 22:26:21.886 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][61/77] Data 0.027 (0.028) Batch 0.065 (0.069) Remain 00:00:11 loss: 0.0755 data: -0.0114 Lr: 0.27597
2024-08-21 22:26:21.958 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][62/77] Data 0.027 (0.028) Batch 0.073 (0.069) Remain 00:00:11 loss: 0.0753 data: -0.0047 Lr: 0.27435
2024-08-21 22:26:21.959 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][62/77] Data 0.027 (0.028) Batch 0.073 (0.069) Remain 00:00:11 loss: 0.0753 data: -0.0136 Lr: 0.27435
2024-08-21 22:26:22.028 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][63/77] Data 0.028 (0.028) Batch 0.070 (0.069) Remain 00:00:11 loss: 0.1045 data: 0.0097 Lr: 0.27273
2024-08-21 22:26:22.028 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][63/77] Data 0.028 (0.028) Batch 0.070 (0.069) Remain 00:00:11 loss: 0.1045 data: -0.0039 Lr: 0.27273
2024-08-21 22:26:22.096 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][64/77] Data 0.028 (0.028) Batch 0.068 (0.069) Remain 00:00:11 loss: 0.0710 data: 0.0087 Lr: 0.27110
2024-08-21 22:26:22.096 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][64/77] Data 0.028 (0.028) Batch 0.068 (0.069) Remain 00:00:11 loss: 0.0710 data: -0.0002 Lr: 0.27110
2024-08-21 22:26:22.161 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][65/77] Data 0.028 (0.028) Batch 0.065 (0.069) Remain 00:00:11 loss: 0.1363 data: 0.0106 Lr: 0.26948
2024-08-21 22:26:22.161 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_450
2024-08-21 22:26:22.161 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][65/77] Data 0.027 (0.028) Batch 0.065 (0.069) Remain 00:00:11 loss: 0.1363 data: 0.0039 Lr: 0.26948
2024-08-21 22:26:22.161 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_450
2024-08-21 22:26:22.181 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -569.8925170898438
2024-08-21 22:26:22.181 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -569.8925170898438
2024-08-21 22:26:22.181 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -270.25
2024-08-21 22:26:22.181 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -299.6426086425781
2024-08-21 22:26:22.249 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][66/77] Data 0.048 (0.029) Batch 0.088 (0.069) Remain 00:00:11 loss: 0.1094 data: -0.0167 Lr: 0.26786
2024-08-21 22:26:22.249 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][66/77] Data 0.048 (0.029) Batch 0.088 (0.069) Remain 00:00:11 loss: 0.1094 data: -0.0019 Lr: 0.26786
2024-08-21 22:26:22.316 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][67/77] Data 0.027 (0.029) Batch 0.067 (0.069) Remain 00:00:11 loss: 0.0490 data: -0.0091 Lr: 0.26623
2024-08-21 22:26:22.316 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][67/77] Data 0.028 (0.028) Batch 0.067 (0.069) Remain 00:00:11 loss: 0.0490 data: -0.0088 Lr: 0.26623
2024-08-21 22:26:22.382 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][68/77] Data 0.027 (0.029) Batch 0.066 (0.069) Remain 00:00:11 loss: 0.1425 data: 0.0082 Lr: 0.26461
2024-08-21 22:26:22.382 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][68/77] Data 0.027 (0.028) Batch 0.066 (0.069) Remain 00:00:11 loss: 0.1425 data: -0.0022 Lr: 0.26461
2024-08-21 22:26:22.451 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][69/77] Data 0.028 (0.029) Batch 0.069 (0.069) Remain 00:00:11 loss: 0.1114 data: -0.0219 Lr: 0.26299
2024-08-21 22:26:22.451 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][69/77] Data 0.028 (0.028) Batch 0.069 (0.069) Remain 00:00:11 loss: 0.1114 data: 0.0064 Lr: 0.26299
2024-08-21 22:26:22.517 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][70/77] Data 0.027 (0.028) Batch 0.066 (0.069) Remain 00:00:11 loss: 0.0843 data: -0.0131 Lr: 0.26136
2024-08-21 22:26:22.517 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][70/77] Data 0.027 (0.028) Batch 0.066 (0.069) Remain 00:00:11 loss: 0.0843 data: -0.0016 Lr: 0.26136
2024-08-21 22:26:22.582 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][71/77] Data 0.027 (0.028) Batch 0.065 (0.069) Remain 00:00:11 loss: 0.0778 data: -0.0031 Lr: 0.25974
2024-08-21 22:26:22.582 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][71/77] Data 0.027 (0.028) Batch 0.065 (0.069) Remain 00:00:11 loss: 0.0778 data: 0.0018 Lr: 0.25974
2024-08-21 22:26:22.648 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][72/77] Data 0.027 (0.028) Batch 0.066 (0.069) Remain 00:00:10 loss: 0.0869 data: 0.0094 Lr: 0.25812
2024-08-21 22:26:22.648 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][72/77] Data 0.027 (0.028) Batch 0.066 (0.069) Remain 00:00:10 loss: 0.0869 data: 0.0098 Lr: 0.25812
2024-08-21 22:26:22.714 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][73/77] Data 0.027 (0.028) Batch 0.066 (0.069) Remain 00:00:10 loss: 0.1049 data: 0.0064 Lr: 0.25649
2024-08-21 22:26:22.714 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][73/77] Data 0.027 (0.028) Batch 0.066 (0.069) Remain 00:00:10 loss: 0.1049 data: 0.0076 Lr: 0.25649
2024-08-21 22:26:22.780 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][74/77] Data 0.027 (0.028) Batch 0.066 (0.069) Remain 00:00:10 loss: 0.0834 data: 0.0072 Lr: 0.25487
2024-08-21 22:26:22.780 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][74/77] Data 0.027 (0.028) Batch 0.066 (0.069) Remain 00:00:10 loss: 0.0834 data: -0.0042 Lr: 0.25487
2024-08-21 22:26:22.848 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][75/77] Data 0.027 (0.028) Batch 0.068 (0.069) Remain 00:00:10 loss: 0.0921 data: 0.0026 Lr: 0.25325
2024-08-21 22:26:22.848 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][75/77] Data 0.027 (0.028) Batch 0.068 (0.069) Remain 00:00:10 loss: 0.0921 data: -0.0094 Lr: 0.25325
2024-08-21 22:26:22.916 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][76/77] Data 0.030 (0.028) Batch 0.068 (0.069) Remain 00:00:10 loss: 0.0910 data: 0.0090 Lr: 0.25162
2024-08-21 22:26:22.916 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][76/77] Data 0.028 (0.028) Batch 0.069 (0.069) Remain 00:00:10 loss: 0.0910 data: -0.0027 Lr: 0.25162
2024-08-21 22:26:22.956 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][77/77] Data 0.029 (0.028) Batch 0.040 (0.068) Remain 00:00:10 loss: 0.1460 data: 0.0192 Lr: 0.25000
2024-08-21 22:26:22.957 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 22:26:22.956 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][77/77] Data 0.029 (0.028) Batch 0.040 (0.068) Remain 00:00:10 loss: 0.1460 data: -0.0109 Lr: 0.25000
2024-08-21 22:26:22.957 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 22:26:27.078 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0323, Accuracy: 0.9886
2024-08-21 22:26:27.078 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0323, Accuracy: 0.9886
2024-08-21 22:26:27.078 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 22:26:27.078 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 22:26:27.078 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 22:26:27.078 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 22:26:27.154 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][1/77] Data 0.045 (0.045) Batch 0.075 (0.075) Remain 00:00:11 loss: 0.1153 data: -0.0011 Lr: 0.24838
2024-08-21 22:26:27.154 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][1/77] Data 0.042 (0.042) Batch 0.075 (0.075) Remain 00:00:11 loss: 0.1153 data: 0.0001 Lr: 0.24838
2024-08-21 22:26:27.207 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][2/77] Data 0.022 (0.022) Batch 0.053 (0.053) Remain 00:00:08 loss: 0.1321 data: 0.0130 Lr: 0.24675
2024-08-21 22:26:27.207 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][2/77] Data 0.021 (0.021) Batch 0.053 (0.053) Remain 00:00:08 loss: 0.1321 data: 0.0024 Lr: 0.24675
2024-08-21 22:26:27.262 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][3/77] Data 0.022 (0.022) Batch 0.055 (0.054) Remain 00:00:08 loss: 0.0508 data: -0.0149 Lr: 0.24513
2024-08-21 22:26:27.262 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][3/77] Data 0.022 (0.022) Batch 0.055 (0.054) Remain 00:00:08 loss: 0.0508 data: -0.0094 Lr: 0.24513
2024-08-21 22:26:27.319 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][4/77] Data 0.024 (0.023) Batch 0.056 (0.055) Remain 00:00:08 loss: 0.1163 data: 0.0154 Lr: 0.24351
2024-08-21 22:26:27.319 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][4/77] Data 0.022 (0.022) Batch 0.057 (0.055) Remain 00:00:08 loss: 0.1163 data: 0.0173 Lr: 0.24351
2024-08-21 22:26:27.375 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][5/77] Data 0.022 (0.023) Batch 0.056 (0.055) Remain 00:00:08 loss: 0.0440 data: 0.0002 Lr: 0.24188
2024-08-21 22:26:27.375 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][5/77] Data 0.023 (0.022) Batch 0.056 (0.055) Remain 00:00:08 loss: 0.0440 data: 0.0112 Lr: 0.24188
2024-08-21 22:26:27.431 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][6/77] Data 0.022 (0.022) Batch 0.056 (0.055) Remain 00:00:08 loss: 0.0464 data: -0.0313 Lr: 0.24026
2024-08-21 22:26:27.431 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][6/77] Data 0.023 (0.022) Batch 0.056 (0.055) Remain 00:00:08 loss: 0.0464 data: -0.0011 Lr: 0.24026
2024-08-21 22:26:27.495 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][7/77] Data 0.023 (0.023) Batch 0.064 (0.057) Remain 00:00:08 loss: 0.1221 data: -0.0014 Lr: 0.23864
2024-08-21 22:26:27.495 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][7/77] Data 0.023 (0.022) Batch 0.064 (0.057) Remain 00:00:08 loss: 0.1221 data: 0.0004 Lr: 0.23864
2024-08-21 22:26:27.559 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][8/77] Data 0.027 (0.023) Batch 0.064 (0.058) Remain 00:00:08 loss: 0.0705 data: 0.0011 Lr: 0.23701
2024-08-21 22:26:27.559 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][8/77] Data 0.021 (0.022) Batch 0.064 (0.058) Remain 00:00:08 loss: 0.0705 data: -0.0148 Lr: 0.23701
2024-08-21 22:26:27.624 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][9/77] Data 0.027 (0.024) Batch 0.066 (0.059) Remain 00:00:08 loss: 0.1271 data: 0.0141 Lr: 0.23539
2024-08-21 22:26:27.624 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][9/77] Data 0.021 (0.022) Batch 0.065 (0.059) Remain 00:00:08 loss: 0.1271 data: 0.0075 Lr: 0.23539
2024-08-21 22:26:27.706 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][10/77] Data 0.022 (0.022) Batch 0.081 (0.061) Remain 00:00:08 loss: 0.1933 data: -0.0173 Lr: 0.23377
2024-08-21 22:26:27.705 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][10/77] Data 0.035 (0.025) Batch 0.081 (0.061) Remain 00:00:08 loss: 0.1933 data: 0.0063 Lr: 0.23377
2024-08-21 22:26:27.785 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][11/77] Data 0.034 (0.026) Batch 0.080 (0.063) Remain 00:00:09 loss: 0.0387 data: 0.0060 Lr: 0.23214
2024-08-21 22:26:27.786 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][11/77] Data 0.021 (0.022) Batch 0.080 (0.063) Remain 00:00:09 loss: 0.0387 data: 0.0108 Lr: 0.23214
2024-08-21 22:26:27.852 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][12/77] Data 0.028 (0.026) Batch 0.066 (0.063) Remain 00:00:09 loss: 0.0663 data: -0.0053 Lr: 0.23052
2024-08-21 22:26:27.852 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][12/77] Data 0.021 (0.022) Batch 0.066 (0.063) Remain 00:00:09 loss: 0.0663 data: 0.0054 Lr: 0.23052
2024-08-21 22:26:27.917 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][13/77] Data 0.027 (0.026) Batch 0.065 (0.064) Remain 00:00:09 loss: 0.0642 data: -0.0085 Lr: 0.22890
2024-08-21 22:26:27.917 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][13/77] Data 0.022 (0.022) Batch 0.065 (0.064) Remain 00:00:09 loss: 0.0642 data: 0.0079 Lr: 0.22890
2024-08-21 22:26:27.984 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][14/77] Data 0.027 (0.026) Batch 0.066 (0.064) Remain 00:00:08 loss: 0.1010 data: 0.0011 Lr: 0.22727
2024-08-21 22:26:27.984 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][14/77] Data 0.021 (0.022) Batch 0.066 (0.064) Remain 00:00:08 loss: 0.1010 data: 0.0143 Lr: 0.22727
2024-08-21 22:26:28.057 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][15/77] Data 0.035 (0.027) Batch 0.073 (0.065) Remain 00:00:09 loss: 0.0620 data: -0.0009 Lr: 0.22565
2024-08-21 22:26:28.057 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][15/77] Data 0.021 (0.022) Batch 0.073 (0.065) Remain 00:00:09 loss: 0.0620 data: 0.0073 Lr: 0.22565
2024-08-21 22:26:28.123 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][16/77] Data 0.027 (0.027) Batch 0.066 (0.065) Remain 00:00:08 loss: 0.1323 data: 0.0100 Lr: 0.22403
2024-08-21 22:26:28.123 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][16/77] Data 0.021 (0.022) Batch 0.066 (0.065) Remain 00:00:08 loss: 0.1323 data: -0.0112 Lr: 0.22403
2024-08-21 22:26:28.188 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][17/77] Data 0.028 (0.027) Batch 0.066 (0.065) Remain 00:00:08 loss: 0.0604 data: -0.0082 Lr: 0.22240
2024-08-21 22:26:28.188 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][17/77] Data 0.022 (0.022) Batch 0.066 (0.065) Remain 00:00:08 loss: 0.0604 data: 0.0017 Lr: 0.22240
2024-08-21 22:26:28.258 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][18/77] Data 0.021 (0.022) Batch 0.069 (0.065) Remain 00:00:08 loss: 0.2008 data: -0.0106 Lr: 0.22078
2024-08-21 22:26:28.258 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][18/77] Data 0.027 (0.027) Batch 0.070 (0.065) Remain 00:00:08 loss: 0.2008 data: 0.0112 Lr: 0.22078
2024-08-21 22:26:28.312 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][19/77] Data 0.023 (0.027) Batch 0.054 (0.064) Remain 00:00:08 loss: 0.0806 data: 0.0047 Lr: 0.21916
2024-08-21 22:26:28.312 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][19/77] Data 0.021 (0.022) Batch 0.054 (0.064) Remain 00:00:08 loss: 0.0806 data: -0.0053 Lr: 0.21916
2024-08-21 22:26:28.368 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][20/77] Data 0.022 (0.027) Batch 0.056 (0.064) Remain 00:00:08 loss: 0.1281 data: 0.0030 Lr: 0.21753
2024-08-21 22:26:28.368 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][20/77] Data 0.023 (0.022) Batch 0.056 (0.064) Remain 00:00:08 loss: 0.1281 data: -0.0103 Lr: 0.21753
2024-08-21 22:26:28.422 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][21/77] Data 0.022 (0.026) Batch 0.054 (0.063) Remain 00:00:08 loss: 0.1139 data: -0.0080 Lr: 0.21591
2024-08-21 22:26:28.422 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][21/77] Data 0.022 (0.022) Batch 0.054 (0.063) Remain 00:00:08 loss: 0.1139 data: 0.0014 Lr: 0.21591
2024-08-21 22:26:28.477 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][22/77] Data 0.023 (0.026) Batch 0.055 (0.063) Remain 00:00:08 loss: 0.1184 data: 0.0072 Lr: 0.21429
2024-08-21 22:26:28.477 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][22/77] Data 0.022 (0.022) Batch 0.055 (0.063) Remain 00:00:08 loss: 0.1184 data: 0.0074 Lr: 0.21429
2024-08-21 22:26:28.532 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][23/77] Data 0.023 (0.026) Batch 0.055 (0.063) Remain 00:00:08 loss: 0.1222 data: -0.0051 Lr: 0.21266
2024-08-21 22:26:28.532 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][23/77] Data 0.022 (0.022) Batch 0.055 (0.063) Remain 00:00:08 loss: 0.1222 data: 0.0091 Lr: 0.21266
2024-08-21 22:26:28.588 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][24/77] Data 0.023 (0.026) Batch 0.056 (0.062) Remain 00:00:08 loss: 0.0530 data: 0.0033 Lr: 0.21104
2024-08-21 22:26:28.588 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][24/77] Data 0.022 (0.022) Batch 0.056 (0.062) Remain 00:00:08 loss: 0.0530 data: 0.0050 Lr: 0.21104
2024-08-21 22:26:28.644 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][25/77] Data 0.023 (0.026) Batch 0.056 (0.062) Remain 00:00:08 loss: 0.0683 data: 0.0152 Lr: 0.20942
2024-08-21 22:26:28.644 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][25/77] Data 0.023 (0.022) Batch 0.056 (0.062) Remain 00:00:08 loss: 0.0683 data: 0.0067 Lr: 0.20942
2024-08-21 22:26:28.701 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][26/77] Data 0.023 (0.026) Batch 0.057 (0.062) Remain 00:00:07 loss: 0.0825 data: -0.0023 Lr: 0.20779
2024-08-21 22:26:28.701 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][26/77] Data 0.023 (0.022) Batch 0.057 (0.062) Remain 00:00:07 loss: 0.0825 data: 0.0008 Lr: 0.20779
2024-08-21 22:26:28.759 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][27/77] Data 0.023 (0.026) Batch 0.058 (0.062) Remain 00:00:07 loss: 0.0618 data: 0.0118 Lr: 0.20617
2024-08-21 22:26:28.759 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][27/77] Data 0.022 (0.022) Batch 0.058 (0.062) Remain 00:00:07 loss: 0.0618 data: 0.0005 Lr: 0.20617
2024-08-21 22:26:28.815 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][28/77] Data 0.023 (0.025) Batch 0.056 (0.062) Remain 00:00:07 loss: 0.0803 data: -0.0045 Lr: 0.20455
2024-08-21 22:26:28.815 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][28/77] Data 0.023 (0.022) Batch 0.056 (0.062) Remain 00:00:07 loss: 0.0803 data: 0.0051 Lr: 0.20455
2024-08-21 22:26:28.870 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][29/77] Data 0.022 (0.025) Batch 0.055 (0.061) Remain 00:00:07 loss: 0.1692 data: -0.0048 Lr: 0.20292
2024-08-21 22:26:28.871 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][29/77] Data 0.023 (0.022) Batch 0.055 (0.061) Remain 00:00:07 loss: 0.1692 data: 0.0000 Lr: 0.20292
2024-08-21 22:26:28.926 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][30/77] Data 0.022 (0.025) Batch 0.056 (0.061) Remain 00:00:07 loss: 0.1181 data: 0.0166 Lr: 0.20130
2024-08-21 22:26:28.927 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][30/77] Data 0.023 (0.022) Batch 0.056 (0.061) Remain 00:00:07 loss: 0.1181 data: 0.0034 Lr: 0.20130
2024-08-21 22:26:28.985 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][31/77] Data 0.023 (0.025) Batch 0.058 (0.061) Remain 00:00:07 loss: 0.0905 data: 0.0001 Lr: 0.19968
2024-08-21 22:26:28.985 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][31/77] Data 0.023 (0.022) Batch 0.058 (0.061) Remain 00:00:07 loss: 0.0905 data: -0.0229 Lr: 0.19968
2024-08-21 22:26:29.071 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][32/77] Data 0.040 (0.026) Batch 0.086 (0.062) Remain 00:00:07 loss: 0.0826 data: 0.0063 Lr: 0.19805
2024-08-21 22:26:29.071 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][32/77] Data 0.040 (0.023) Batch 0.086 (0.062) Remain 00:00:07 loss: 0.0826 data: 0.0009 Lr: 0.19805
2024-08-21 22:26:29.136 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][33/77] Data 0.029 (0.023) Batch 0.066 (0.062) Remain 00:00:07 loss: 0.1176 data: -0.0264 Lr: 0.19643
2024-08-21 22:26:29.136 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][33/77] Data 0.033 (0.026) Batch 0.066 (0.062) Remain 00:00:07 loss: 0.1176 data: -0.0038 Lr: 0.19643
2024-08-21 22:26:29.207 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][34/77] Data 0.027 (0.026) Batch 0.071 (0.062) Remain 00:00:07 loss: 0.0912 data: -0.0071 Lr: 0.19481
2024-08-21 22:26:29.208 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][34/77] Data 0.022 (0.023) Batch 0.071 (0.062) Remain 00:00:07 loss: 0.0912 data: -0.0025 Lr: 0.19481
2024-08-21 22:26:29.273 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][35/77] Data 0.027 (0.026) Batch 0.066 (0.062) Remain 00:00:07 loss: 0.1146 data: -0.0320 Lr: 0.19318
2024-08-21 22:26:29.273 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][35/77] Data 0.027 (0.023) Batch 0.066 (0.062) Remain 00:00:07 loss: 0.1146 data: 0.0015 Lr: 0.19318
2024-08-21 22:26:29.344 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][36/77] Data 0.027 (0.023) Batch 0.070 (0.063) Remain 00:00:07 loss: 0.0841 data: -0.0123 Lr: 0.19156
2024-08-21 22:26:29.344 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][36/77] Data 0.028 (0.026) Batch 0.071 (0.063) Remain 00:00:07 loss: 0.0841 data: 0.0079 Lr: 0.19156
2024-08-21 22:26:29.409 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][37/77] Data 0.025 (0.026) Batch 0.065 (0.063) Remain 00:00:07 loss: 0.1275 data: 0.0035 Lr: 0.18994
2024-08-21 22:26:29.409 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][37/77] Data 0.027 (0.023) Batch 0.066 (0.063) Remain 00:00:07 loss: 0.1275 data: 0.0167 Lr: 0.18994
2024-08-21 22:26:29.475 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][38/77] Data 0.029 (0.026) Batch 0.065 (0.063) Remain 00:00:07 loss: 0.0657 data: 0.0007 Lr: 0.18831
2024-08-21 22:26:29.475 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_500
2024-08-21 22:26:29.475 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][38/77] Data 0.027 (0.023) Batch 0.065 (0.063) Remain 00:00:07 loss: 0.0657 data: -0.0054 Lr: 0.18831
2024-08-21 22:26:29.475 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_500
2024-08-21 22:26:29.494 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -578.3916625976562
2024-08-21 22:26:29.494 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -578.3916625976562
2024-08-21 22:26:29.495 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -275.4422607421875
2024-08-21 22:26:29.495 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -302.94940185546875
2024-08-21 22:26:29.561 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][39/77] Data 0.047 (0.027) Batch 0.086 (0.063) Remain 00:00:07 loss: 0.1021 data: 0.0020 Lr: 0.18669
2024-08-21 22:26:29.561 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][39/77] Data 0.047 (0.024) Batch 0.086 (0.063) Remain 00:00:07 loss: 0.1021 data: -0.0102 Lr: 0.18669
2024-08-21 22:26:29.626 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][40/77] Data 0.027 (0.027) Batch 0.065 (0.063) Remain 00:00:07 loss: 0.1224 data: -0.0045 Lr: 0.18506
2024-08-21 22:26:29.626 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][40/77] Data 0.027 (0.024) Batch 0.065 (0.063) Remain 00:00:07 loss: 0.1224 data: -0.0030 Lr: 0.18506
2024-08-21 22:26:29.691 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][41/77] Data 0.027 (0.027) Batch 0.065 (0.063) Remain 00:00:07 loss: 0.0669 data: 0.0053 Lr: 0.18344
2024-08-21 22:26:29.691 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][41/77] Data 0.027 (0.024) Batch 0.065 (0.063) Remain 00:00:07 loss: 0.0669 data: -0.0008 Lr: 0.18344
2024-08-21 22:26:29.756 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][42/77] Data 0.027 (0.027) Batch 0.065 (0.063) Remain 00:00:07 loss: 0.0637 data: -0.0071 Lr: 0.18182
2024-08-21 22:26:29.756 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][42/77] Data 0.027 (0.024) Batch 0.065 (0.063) Remain 00:00:07 loss: 0.0637 data: 0.0082 Lr: 0.18182
2024-08-21 22:26:29.821 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][43/77] Data 0.027 (0.027) Batch 0.066 (0.064) Remain 00:00:07 loss: 0.1099 data: -0.0066 Lr: 0.18019
2024-08-21 22:26:29.821 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][43/77] Data 0.027 (0.024) Batch 0.066 (0.064) Remain 00:00:07 loss: 0.1099 data: -0.0112 Lr: 0.18019
2024-08-21 22:26:29.887 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][44/77] Data 0.027 (0.027) Batch 0.065 (0.064) Remain 00:00:07 loss: 0.0489 data: 0.0052 Lr: 0.17857
2024-08-21 22:26:29.887 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][44/77] Data 0.027 (0.024) Batch 0.065 (0.064) Remain 00:00:07 loss: 0.0489 data: -0.0066 Lr: 0.17857
2024-08-21 22:26:29.952 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][45/77] Data 0.027 (0.027) Batch 0.066 (0.064) Remain 00:00:06 loss: 0.1440 data: 0.0016 Lr: 0.17695
2024-08-21 22:26:29.952 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][45/77] Data 0.027 (0.024) Batch 0.066 (0.064) Remain 00:00:06 loss: 0.1440 data: 0.0145 Lr: 0.17695
2024-08-21 22:26:30.017 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][46/77] Data 0.027 (0.027) Batch 0.065 (0.064) Remain 00:00:06 loss: 0.0627 data: -0.0031 Lr: 0.17532
2024-08-21 22:26:30.017 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][46/77] Data 0.027 (0.024) Batch 0.065 (0.064) Remain 00:00:06 loss: 0.0627 data: -0.0037 Lr: 0.17532
2024-08-21 22:26:30.082 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][47/77] Data 0.027 (0.027) Batch 0.065 (0.064) Remain 00:00:06 loss: 0.0860 data: 0.0088 Lr: 0.17370
2024-08-21 22:26:30.082 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][47/77] Data 0.027 (0.024) Batch 0.065 (0.064) Remain 00:00:06 loss: 0.0860 data: 0.0097 Lr: 0.17370
2024-08-21 22:26:30.147 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][48/77] Data 0.027 (0.027) Batch 0.065 (0.064) Remain 00:00:06 loss: 0.0486 data: -0.0063 Lr: 0.17208
2024-08-21 22:26:30.147 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][48/77] Data 0.027 (0.024) Batch 0.065 (0.064) Remain 00:00:06 loss: 0.0486 data: 0.0044 Lr: 0.17208
2024-08-21 22:26:30.211 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][49/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:06 loss: 0.0677 data: 0.0184 Lr: 0.17045
2024-08-21 22:26:30.211 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][49/77] Data 0.027 (0.025) Batch 0.064 (0.064) Remain 00:00:06 loss: 0.0677 data: 0.0028 Lr: 0.17045
2024-08-21 22:26:30.276 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][50/77] Data 0.027 (0.027) Batch 0.065 (0.064) Remain 00:00:06 loss: 0.0767 data: 0.0187 Lr: 0.16883
2024-08-21 22:26:30.276 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][50/77] Data 0.027 (0.025) Batch 0.065 (0.064) Remain 00:00:06 loss: 0.0767 data: -0.0056 Lr: 0.16883
2024-08-21 22:26:30.340 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][51/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:06 loss: 0.0648 data: -0.0055 Lr: 0.16721
2024-08-21 22:26:30.340 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][51/77] Data 0.027 (0.025) Batch 0.064 (0.064) Remain 00:00:06 loss: 0.0648 data: 0.0066 Lr: 0.16721
2024-08-21 22:26:30.405 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][52/77] Data 0.027 (0.027) Batch 0.065 (0.064) Remain 00:00:06 loss: 0.1287 data: -0.0154 Lr: 0.16558
2024-08-21 22:26:30.405 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][52/77] Data 0.027 (0.025) Batch 0.065 (0.064) Remain 00:00:06 loss: 0.1287 data: 0.0024 Lr: 0.16558
2024-08-21 22:26:30.470 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][53/77] Data 0.027 (0.027) Batch 0.065 (0.064) Remain 00:00:06 loss: 0.0694 data: 0.0010 Lr: 0.16396
2024-08-21 22:26:30.470 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][53/77] Data 0.027 (0.025) Batch 0.064 (0.064) Remain 00:00:06 loss: 0.0694 data: 0.0070 Lr: 0.16396
2024-08-21 22:26:30.552 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][54/77] Data 0.027 (0.027) Batch 0.082 (0.064) Remain 00:00:06 loss: 0.1354 data: -0.0035 Lr: 0.16234
2024-08-21 22:26:30.552 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][54/77] Data 0.039 (0.025) Batch 0.082 (0.064) Remain 00:00:06 loss: 0.1354 data: -0.0023 Lr: 0.16234
2024-08-21 22:26:30.624 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][55/77] Data 0.027 (0.027) Batch 0.073 (0.064) Remain 00:00:06 loss: 0.1063 data: 0.0044 Lr: 0.16071
2024-08-21 22:26:30.624 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][55/77] Data 0.027 (0.025) Batch 0.073 (0.064) Remain 00:00:06 loss: 0.1063 data: 0.0008 Lr: 0.16071
2024-08-21 22:26:30.687 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][56/77] Data 0.027 (0.027) Batch 0.063 (0.064) Remain 00:00:06 loss: 0.0948 data: -0.0034 Lr: 0.15909
2024-08-21 22:26:30.688 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][56/77] Data 0.027 (0.025) Batch 0.063 (0.064) Remain 00:00:06 loss: 0.0948 data: -0.0045 Lr: 0.15909
2024-08-21 22:26:30.750 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][57/77] Data 0.027 (0.027) Batch 0.062 (0.064) Remain 00:00:06 loss: 0.0545 data: 0.0192 Lr: 0.15747
2024-08-21 22:26:30.750 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][57/77] Data 0.022 (0.025) Batch 0.062 (0.064) Remain 00:00:06 loss: 0.0545 data: -0.0022 Lr: 0.15747
2024-08-21 22:26:30.812 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][58/77] Data 0.027 (0.027) Batch 0.063 (0.064) Remain 00:00:06 loss: 0.1029 data: -0.0038 Lr: 0.15584
2024-08-21 22:26:30.813 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][58/77] Data 0.026 (0.025) Batch 0.063 (0.064) Remain 00:00:06 loss: 0.1029 data: 0.0035 Lr: 0.15584
2024-08-21 22:26:30.876 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][59/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:06 loss: 0.0652 data: -0.0079 Lr: 0.15422
2024-08-21 22:26:30.876 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][59/77] Data 0.022 (0.025) Batch 0.064 (0.064) Remain 00:00:06 loss: 0.0652 data: -0.0107 Lr: 0.15422
2024-08-21 22:26:30.965 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][60/77] Data 0.027 (0.027) Batch 0.088 (0.065) Remain 00:00:06 loss: 0.0644 data: -0.0010 Lr: 0.15260
2024-08-21 22:26:30.965 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][60/77] Data 0.038 (0.025) Batch 0.088 (0.065) Remain 00:00:06 loss: 0.0644 data: -0.0031 Lr: 0.15260
2024-08-21 22:26:31.040 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][61/77] Data 0.030 (0.027) Batch 0.075 (0.065) Remain 00:00:06 loss: 0.0508 data: -0.0009 Lr: 0.15097
2024-08-21 22:26:31.040 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][61/77] Data 0.027 (0.025) Batch 0.075 (0.065) Remain 00:00:06 loss: 0.0508 data: -0.0113 Lr: 0.15097
2024-08-21 22:26:31.112 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][62/77] Data 0.035 (0.027) Batch 0.072 (0.065) Remain 00:00:06 loss: 0.0871 data: -0.0007 Lr: 0.14935
2024-08-21 22:26:31.112 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][62/77] Data 0.027 (0.025) Batch 0.072 (0.065) Remain 00:00:06 loss: 0.0871 data: -0.0043 Lr: 0.14935
2024-08-21 22:26:31.178 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][63/77] Data 0.027 (0.027) Batch 0.066 (0.065) Remain 00:00:05 loss: 0.0821 data: -0.0075 Lr: 0.14773
2024-08-21 22:26:31.178 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][63/77] Data 0.027 (0.025) Batch 0.066 (0.065) Remain 00:00:05 loss: 0.0821 data: 0.0052 Lr: 0.14773
2024-08-21 22:26:31.245 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][64/77] Data 0.028 (0.027) Batch 0.067 (0.065) Remain 00:00:05 loss: 0.1587 data: 0.0079 Lr: 0.14610
2024-08-21 22:26:31.245 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][64/77] Data 0.027 (0.025) Batch 0.067 (0.065) Remain 00:00:05 loss: 0.1587 data: 0.0068 Lr: 0.14610
2024-08-21 22:26:31.311 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][65/77] Data 0.028 (0.027) Batch 0.067 (0.065) Remain 00:00:05 loss: 0.0946 data: -0.0062 Lr: 0.14448
2024-08-21 22:26:31.312 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][65/77] Data 0.027 (0.025) Batch 0.067 (0.065) Remain 00:00:05 loss: 0.0946 data: 0.0012 Lr: 0.14448
2024-08-21 22:26:31.381 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][66/77] Data 0.028 (0.027) Batch 0.069 (0.065) Remain 00:00:05 loss: 0.0839 data: 0.0081 Lr: 0.14286
2024-08-21 22:26:31.381 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][66/77] Data 0.027 (0.025) Batch 0.069 (0.065) Remain 00:00:05 loss: 0.0839 data: -0.0030 Lr: 0.14286
2024-08-21 22:26:31.452 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][67/77] Data 0.028 (0.027) Batch 0.071 (0.065) Remain 00:00:05 loss: 0.1037 data: -0.0052 Lr: 0.14123
2024-08-21 22:26:31.452 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][67/77] Data 0.028 (0.025) Batch 0.071 (0.065) Remain 00:00:05 loss: 0.1037 data: -0.0037 Lr: 0.14123
2024-08-21 22:26:31.522 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][68/77] Data 0.028 (0.027) Batch 0.070 (0.065) Remain 00:00:05 loss: 0.0691 data: -0.0219 Lr: 0.13961
2024-08-21 22:26:31.522 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][68/77] Data 0.028 (0.025) Batch 0.070 (0.065) Remain 00:00:05 loss: 0.0691 data: 0.0103 Lr: 0.13961
2024-08-21 22:26:31.591 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][69/77] Data 0.028 (0.027) Batch 0.069 (0.065) Remain 00:00:05 loss: 0.1038 data: -0.0128 Lr: 0.13799
2024-08-21 22:26:31.592 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][69/77] Data 0.028 (0.025) Batch 0.069 (0.065) Remain 00:00:05 loss: 0.1038 data: -0.0022 Lr: 0.13799
2024-08-21 22:26:31.658 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][70/77] Data 0.028 (0.027) Batch 0.066 (0.065) Remain 00:00:05 loss: 0.1654 data: -0.0059 Lr: 0.13636
2024-08-21 22:26:31.658 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][70/77] Data 0.027 (0.026) Batch 0.066 (0.065) Remain 00:00:05 loss: 0.1654 data: -0.0035 Lr: 0.13636
2024-08-21 22:26:31.722 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][71/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:05 loss: 0.0831 data: -0.0177 Lr: 0.13474
2024-08-21 22:26:31.722 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][71/77] Data 0.027 (0.026) Batch 0.064 (0.065) Remain 00:00:05 loss: 0.0831 data: 0.0189 Lr: 0.13474
2024-08-21 22:26:31.788 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][72/77] Data 0.029 (0.027) Batch 0.066 (0.065) Remain 00:00:05 loss: 0.0880 data: 0.0072 Lr: 0.13312
2024-08-21 22:26:31.788 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][72/77] Data 0.027 (0.026) Batch 0.066 (0.065) Remain 00:00:05 loss: 0.0880 data: -0.0186 Lr: 0.13312
2024-08-21 22:26:31.852 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][73/77] Data 0.027 (0.027) Batch 0.064 (0.065) Remain 00:00:05 loss: 0.1304 data: -0.0020 Lr: 0.13149
2024-08-21 22:26:31.852 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][73/77] Data 0.027 (0.026) Batch 0.064 (0.065) Remain 00:00:05 loss: 0.1304 data: -0.0192 Lr: 0.13149
2024-08-21 22:26:31.917 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][74/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:05 loss: 0.1350 data: 0.0071 Lr: 0.12987
2024-08-21 22:26:31.917 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][74/77] Data 0.027 (0.026) Batch 0.065 (0.065) Remain 00:00:05 loss: 0.1350 data: -0.0122 Lr: 0.12987
2024-08-21 22:26:31.983 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][75/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:05 loss: 0.1029 data: 0.0052 Lr: 0.12825
2024-08-21 22:26:31.983 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][75/77] Data 0.027 (0.026) Batch 0.065 (0.065) Remain 00:00:05 loss: 0.1029 data: 0.0056 Lr: 0.12825
2024-08-21 22:26:32.048 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][76/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:05 loss: 0.0981 data: -0.0066 Lr: 0.12662
2024-08-21 22:26:32.048 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][76/77] Data 0.027 (0.026) Batch 0.065 (0.065) Remain 00:00:05 loss: 0.0981 data: 0.0208 Lr: 0.12662
2024-08-21 22:26:32.089 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][77/77] Data 0.029 (0.027) Batch 0.041 (0.065) Remain 00:00:05 loss: 0.0675 data: 0.0074 Lr: 0.12500
2024-08-21 22:26:32.089 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 22:26:32.089 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][77/77] Data 0.029 (0.026) Batch 0.041 (0.065) Remain 00:00:05 loss: 0.0675 data: 0.0015 Lr: 0.12500
2024-08-21 22:26:32.089 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 22:26:36.539 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0302, Accuracy: 0.9895
2024-08-21 22:26:36.539 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0302, Accuracy: 0.9895
2024-08-21 22:26:36.539 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 22:26:36.539 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 22:26:36.539 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 22:26:36.539 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 22:26:36.617 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][1/77] Data 0.042 (0.042) Batch 0.077 (0.077) Remain 00:00:05 loss: 0.0710 data: -0.0064 Lr: 0.12338
2024-08-21 22:26:36.617 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][1/77] Data 0.044 (0.044) Batch 0.077 (0.077) Remain 00:00:05 loss: 0.0710 data: 0.0044 Lr: 0.12338
2024-08-21 22:26:36.673 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][2/77] Data 0.024 (0.024) Batch 0.056 (0.056) Remain 00:00:04 loss: 0.0904 data: -0.0154 Lr: 0.12175
2024-08-21 22:26:36.673 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][2/77] Data 0.022 (0.022) Batch 0.056 (0.056) Remain 00:00:04 loss: 0.0904 data: -0.0069 Lr: 0.12175
2024-08-21 22:26:36.729 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][3/77] Data 0.023 (0.023) Batch 0.056 (0.056) Remain 00:00:04 loss: 0.0997 data: 0.0051 Lr: 0.12013
2024-08-21 22:26:36.729 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][3/77] Data 0.022 (0.022) Batch 0.056 (0.056) Remain 00:00:04 loss: 0.0997 data: 0.0093 Lr: 0.12013
2024-08-21 22:26:36.784 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][4/77] Data 0.023 (0.023) Batch 0.055 (0.056) Remain 00:00:04 loss: 0.0702 data: -0.0016 Lr: 0.11851
2024-08-21 22:26:36.784 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][4/77] Data 0.024 (0.023) Batch 0.055 (0.056) Remain 00:00:04 loss: 0.0702 data: -0.0150 Lr: 0.11851
2024-08-21 22:26:36.841 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][5/77] Data 0.024 (0.023) Batch 0.057 (0.056) Remain 00:00:04 loss: 0.0845 data: -0.0099 Lr: 0.11688
2024-08-21 22:26:36.841 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][5/77] Data 0.023 (0.023) Batch 0.057 (0.056) Remain 00:00:04 loss: 0.0845 data: 0.0039 Lr: 0.11688
2024-08-21 22:26:36.909 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][6/77] Data 0.023 (0.023) Batch 0.068 (0.058) Remain 00:00:04 loss: 0.0807 data: 0.0066 Lr: 0.11526
2024-08-21 22:26:36.909 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][6/77] Data 0.031 (0.024) Batch 0.068 (0.058) Remain 00:00:04 loss: 0.0807 data: 0.0028 Lr: 0.11526
2024-08-21 22:26:36.974 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][7/77] Data 0.022 (0.023) Batch 0.066 (0.060) Remain 00:00:04 loss: 0.1022 data: 0.0012 Lr: 0.11364
2024-08-21 22:26:36.975 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][7/77] Data 0.027 (0.025) Batch 0.066 (0.060) Remain 00:00:04 loss: 0.1022 data: 0.0029 Lr: 0.11364
2024-08-21 22:26:37.040 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][8/77] Data 0.021 (0.023) Batch 0.065 (0.060) Remain 00:00:04 loss: 0.0668 data: -0.0092 Lr: 0.11201
2024-08-21 22:26:37.040 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][8/77] Data 0.027 (0.025) Batch 0.065 (0.060) Remain 00:00:04 loss: 0.0668 data: -0.0132 Lr: 0.11201
2024-08-21 22:26:37.105 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][9/77] Data 0.021 (0.023) Batch 0.065 (0.061) Remain 00:00:04 loss: 0.0669 data: 0.0104 Lr: 0.11039
2024-08-21 22:26:37.105 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][9/77] Data 0.027 (0.026) Batch 0.065 (0.061) Remain 00:00:04 loss: 0.0669 data: 0.0191 Lr: 0.11039
2024-08-21 22:26:37.170 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][10/77] Data 0.022 (0.022) Batch 0.065 (0.062) Remain 00:00:04 loss: 0.0863 data: -0.0118 Lr: 0.10877
2024-08-21 22:26:37.170 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][10/77] Data 0.027 (0.026) Batch 0.065 (0.062) Remain 00:00:04 loss: 0.0863 data: -0.0013 Lr: 0.10877
2024-08-21 22:26:37.235 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][11/77] Data 0.021 (0.022) Batch 0.065 (0.062) Remain 00:00:04 loss: 0.1087 data: 0.0034 Lr: 0.10714
2024-08-21 22:26:37.235 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_550
2024-08-21 22:26:37.235 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][11/77] Data 0.027 (0.026) Batch 0.065 (0.062) Remain 00:00:04 loss: 0.1087 data: -0.0117 Lr: 0.10714
2024-08-21 22:26:37.236 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_550
2024-08-21 22:26:37.255 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -584.8433837890625
2024-08-21 22:26:37.256 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -283.0082702636719
2024-08-21 22:26:37.256 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -584.8433837890625
2024-08-21 22:26:37.256 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -301.8351135253906
2024-08-21 22:26:37.322 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][12/77] Data 0.041 (0.024) Batch 0.086 (0.064) Remain 00:00:04 loss: 0.0734 data: -0.0093 Lr: 0.10552
2024-08-21 22:26:37.322 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][12/77] Data 0.048 (0.028) Batch 0.086 (0.064) Remain 00:00:04 loss: 0.0734 data: 0.0032 Lr: 0.10552
2024-08-21 22:26:37.387 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][13/77] Data 0.023 (0.024) Batch 0.065 (0.064) Remain 00:00:04 loss: 0.0667 data: 0.0014 Lr: 0.10390
2024-08-21 22:26:37.387 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][13/77] Data 0.027 (0.028) Batch 0.065 (0.064) Remain 00:00:04 loss: 0.0667 data: 0.0072 Lr: 0.10390
2024-08-21 22:26:37.452 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][14/77] Data 0.021 (0.024) Batch 0.065 (0.064) Remain 00:00:04 loss: 0.0568 data: -0.0302 Lr: 0.10227
2024-08-21 22:26:37.452 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][14/77] Data 0.027 (0.028) Batch 0.065 (0.064) Remain 00:00:04 loss: 0.0568 data: -0.0137 Lr: 0.10227
2024-08-21 22:26:37.517 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][15/77] Data 0.023 (0.024) Batch 0.065 (0.064) Remain 00:00:04 loss: 0.0765 data: -0.0052 Lr: 0.10065
2024-08-21 22:26:37.517 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][15/77] Data 0.027 (0.028) Batch 0.065 (0.064) Remain 00:00:04 loss: 0.0765 data: 0.0111 Lr: 0.10065
2024-08-21 22:26:37.582 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][16/77] Data 0.022 (0.024) Batch 0.065 (0.064) Remain 00:00:03 loss: 0.0395 data: 0.0036 Lr: 0.09903
2024-08-21 22:26:37.582 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][16/77] Data 0.027 (0.028) Batch 0.065 (0.064) Remain 00:00:03 loss: 0.0395 data: -0.0124 Lr: 0.09903
2024-08-21 22:26:37.647 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][17/77] Data 0.022 (0.023) Batch 0.065 (0.064) Remain 00:00:03 loss: 0.0689 data: 0.0048 Lr: 0.09740
2024-08-21 22:26:37.647 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][17/77] Data 0.027 (0.028) Batch 0.065 (0.064) Remain 00:00:03 loss: 0.0689 data: 0.0047 Lr: 0.09740
2024-08-21 22:26:37.712 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][18/77] Data 0.021 (0.023) Batch 0.065 (0.064) Remain 00:00:03 loss: 0.0892 data: 0.0172 Lr: 0.09578
2024-08-21 22:26:37.712 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][18/77] Data 0.027 (0.028) Batch 0.065 (0.064) Remain 00:00:03 loss: 0.0892 data: -0.0082 Lr: 0.09578
2024-08-21 22:26:37.777 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][19/77] Data 0.021 (0.023) Batch 0.065 (0.064) Remain 00:00:03 loss: 0.0433 data: 0.0141 Lr: 0.09416
2024-08-21 22:26:37.777 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][19/77] Data 0.027 (0.028) Batch 0.065 (0.064) Remain 00:00:03 loss: 0.0433 data: -0.0048 Lr: 0.09416
2024-08-21 22:26:37.842 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][20/77] Data 0.021 (0.023) Batch 0.065 (0.064) Remain 00:00:03 loss: 0.0712 data: 0.0081 Lr: 0.09253
2024-08-21 22:26:37.842 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][20/77] Data 0.027 (0.028) Batch 0.065 (0.064) Remain 00:00:03 loss: 0.0712 data: 0.0308 Lr: 0.09253
2024-08-21 22:26:37.907 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][21/77] Data 0.021 (0.023) Batch 0.065 (0.065) Remain 00:00:03 loss: 0.1545 data: 0.0044 Lr: 0.09091
2024-08-21 22:26:37.907 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][21/77] Data 0.027 (0.028) Batch 0.065 (0.065) Remain 00:00:03 loss: 0.1545 data: -0.0009 Lr: 0.09091
2024-08-21 22:26:37.971 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][22/77] Data 0.021 (0.023) Batch 0.065 (0.065) Remain 00:00:03 loss: 0.0662 data: -0.0057 Lr: 0.08929
2024-08-21 22:26:37.972 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][22/77] Data 0.027 (0.028) Batch 0.065 (0.065) Remain 00:00:03 loss: 0.0662 data: 0.0053 Lr: 0.08929
2024-08-21 22:26:38.036 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][23/77] Data 0.021 (0.023) Batch 0.065 (0.065) Remain 00:00:03 loss: 0.1177 data: -0.0005 Lr: 0.08766
2024-08-21 22:26:38.037 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][23/77] Data 0.027 (0.028) Batch 0.065 (0.065) Remain 00:00:03 loss: 0.1177 data: 0.0018 Lr: 0.08766
2024-08-21 22:26:38.101 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][24/77] Data 0.023 (0.023) Batch 0.065 (0.065) Remain 00:00:03 loss: 0.0732 data: -0.0035 Lr: 0.08604
2024-08-21 22:26:38.101 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][24/77] Data 0.027 (0.028) Batch 0.065 (0.065) Remain 00:00:03 loss: 0.0732 data: 0.0055 Lr: 0.08604
2024-08-21 22:26:38.167 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][25/77] Data 0.023 (0.023) Batch 0.065 (0.065) Remain 00:00:03 loss: 0.0845 data: -0.0067 Lr: 0.08442
2024-08-21 22:26:38.167 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][25/77] Data 0.027 (0.028) Batch 0.065 (0.065) Remain 00:00:03 loss: 0.0845 data: -0.0088 Lr: 0.08442
2024-08-21 22:26:38.231 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][26/77] Data 0.021 (0.023) Batch 0.065 (0.065) Remain 00:00:03 loss: 0.0610 data: 0.0067 Lr: 0.08279
2024-08-21 22:26:38.232 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][26/77] Data 0.027 (0.028) Batch 0.065 (0.065) Remain 00:00:03 loss: 0.0610 data: -0.0076 Lr: 0.08279
2024-08-21 22:26:38.295 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][27/77] Data 0.021 (0.023) Batch 0.063 (0.065) Remain 00:00:03 loss: 0.0367 data: 0.0105 Lr: 0.08117
2024-08-21 22:26:38.295 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][27/77] Data 0.027 (0.027) Batch 0.063 (0.065) Remain 00:00:03 loss: 0.0367 data: -0.0088 Lr: 0.08117
2024-08-21 22:26:38.361 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][28/77] Data 0.021 (0.023) Batch 0.066 (0.065) Remain 00:00:03 loss: 0.0384 data: -0.0124 Lr: 0.07955
2024-08-21 22:26:38.361 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][28/77] Data 0.027 (0.027) Batch 0.066 (0.065) Remain 00:00:03 loss: 0.0384 data: -0.0090 Lr: 0.07955
2024-08-21 22:26:38.426 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][29/77] Data 0.021 (0.023) Batch 0.065 (0.065) Remain 00:00:03 loss: 0.0819 data: 0.0209 Lr: 0.07792
2024-08-21 22:26:38.426 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][29/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:03 loss: 0.0819 data: -0.0040 Lr: 0.07792
2024-08-21 22:26:38.492 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][30/77] Data 0.022 (0.023) Batch 0.066 (0.065) Remain 00:00:03 loss: 0.0608 data: 0.0119 Lr: 0.07630
2024-08-21 22:26:38.492 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][30/77] Data 0.027 (0.027) Batch 0.066 (0.065) Remain 00:00:03 loss: 0.0608 data: -0.0084 Lr: 0.07630
2024-08-21 22:26:38.557 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][31/77] Data 0.021 (0.022) Batch 0.065 (0.065) Remain 00:00:03 loss: 0.0989 data: -0.0029 Lr: 0.07468
2024-08-21 22:26:38.557 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][31/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:03 loss: 0.0989 data: 0.0025 Lr: 0.07468
2024-08-21 22:26:38.623 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][32/77] Data 0.023 (0.023) Batch 0.066 (0.065) Remain 00:00:02 loss: 0.1261 data: -0.0170 Lr: 0.07305
2024-08-21 22:26:38.623 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][32/77] Data 0.027 (0.027) Batch 0.066 (0.065) Remain 00:00:02 loss: 0.1261 data: -0.0100 Lr: 0.07305
2024-08-21 22:26:38.688 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][33/77] Data 0.021 (0.022) Batch 0.065 (0.065) Remain 00:00:02 loss: 0.0698 data: -0.0237 Lr: 0.07143
2024-08-21 22:26:38.688 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][33/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:02 loss: 0.0698 data: -0.0142 Lr: 0.07143
2024-08-21 22:26:38.754 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][34/77] Data 0.030 (0.023) Batch 0.067 (0.065) Remain 00:00:02 loss: 0.0497 data: -0.0005 Lr: 0.06981
2024-08-21 22:26:38.754 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][34/77] Data 0.027 (0.027) Batch 0.067 (0.065) Remain 00:00:02 loss: 0.0497 data: 0.0079 Lr: 0.06981
2024-08-21 22:26:38.825 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][35/77] Data 0.027 (0.023) Batch 0.071 (0.065) Remain 00:00:02 loss: 0.1776 data: -0.0102 Lr: 0.06818
2024-08-21 22:26:38.825 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][35/77] Data 0.027 (0.027) Batch 0.071 (0.065) Remain 00:00:02 loss: 0.1776 data: -0.0089 Lr: 0.06818
2024-08-21 22:26:38.891 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][36/77] Data 0.027 (0.023) Batch 0.066 (0.065) Remain 00:00:02 loss: 0.0544 data: -0.0073 Lr: 0.06656
2024-08-21 22:26:38.891 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][36/77] Data 0.027 (0.027) Batch 0.066 (0.065) Remain 00:00:02 loss: 0.0544 data: 0.0150 Lr: 0.06656
2024-08-21 22:26:38.957 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][37/77] Data 0.027 (0.023) Batch 0.066 (0.065) Remain 00:00:02 loss: 0.1090 data: 0.0178 Lr: 0.06494
2024-08-21 22:26:38.957 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][37/77] Data 0.027 (0.027) Batch 0.066 (0.065) Remain 00:00:02 loss: 0.1090 data: 0.0072 Lr: 0.06494
2024-08-21 22:26:39.032 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][38/77] Data 0.033 (0.023) Batch 0.075 (0.065) Remain 00:00:02 loss: 0.0473 data: 0.0097 Lr: 0.06331
2024-08-21 22:26:39.033 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][38/77] Data 0.030 (0.027) Batch 0.075 (0.065) Remain 00:00:02 loss: 0.0473 data: 0.0146 Lr: 0.06331
2024-08-21 22:26:39.102 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][39/77] Data 0.032 (0.024) Batch 0.070 (0.065) Remain 00:00:02 loss: 0.0392 data: 0.0174 Lr: 0.06169
2024-08-21 22:26:39.103 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][39/77] Data 0.027 (0.027) Batch 0.070 (0.065) Remain 00:00:02 loss: 0.0392 data: -0.0041 Lr: 0.06169
2024-08-21 22:26:39.168 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][40/77] Data 0.027 (0.024) Batch 0.065 (0.065) Remain 00:00:02 loss: 0.0361 data: -0.0229 Lr: 0.06006
2024-08-21 22:26:39.168 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][40/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:02 loss: 0.0361 data: -0.0103 Lr: 0.06006
2024-08-21 22:26:39.232 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][41/77] Data 0.027 (0.024) Batch 0.064 (0.065) Remain 00:00:02 loss: 0.0638 data: 0.0221 Lr: 0.05844
2024-08-21 22:26:39.232 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][41/77] Data 0.027 (0.027) Batch 0.064 (0.065) Remain 00:00:02 loss: 0.0638 data: 0.0055 Lr: 0.05844
2024-08-21 22:26:39.297 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][42/77] Data 0.027 (0.024) Batch 0.065 (0.065) Remain 00:00:02 loss: 0.1610 data: 0.0027 Lr: 0.05682
2024-08-21 22:26:39.298 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][42/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:02 loss: 0.1610 data: -0.0051 Lr: 0.05682
2024-08-21 22:26:39.363 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][43/77] Data 0.027 (0.024) Batch 0.065 (0.065) Remain 00:00:02 loss: 0.0642 data: -0.0048 Lr: 0.05519
2024-08-21 22:26:39.363 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][43/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:02 loss: 0.0642 data: 0.0180 Lr: 0.05519
2024-08-21 22:26:39.430 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][44/77] Data 0.027 (0.024) Batch 0.067 (0.065) Remain 00:00:02 loss: 0.0874 data: 0.0079 Lr: 0.05357
2024-08-21 22:26:39.430 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][44/77] Data 0.027 (0.027) Batch 0.067 (0.065) Remain 00:00:02 loss: 0.0874 data: -0.0078 Lr: 0.05357
2024-08-21 22:26:39.495 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][45/77] Data 0.027 (0.024) Batch 0.066 (0.065) Remain 00:00:02 loss: 0.1280 data: 0.0060 Lr: 0.05195
2024-08-21 22:26:39.495 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][45/77] Data 0.027 (0.027) Batch 0.066 (0.065) Remain 00:00:02 loss: 0.1280 data: 0.0004 Lr: 0.05195
2024-08-21 22:26:39.563 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][46/77] Data 0.027 (0.024) Batch 0.068 (0.065) Remain 00:00:02 loss: 0.0821 data: 0.0093 Lr: 0.05032
2024-08-21 22:26:39.563 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][46/77] Data 0.027 (0.027) Batch 0.068 (0.065) Remain 00:00:02 loss: 0.0821 data: -0.0119 Lr: 0.05032
2024-08-21 22:26:39.634 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][47/77] Data 0.032 (0.024) Batch 0.071 (0.066) Remain 00:00:02 loss: 0.0982 data: 0.0069 Lr: 0.04870
2024-08-21 22:26:39.634 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][47/77] Data 0.028 (0.027) Batch 0.071 (0.066) Remain 00:00:02 loss: 0.0982 data: -0.0043 Lr: 0.04870
2024-08-21 22:26:39.707 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][48/77] Data 0.027 (0.024) Batch 0.073 (0.066) Remain 00:00:01 loss: 0.0683 data: -0.0116 Lr: 0.04708
2024-08-21 22:26:39.707 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][48/77] Data 0.027 (0.027) Batch 0.073 (0.066) Remain 00:00:01 loss: 0.0683 data: 0.0009 Lr: 0.04708
2024-08-21 22:26:39.774 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][49/77] Data 0.027 (0.024) Batch 0.068 (0.066) Remain 00:00:01 loss: 0.0905 data: 0.0085 Lr: 0.04545
2024-08-21 22:26:39.775 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][49/77] Data 0.027 (0.027) Batch 0.068 (0.066) Remain 00:00:01 loss: 0.0905 data: 0.0096 Lr: 0.04545
2024-08-21 22:26:39.844 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][50/77] Data 0.027 (0.024) Batch 0.070 (0.066) Remain 00:00:01 loss: 0.0733 data: -0.0132 Lr: 0.04383
2024-08-21 22:26:39.844 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][50/77] Data 0.031 (0.028) Batch 0.070 (0.066) Remain 00:00:01 loss: 0.0733 data: -0.0011 Lr: 0.04383
2024-08-21 22:26:39.910 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][51/77] Data 0.027 (0.025) Batch 0.066 (0.066) Remain 00:00:01 loss: 0.1146 data: -0.0028 Lr: 0.04221
2024-08-21 22:26:39.910 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][51/77] Data 0.027 (0.028) Batch 0.066 (0.066) Remain 00:00:01 loss: 0.1146 data: -0.0086 Lr: 0.04221
2024-08-21 22:26:39.975 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][52/77] Data 0.027 (0.025) Batch 0.065 (0.066) Remain 00:00:01 loss: 0.0730 data: 0.0056 Lr: 0.04058
2024-08-21 22:26:39.975 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][52/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:01 loss: 0.0730 data: -0.0101 Lr: 0.04058
2024-08-21 22:26:40.042 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][53/77] Data 0.027 (0.025) Batch 0.066 (0.066) Remain 00:00:01 loss: 0.1100 data: 0.0171 Lr: 0.03896
2024-08-21 22:26:40.042 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][53/77] Data 0.027 (0.027) Batch 0.066 (0.066) Remain 00:00:01 loss: 0.1100 data: -0.0067 Lr: 0.03896
2024-08-21 22:26:40.107 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][54/77] Data 0.027 (0.025) Batch 0.066 (0.066) Remain 00:00:01 loss: 0.0827 data: -0.0228 Lr: 0.03734
2024-08-21 22:26:40.107 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][54/77] Data 0.027 (0.027) Batch 0.066 (0.066) Remain 00:00:01 loss: 0.0827 data: 0.0016 Lr: 0.03734
2024-08-21 22:26:40.174 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][55/77] Data 0.028 (0.025) Batch 0.066 (0.066) Remain 00:00:01 loss: 0.0864 data: -0.0035 Lr: 0.03571
2024-08-21 22:26:40.174 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][55/77] Data 0.028 (0.027) Batch 0.066 (0.066) Remain 00:00:01 loss: 0.0864 data: -0.0121 Lr: 0.03571
2024-08-21 22:26:40.239 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][56/77] Data 0.027 (0.025) Batch 0.065 (0.066) Remain 00:00:01 loss: 0.0407 data: 0.0114 Lr: 0.03409
2024-08-21 22:26:40.239 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][56/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:01 loss: 0.0407 data: 0.0259 Lr: 0.03409
2024-08-21 22:26:40.307 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][57/77] Data 0.027 (0.025) Batch 0.068 (0.066) Remain 00:00:01 loss: 0.0745 data: -0.0014 Lr: 0.03247
2024-08-21 22:26:40.307 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][57/77] Data 0.031 (0.028) Batch 0.068 (0.066) Remain 00:00:01 loss: 0.0745 data: 0.0205 Lr: 0.03247
2024-08-21 22:26:40.371 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][58/77] Data 0.027 (0.025) Batch 0.064 (0.066) Remain 00:00:01 loss: 0.0920 data: -0.0027 Lr: 0.03084
2024-08-21 22:26:40.371 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][58/77] Data 0.027 (0.028) Batch 0.064 (0.066) Remain 00:00:01 loss: 0.0920 data: 0.0059 Lr: 0.03084
2024-08-21 22:26:40.435 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][59/77] Data 0.027 (0.025) Batch 0.064 (0.066) Remain 00:00:01 loss: 0.0819 data: 0.0132 Lr: 0.02922
2024-08-21 22:26:40.435 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][59/77] Data 0.027 (0.028) Batch 0.064 (0.066) Remain 00:00:01 loss: 0.0819 data: -0.0040 Lr: 0.02922
2024-08-21 22:26:40.509 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][60/77] Data 0.027 (0.025) Batch 0.074 (0.066) Remain 00:00:01 loss: 0.0944 data: -0.0033 Lr: 0.02760
2024-08-21 22:26:40.509 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][60/77] Data 0.021 (0.027) Batch 0.074 (0.066) Remain 00:00:01 loss: 0.0944 data: 0.0013 Lr: 0.02760
2024-08-21 22:26:40.588 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][61/77] Data 0.027 (0.025) Batch 0.079 (0.066) Remain 00:00:01 loss: 0.1111 data: -0.0037 Lr: 0.02597
2024-08-21 22:26:40.588 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_600
2024-08-21 22:26:40.588 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][61/77] Data 0.035 (0.028) Batch 0.079 (0.066) Remain 00:00:01 loss: 0.1111 data: -0.0081 Lr: 0.02597
2024-08-21 22:26:40.588 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_600
2024-08-21 22:26:40.612 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -582.36865234375
2024-08-21 22:26:40.612 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -281.17742919921875
2024-08-21 22:26:40.612 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -582.36865234375
2024-08-21 22:26:40.613 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -301.1912841796875
2024-08-21 22:26:40.689 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][62/77] Data 0.052 (0.025) Batch 0.101 (0.067) Remain 00:00:01 loss: 0.1209 data: 0.0063 Lr: 0.02435
2024-08-21 22:26:40.690 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][62/77] Data 0.060 (0.028) Batch 0.101 (0.067) Remain 00:00:01 loss: 0.1209 data: -0.0103 Lr: 0.02435
2024-08-21 22:26:40.760 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][63/77] Data 0.028 (0.025) Batch 0.071 (0.067) Remain 00:00:01 loss: 0.0839 data: 0.0070 Lr: 0.02273
2024-08-21 22:26:40.760 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][63/77] Data 0.028 (0.028) Batch 0.071 (0.067) Remain 00:00:01 loss: 0.0839 data: 0.0227 Lr: 0.02273
2024-08-21 22:26:40.851 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][64/77] Data 0.028 (0.025) Batch 0.091 (0.067) Remain 00:00:00 loss: 0.0999 data: 0.0008 Lr: 0.02110
2024-08-21 22:26:40.851 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][64/77] Data 0.038 (0.028) Batch 0.091 (0.067) Remain 00:00:00 loss: 0.0999 data: -0.0184 Lr: 0.02110
2024-08-21 22:26:40.930 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][65/77] Data 0.028 (0.026) Batch 0.079 (0.067) Remain 00:00:00 loss: 0.1116 data: 0.0134 Lr: 0.01948
2024-08-21 22:26:40.930 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][65/77] Data 0.037 (0.028) Batch 0.079 (0.067) Remain 00:00:00 loss: 0.1116 data: -0.0000 Lr: 0.01948
2024-08-21 22:26:40.997 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][66/77] Data 0.027 (0.026) Batch 0.067 (0.067) Remain 00:00:00 loss: 0.1193 data: -0.0271 Lr: 0.01786
2024-08-21 22:26:40.997 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][66/77] Data 0.029 (0.028) Batch 0.067 (0.067) Remain 00:00:00 loss: 0.1193 data: 0.0295 Lr: 0.01786
2024-08-21 22:26:41.062 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][67/77] Data 0.027 (0.026) Batch 0.065 (0.067) Remain 00:00:00 loss: 0.0628 data: 0.0089 Lr: 0.01623
2024-08-21 22:26:41.062 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][67/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:00 loss: 0.0628 data: -0.0121 Lr: 0.01623
2024-08-21 22:26:41.129 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][68/77] Data 0.027 (0.026) Batch 0.067 (0.067) Remain 00:00:00 loss: 0.1181 data: 0.0155 Lr: 0.01461
2024-08-21 22:26:41.129 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][68/77] Data 0.027 (0.028) Batch 0.067 (0.067) Remain 00:00:00 loss: 0.1181 data: 0.0026 Lr: 0.01461
2024-08-21 22:26:41.193 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][69/77] Data 0.027 (0.026) Batch 0.064 (0.067) Remain 00:00:00 loss: 0.0372 data: -0.0251 Lr: 0.01299
2024-08-21 22:26:41.193 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][69/77] Data 0.027 (0.028) Batch 0.064 (0.067) Remain 00:00:00 loss: 0.0372 data: -0.0171 Lr: 0.01299
2024-08-21 22:26:41.259 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][70/77] Data 0.027 (0.026) Batch 0.066 (0.067) Remain 00:00:00 loss: 0.0785 data: -0.0143 Lr: 0.01136
2024-08-21 22:26:41.259 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][70/77] Data 0.027 (0.028) Batch 0.066 (0.067) Remain 00:00:00 loss: 0.0785 data: 0.0014 Lr: 0.01136
2024-08-21 22:26:41.324 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][71/77] Data 0.027 (0.026) Batch 0.065 (0.067) Remain 00:00:00 loss: 0.0458 data: -0.0263 Lr: 0.00974
2024-08-21 22:26:41.324 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][71/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:00 loss: 0.0458 data: -0.0020 Lr: 0.00974
2024-08-21 22:26:41.390 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][72/77] Data 0.027 (0.026) Batch 0.067 (0.067) Remain 00:00:00 loss: 0.1089 data: 0.0012 Lr: 0.00812
2024-08-21 22:26:41.390 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][72/77] Data 0.027 (0.028) Batch 0.067 (0.067) Remain 00:00:00 loss: 0.1089 data: -0.0104 Lr: 0.00812
2024-08-21 22:26:41.460 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][73/77] Data 0.028 (0.026) Batch 0.070 (0.067) Remain 00:00:00 loss: 0.0840 data: -0.0014 Lr: 0.00649
2024-08-21 22:26:41.460 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][73/77] Data 0.028 (0.028) Batch 0.070 (0.067) Remain 00:00:00 loss: 0.0840 data: -0.0032 Lr: 0.00649
2024-08-21 22:26:41.530 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][74/77] Data 0.028 (0.026) Batch 0.070 (0.067) Remain 00:00:00 loss: 0.0875 data: 0.0047 Lr: 0.00487
2024-08-21 22:26:41.530 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][74/77] Data 0.028 (0.028) Batch 0.070 (0.067) Remain 00:00:00 loss: 0.0875 data: -0.0035 Lr: 0.00487
2024-08-21 22:26:41.601 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][75/77] Data 0.028 (0.026) Batch 0.071 (0.067) Remain 00:00:00 loss: 0.0517 data: -0.0082 Lr: 0.00325
2024-08-21 22:26:41.601 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][75/77] Data 0.028 (0.028) Batch 0.071 (0.067) Remain 00:00:00 loss: 0.0517 data: -0.0162 Lr: 0.00325
2024-08-21 22:26:41.673 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][76/77] Data 0.028 (0.026) Batch 0.072 (0.067) Remain 00:00:00 loss: 0.0703 data: 0.0136 Lr: 0.00162
2024-08-21 22:26:41.673 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][76/77] Data 0.028 (0.028) Batch 0.072 (0.067) Remain 00:00:00 loss: 0.0703 data: 0.0065 Lr: 0.00162
2024-08-21 22:26:41.721 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][77/77] Data 0.032 (0.026) Batch 0.048 (0.067) Remain 00:00:00 loss: 0.1636 data: -0.0050 Lr: 0.00000
2024-08-21 22:26:41.721 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 22:26:41.721 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][77/77] Data 0.032 (0.028) Batch 0.048 (0.067) Remain 00:00:00 loss: 0.1636 data: 0.0072 Lr: 0.00000
2024-08-21 22:26:41.721 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 22:26:45.676 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0301, Accuracy: 0.9896
2024-08-21 22:26:45.677 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 22:26:45.676 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0301, Accuracy: 0.9896
2024-08-21 22:26:45.677 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 22:26:45.677 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 22:26:45.677 | INFO     | trim.callbacks.evaluator:on_training_phase_end:49 - Best mIoU: 0.9896, epoch at  7
2024-08-21 22:26:45.677 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 22:26:45.677 | INFO     | trim.callbacks.evaluator:on_training_phase_end:49 - Best mIoU: 0.9896, epoch at  7
