2024-08-21 21:57:48.931 | INFO     | __main__:configure_model:71 - => creating model ...
2024-08-21 21:57:48.966 | INFO     | __main__:configure_model:74 - Number of parameters: 1199882
2024-08-21 21:57:51.147 | INFO     | trim.callbacks.misc:resume:179 - => Resuming from checkpoint: output/step_100
2024-08-21 21:57:51.188 | INFO     | trim.callbacks.misc:resume:184 - ### Model weight: -333.2380676269531
2024-08-21 21:57:51.188 | INFO     | trim.callbacks.misc:resume:185 - ### Optimizer weight: -169.3326416015625
2024-08-21 21:58:00.618 | INFO     | trim.callbacks.misc:on_training_phase_start:70 - Namespace(block_size=None, checkpointing_steps='300', gamma=0.7, load_best_model=False, log_project='accl_test', log_tag='init_1', lr=1.0, lr_scheduler_type='linear', num_train_epochs=8, num_warmup_steps=0, output_dir='./output', per_device_eval_batch_size=1, per_device_train_batch_size=196, preprocessing_num_workers=None, report_to='all', resume_from_checkpoint='output/step_300', save_path='output/', seed=1, weight_decay=0.0, with_tracking=False)
2024-08-21 21:58:00.618 | INFO     | trim.engine.trainer:fit:125 - >>>>>>>>>>>>>>>> Start Training >>>>>>>>>>>>>>>>
2024-08-21 21:58:01.669 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][24/77] Data 0.045 (0.045) Batch 1.050 (1.050) Remain 00:09:01 loss: 0.2740 data: 0.0237 Lr: 0.83604
2024-08-21 21:58:01.737 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][25/77] Data 0.031 (0.031) Batch 0.068 (0.068) Remain 00:00:35 loss: 0.2616 data: -0.0124 Lr: 0.83442
2024-08-21 21:58:01.801 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][26/77] Data 0.027 (0.029) Batch 0.064 (0.066) Remain 00:00:33 loss: 0.2155 data: 0.0091 Lr: 0.83279
2024-08-21 21:58:01.866 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][27/77] Data 0.027 (0.028) Batch 0.065 (0.066) Remain 00:00:33 loss: 0.2735 data: -0.0085 Lr: 0.83117
2024-08-21 21:58:01.930 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][28/77] Data 0.027 (0.028) Batch 0.064 (0.065) Remain 00:00:33 loss: 0.3534 data: -0.0011 Lr: 0.82955
2024-08-21 21:58:01.994 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][29/77] Data 0.027 (0.028) Batch 0.064 (0.065) Remain 00:00:33 loss: 0.2840 data: 0.0135 Lr: 0.82792
2024-08-21 21:58:02.058 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][30/77] Data 0.027 (0.028) Batch 0.064 (0.065) Remain 00:00:33 loss: 0.3001 data: 0.0096 Lr: 0.82630
2024-08-21 21:58:02.122 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][31/77] Data 0.027 (0.028) Batch 0.064 (0.065) Remain 00:00:32 loss: 0.2262 data: 0.0054 Lr: 0.82468
2024-08-21 21:58:02.187 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][32/77] Data 0.029 (0.028) Batch 0.065 (0.065) Remain 00:00:32 loss: 0.2708 data: -0.0337 Lr: 0.82305
2024-08-21 21:58:02.251 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][33/77] Data 0.027 (0.028) Batch 0.064 (0.065) Remain 00:00:32 loss: 0.2878 data: -0.0012 Lr: 0.82143
2024-08-21 21:58:02.316 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][34/77] Data 0.027 (0.028) Batch 0.065 (0.065) Remain 00:00:32 loss: 0.2708 data: 0.0190 Lr: 0.81981
2024-08-21 21:58:02.380 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][35/77] Data 0.027 (0.028) Batch 0.064 (0.065) Remain 00:00:32 loss: 0.3000 data: -0.0044 Lr: 0.81818
2024-08-21 21:58:02.445 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][36/77] Data 0.027 (0.028) Batch 0.065 (0.065) Remain 00:00:32 loss: 0.2408 data: -0.0074 Lr: 0.81656
2024-08-21 21:58:02.510 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][37/77] Data 0.027 (0.028) Batch 0.065 (0.065) Remain 00:00:32 loss: 0.2486 data: -0.0128 Lr: 0.81494
2024-08-21 21:58:02.575 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][38/77] Data 0.027 (0.028) Batch 0.066 (0.065) Remain 00:00:32 loss: 0.3245 data: 0.0128 Lr: 0.81331
2024-08-21 21:58:02.641 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][39/77] Data 0.027 (0.028) Batch 0.066 (0.065) Remain 00:00:32 loss: 0.3122 data: 0.0293 Lr: 0.81169
2024-08-21 21:58:02.707 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][40/77] Data 0.030 (0.028) Batch 0.066 (0.065) Remain 00:00:32 loss: 0.2672 data: -0.0067 Lr: 0.81006
2024-08-21 21:58:02.772 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][41/77] Data 0.027 (0.028) Batch 0.065 (0.065) Remain 00:00:32 loss: 0.2385 data: -0.0067 Lr: 0.80844
2024-08-21 21:58:02.846 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][42/77] Data 0.028 (0.028) Batch 0.074 (0.065) Remain 00:00:32 loss: 0.2943 data: 0.0099 Lr: 0.80682
2024-08-21 21:58:02.936 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][43/77] Data 0.035 (0.028) Batch 0.090 (0.067) Remain 00:00:33 loss: 0.2051 data: -0.0086 Lr: 0.80519
2024-08-21 21:58:03.001 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][44/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:33 loss: 0.2940 data: 0.0017 Lr: 0.80357
2024-08-21 21:58:03.080 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][45/77] Data 0.027 (0.028) Batch 0.079 (0.067) Remain 00:00:33 loss: 0.2053 data: -0.0113 Lr: 0.80195
2024-08-21 21:58:03.146 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][46/77] Data 0.027 (0.028) Batch 0.066 (0.067) Remain 00:00:33 loss: 0.2755 data: -0.0087 Lr: 0.80032
2024-08-21 21:58:03.213 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][47/77] Data 0.027 (0.028) Batch 0.066 (0.067) Remain 00:00:33 loss: 0.2009 data: 0.0070 Lr: 0.79870
2024-08-21 21:58:03.294 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][48/77] Data 0.027 (0.028) Batch 0.082 (0.068) Remain 00:00:33 loss: 0.2606 data: -0.0072 Lr: 0.79708
2024-08-21 21:58:03.361 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][49/77] Data 0.027 (0.028) Batch 0.067 (0.068) Remain 00:00:33 loss: 0.2353 data: 0.0042 Lr: 0.79545
2024-08-21 21:58:03.436 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][50/77] Data 0.033 (0.028) Batch 0.075 (0.068) Remain 00:00:33 loss: 0.1745 data: -0.0272 Lr: 0.79383
2024-08-21 21:58:03.501 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][51/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:33 loss: 0.1936 data: -0.0063 Lr: 0.79221
2024-08-21 21:58:03.566 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][52/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:33 loss: 0.2349 data: -0.0056 Lr: 0.79058
2024-08-21 21:58:03.631 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][53/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:32 loss: 0.3076 data: 0.0066 Lr: 0.78896
2024-08-21 21:58:03.696 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][54/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:32 loss: 0.2469 data: 0.0115 Lr: 0.78734
2024-08-21 21:58:03.761 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][55/77] Data 0.027 (0.028) Batch 0.064 (0.067) Remain 00:00:32 loss: 0.1617 data: 0.0112 Lr: 0.78571
2024-08-21 21:58:03.825 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][56/77] Data 0.027 (0.028) Batch 0.064 (0.067) Remain 00:00:32 loss: 0.2798 data: 0.0039 Lr: 0.78409
2024-08-21 21:58:03.891 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][57/77] Data 0.027 (0.028) Batch 0.066 (0.067) Remain 00:00:32 loss: 0.1543 data: -0.0106 Lr: 0.78247
2024-08-21 21:58:03.956 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][58/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:32 loss: 0.1863 data: -0.0061 Lr: 0.78084
2024-08-21 21:58:04.021 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][59/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:32 loss: 0.2062 data: 0.0112 Lr: 0.77922
2024-08-21 21:58:04.086 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][60/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:32 loss: 0.1990 data: -0.0083 Lr: 0.77760
2024-08-21 21:58:04.152 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][61/77] Data 0.027 (0.028) Batch 0.066 (0.067) Remain 00:00:32 loss: 0.2191 data: 0.0105 Lr: 0.77597
2024-08-21 21:58:04.221 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][62/77] Data 0.027 (0.028) Batch 0.069 (0.067) Remain 00:00:32 loss: 0.3397 data: 0.0028 Lr: 0.77435
2024-08-21 21:58:04.292 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][63/77] Data 0.028 (0.028) Batch 0.071 (0.067) Remain 00:00:32 loss: 0.1951 data: 0.0114 Lr: 0.77273
2024-08-21 21:58:04.362 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][64/77] Data 0.028 (0.028) Batch 0.071 (0.067) Remain 00:00:32 loss: 0.2513 data: -0.0030 Lr: 0.77110
2024-08-21 21:58:04.433 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][65/77] Data 0.028 (0.028) Batch 0.070 (0.067) Remain 00:00:32 loss: 0.1405 data: 0.0200 Lr: 0.76948
2024-08-21 21:58:04.503 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][66/77] Data 0.028 (0.028) Batch 0.070 (0.067) Remain 00:00:31 loss: 0.2146 data: -0.0093 Lr: 0.76786
2024-08-21 21:58:04.573 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][67/77] Data 0.028 (0.028) Batch 0.070 (0.068) Remain 00:00:31 loss: 0.2298 data: -0.0030 Lr: 0.76623
2024-08-21 21:58:04.640 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][68/77] Data 0.028 (0.028) Batch 0.067 (0.068) Remain 00:00:31 loss: 0.1592 data: -0.0148 Lr: 0.76461
2024-08-21 21:58:04.706 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][69/77] Data 0.027 (0.028) Batch 0.066 (0.067) Remain 00:00:31 loss: 0.2260 data: -0.0094 Lr: 0.76299
2024-08-21 21:58:04.773 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][70/77] Data 0.027 (0.028) Batch 0.067 (0.067) Remain 00:00:31 loss: 0.1909 data: -0.0026 Lr: 0.76136
2024-08-21 21:58:04.839 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][71/77] Data 0.027 (0.028) Batch 0.066 (0.067) Remain 00:00:31 loss: 0.1467 data: 0.0111 Lr: 0.75974
2024-08-21 21:58:04.910 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][72/77] Data 0.028 (0.028) Batch 0.071 (0.068) Remain 00:00:31 loss: 0.2110 data: 0.0086 Lr: 0.75812
2024-08-21 21:58:04.982 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][73/77] Data 0.028 (0.028) Batch 0.072 (0.068) Remain 00:00:31 loss: 0.2567 data: 0.0017 Lr: 0.75649
2024-08-21 21:58:04.982 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_150
2024-08-21 21:58:05.020 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -392.3818359375
2024-08-21 21:58:05.021 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -184.99835205078125
2024-08-21 21:58:05.093 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][74/77] Data 0.067 (0.029) Batch 0.111 (0.068) Remain 00:00:31 loss: 0.2293 data: -0.0004 Lr: 0.75487
2024-08-21 21:58:05.159 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][75/77] Data 0.027 (0.029) Batch 0.066 (0.068) Remain 00:00:31 loss: 0.1979 data: -0.0026 Lr: 0.75325
2024-08-21 21:58:05.227 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][76/77] Data 0.027 (0.029) Batch 0.066 (0.068) Remain 00:00:31 loss: 0.1497 data: -0.0095 Lr: 0.75162
2024-08-21 21:58:05.271 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [2/8][77/77] Data 0.032 (0.029) Batch 0.045 (0.068) Remain 00:00:31 loss: 0.2286 data: -0.0129 Lr: 0.75000
2024-08-21 21:58:05.271 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 21:58:09.449 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0645, Accuracy: 0.9782
2024-08-21 21:58:09.450 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 21:58:09.454 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 21:58:09.540 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][1/77] Data 0.043 (0.043) Batch 0.086 (0.086) Remain 00:00:39 loss: 0.1560 data: -0.0047 Lr: 0.74838
2024-08-21 21:58:09.603 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][2/77] Data 0.027 (0.027) Batch 0.063 (0.063) Remain 00:00:29 loss: 0.1703 data: -0.0086 Lr: 0.74675
2024-08-21 21:58:09.667 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][3/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:29 loss: 0.1995 data: 0.0005 Lr: 0.74513
2024-08-21 21:58:09.732 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][4/77] Data 0.027 (0.027) Batch 0.065 (0.064) Remain 00:00:29 loss: 0.2562 data: -0.0244 Lr: 0.74351
2024-08-21 21:58:09.795 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][5/77] Data 0.027 (0.027) Batch 0.063 (0.064) Remain 00:00:29 loss: 0.1551 data: 0.0004 Lr: 0.74188
2024-08-21 21:58:09.859 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][6/77] Data 0.027 (0.027) Batch 0.063 (0.064) Remain 00:00:29 loss: 0.1571 data: 0.0166 Lr: 0.74026
2024-08-21 21:58:09.922 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][7/77] Data 0.027 (0.027) Batch 0.063 (0.064) Remain 00:00:29 loss: 0.1945 data: -0.0050 Lr: 0.73864
2024-08-21 21:58:09.985 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][8/77] Data 0.027 (0.027) Batch 0.063 (0.064) Remain 00:00:28 loss: 0.2468 data: -0.0056 Lr: 0.73701
2024-08-21 21:58:10.048 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][9/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:28 loss: 0.1940 data: 0.0029 Lr: 0.73539
2024-08-21 21:58:10.112 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][10/77] Data 0.027 (0.027) Batch 0.063 (0.064) Remain 00:00:28 loss: 0.1218 data: 0.0060 Lr: 0.73377
2024-08-21 21:58:10.175 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][11/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:28 loss: 0.2129 data: 0.0084 Lr: 0.73214
2024-08-21 21:58:10.238 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][12/77] Data 0.027 (0.027) Batch 0.063 (0.064) Remain 00:00:28 loss: 0.1417 data: 0.0143 Lr: 0.73052
2024-08-21 21:58:10.302 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][13/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:28 loss: 0.2044 data: 0.0112 Lr: 0.72890
2024-08-21 21:58:10.365 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][14/77] Data 0.027 (0.027) Batch 0.063 (0.063) Remain 00:00:28 loss: 0.2008 data: 0.0104 Lr: 0.72727
2024-08-21 21:58:10.428 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][15/77] Data 0.027 (0.027) Batch 0.064 (0.063) Remain 00:00:28 loss: 0.1858 data: -0.0038 Lr: 0.72565
2024-08-21 21:58:10.494 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][16/77] Data 0.027 (0.027) Batch 0.065 (0.064) Remain 00:00:28 loss: 0.1904 data: -0.0055 Lr: 0.72403
2024-08-21 21:58:10.558 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][17/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:28 loss: 0.2262 data: -0.0110 Lr: 0.72240
2024-08-21 21:58:10.623 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][18/77] Data 0.027 (0.027) Batch 0.065 (0.064) Remain 00:00:28 loss: 0.2132 data: 0.0192 Lr: 0.72078
2024-08-21 21:58:10.701 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][19/77] Data 0.028 (0.027) Batch 0.077 (0.064) Remain 00:00:28 loss: 0.1791 data: 0.0032 Lr: 0.71916
2024-08-21 21:58:10.775 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][20/77] Data 0.031 (0.027) Batch 0.074 (0.065) Remain 00:00:28 loss: 0.1981 data: 0.0024 Lr: 0.71753
2024-08-21 21:58:10.842 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][21/77] Data 0.027 (0.027) Batch 0.067 (0.065) Remain 00:00:28 loss: 0.2554 data: -0.0058 Lr: 0.71591
2024-08-21 21:58:10.906 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][22/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:28 loss: 0.1550 data: 0.0166 Lr: 0.71429
2024-08-21 21:58:10.971 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][23/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:28 loss: 0.1678 data: 0.0021 Lr: 0.71266
2024-08-21 21:58:11.037 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][24/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:28 loss: 0.1463 data: -0.0047 Lr: 0.71104
2024-08-21 21:58:11.102 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][25/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:28 loss: 0.1823 data: -0.0039 Lr: 0.70942
2024-08-21 21:58:11.168 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][26/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:28 loss: 0.1986 data: 0.0041 Lr: 0.70779
2024-08-21 21:58:11.233 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][27/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:28 loss: 0.1605 data: -0.0054 Lr: 0.70617
2024-08-21 21:58:11.299 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][28/77] Data 0.027 (0.027) Batch 0.066 (0.065) Remain 00:00:28 loss: 0.1940 data: 0.0072 Lr: 0.70455
2024-08-21 21:58:11.364 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][29/77] Data 0.027 (0.027) Batch 0.066 (0.065) Remain 00:00:28 loss: 0.0916 data: -0.0074 Lr: 0.70292
2024-08-21 21:58:11.429 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][30/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:28 loss: 0.1980 data: 0.0148 Lr: 0.70130
2024-08-21 21:58:11.495 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][31/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:28 loss: 0.1260 data: -0.0003 Lr: 0.69968
2024-08-21 21:58:11.561 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][32/77] Data 0.027 (0.027) Batch 0.066 (0.065) Remain 00:00:28 loss: 0.0970 data: -0.0023 Lr: 0.69805
2024-08-21 21:58:11.626 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][33/77] Data 0.027 (0.027) Batch 0.066 (0.065) Remain 00:00:28 loss: 0.1696 data: -0.0035 Lr: 0.69643
2024-08-21 21:58:11.691 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][34/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:27 loss: 0.1278 data: -0.0073 Lr: 0.69481
2024-08-21 21:58:11.758 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][35/77] Data 0.027 (0.027) Batch 0.067 (0.065) Remain 00:00:27 loss: 0.1600 data: 0.0062 Lr: 0.69318
2024-08-21 21:58:11.839 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][36/77] Data 0.034 (0.027) Batch 0.081 (0.066) Remain 00:00:28 loss: 0.0735 data: -0.0185 Lr: 0.69156
2024-08-21 21:58:11.924 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][37/77] Data 0.038 (0.028) Batch 0.085 (0.066) Remain 00:00:28 loss: 0.2311 data: 0.0076 Lr: 0.68994
2024-08-21 21:58:12.010 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][38/77] Data 0.034 (0.028) Batch 0.086 (0.067) Remain 00:00:28 loss: 0.1634 data: 0.0169 Lr: 0.68831
2024-08-21 21:58:12.087 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][39/77] Data 0.030 (0.028) Batch 0.077 (0.067) Remain 00:00:28 loss: 0.2429 data: 0.0041 Lr: 0.68669
2024-08-21 21:58:12.152 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][40/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:28 loss: 0.1088 data: -0.0162 Lr: 0.68506
2024-08-21 21:58:12.217 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][41/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:28 loss: 0.3055 data: -0.0118 Lr: 0.68344
2024-08-21 21:58:12.289 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][42/77] Data 0.033 (0.028) Batch 0.072 (0.067) Remain 00:00:28 loss: 0.1558 data: 0.0077 Lr: 0.68182
2024-08-21 21:58:12.355 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][43/77] Data 0.028 (0.028) Batch 0.066 (0.067) Remain 00:00:28 loss: 0.1888 data: 0.0034 Lr: 0.68019
2024-08-21 21:58:12.422 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][44/77] Data 0.027 (0.028) Batch 0.066 (0.067) Remain 00:00:28 loss: 0.1648 data: -0.0086 Lr: 0.67857
2024-08-21 21:58:12.489 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][45/77] Data 0.031 (0.028) Batch 0.067 (0.067) Remain 00:00:28 loss: 0.1708 data: 0.0038 Lr: 0.67695
2024-08-21 21:58:12.554 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][46/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:27 loss: 0.1462 data: -0.0025 Lr: 0.67532
2024-08-21 21:58:12.554 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_200
2024-08-21 21:58:12.586 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -469.1923522949219
2024-08-21 21:58:12.587 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -225.95529174804688
2024-08-21 21:58:12.654 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][47/77] Data 0.062 (0.029) Batch 0.100 (0.068) Remain 00:00:28 loss: 0.2029 data: 0.0173 Lr: 0.67370
2024-08-21 21:58:12.718 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][48/77] Data 0.027 (0.029) Batch 0.065 (0.068) Remain 00:00:28 loss: 0.2124 data: -0.0020 Lr: 0.67208
2024-08-21 21:58:12.783 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][49/77] Data 0.027 (0.029) Batch 0.064 (0.068) Remain 00:00:27 loss: 0.2100 data: -0.0034 Lr: 0.67045
2024-08-21 21:58:12.847 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][50/77] Data 0.027 (0.029) Batch 0.064 (0.067) Remain 00:00:27 loss: 0.1895 data: -0.0050 Lr: 0.66883
2024-08-21 21:58:12.912 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][51/77] Data 0.027 (0.029) Batch 0.064 (0.067) Remain 00:00:27 loss: 0.1988 data: -0.0151 Lr: 0.66721
2024-08-21 21:58:12.976 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][52/77] Data 0.027 (0.028) Batch 0.064 (0.067) Remain 00:00:27 loss: 0.1742 data: 0.0062 Lr: 0.66558
2024-08-21 21:58:13.044 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][53/77] Data 0.027 (0.028) Batch 0.068 (0.067) Remain 00:00:27 loss: 0.2328 data: -0.0032 Lr: 0.66396
2024-08-21 21:58:13.114 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][54/77] Data 0.027 (0.028) Batch 0.070 (0.067) Remain 00:00:27 loss: 0.1475 data: 0.0097 Lr: 0.66234
2024-08-21 21:58:13.179 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][55/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:27 loss: 0.1329 data: -0.0165 Lr: 0.66071
2024-08-21 21:58:13.256 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][56/77] Data 0.027 (0.028) Batch 0.077 (0.068) Remain 00:00:27 loss: 0.1111 data: -0.0279 Lr: 0.65909
2024-08-21 21:58:13.348 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][57/77] Data 0.027 (0.028) Batch 0.093 (0.068) Remain 00:00:27 loss: 0.1476 data: -0.0152 Lr: 0.65747
2024-08-21 21:58:13.427 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][58/77] Data 0.028 (0.028) Batch 0.078 (0.068) Remain 00:00:27 loss: 0.1861 data: -0.0062 Lr: 0.65584
2024-08-21 21:58:13.493 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][59/77] Data 0.027 (0.028) Batch 0.067 (0.068) Remain 00:00:27 loss: 0.2173 data: -0.0070 Lr: 0.65422
2024-08-21 21:58:13.557 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][60/77] Data 0.027 (0.028) Batch 0.064 (0.068) Remain 00:00:27 loss: 0.2204 data: -0.0038 Lr: 0.65260
2024-08-21 21:58:13.625 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][61/77] Data 0.027 (0.028) Batch 0.067 (0.068) Remain 00:00:27 loss: 0.2445 data: 0.0051 Lr: 0.65097
2024-08-21 21:58:13.695 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][62/77] Data 0.028 (0.028) Batch 0.071 (0.068) Remain 00:00:27 loss: 0.0693 data: -0.0009 Lr: 0.64935
2024-08-21 21:58:13.768 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][63/77] Data 0.029 (0.028) Batch 0.073 (0.068) Remain 00:00:27 loss: 0.0875 data: -0.0041 Lr: 0.64773
2024-08-21 21:58:13.840 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][64/77] Data 0.029 (0.028) Batch 0.072 (0.068) Remain 00:00:27 loss: 0.1058 data: -0.0126 Lr: 0.64610
2024-08-21 21:58:13.913 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][65/77] Data 0.028 (0.028) Batch 0.072 (0.068) Remain 00:00:27 loss: 0.0880 data: -0.0044 Lr: 0.64448
2024-08-21 21:58:13.986 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][66/77] Data 0.029 (0.028) Batch 0.073 (0.068) Remain 00:00:27 loss: 0.1325 data: -0.0004 Lr: 0.64286
2024-08-21 21:58:14.056 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][67/77] Data 0.028 (0.028) Batch 0.070 (0.068) Remain 00:00:27 loss: 0.0982 data: -0.0107 Lr: 0.64123
2024-08-21 21:58:14.123 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][68/77] Data 0.028 (0.028) Batch 0.066 (0.068) Remain 00:00:27 loss: 0.2183 data: 0.0190 Lr: 0.63961
2024-08-21 21:58:14.191 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][69/77] Data 0.027 (0.028) Batch 0.069 (0.068) Remain 00:00:26 loss: 0.2367 data: 0.0135 Lr: 0.63799
2024-08-21 21:58:14.264 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][70/77] Data 0.028 (0.028) Batch 0.073 (0.068) Remain 00:00:26 loss: 0.1600 data: 0.0258 Lr: 0.63636
2024-08-21 21:58:14.334 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][71/77] Data 0.028 (0.028) Batch 0.070 (0.068) Remain 00:00:26 loss: 0.1454 data: 0.0188 Lr: 0.63474
2024-08-21 21:58:14.400 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][72/77] Data 0.028 (0.028) Batch 0.066 (0.068) Remain 00:00:26 loss: 0.1619 data: -0.0225 Lr: 0.63312
2024-08-21 21:58:14.465 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][73/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:26 loss: 0.1828 data: 0.0146 Lr: 0.63149
2024-08-21 21:58:14.529 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][74/77] Data 0.027 (0.028) Batch 0.064 (0.068) Remain 00:00:26 loss: 0.1450 data: 0.0089 Lr: 0.62987
2024-08-21 21:58:14.595 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][75/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:26 loss: 0.1393 data: 0.0108 Lr: 0.62825
2024-08-21 21:58:14.665 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][76/77] Data 0.028 (0.028) Batch 0.070 (0.068) Remain 00:00:26 loss: 0.0998 data: -0.0068 Lr: 0.62662
2024-08-21 21:58:14.712 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [3/8][77/77] Data 0.031 (0.028) Batch 0.046 (0.068) Remain 00:00:26 loss: 0.1420 data: 0.0080 Lr: 0.62500
2024-08-21 21:58:14.712 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 21:58:19.570 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0477, Accuracy: 0.9838
2024-08-21 21:58:19.570 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 21:58:19.570 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 21:58:19.668 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][1/77] Data 0.044 (0.044) Batch 0.097 (0.097) Remain 00:00:37 loss: 0.2108 data: 0.0083 Lr: 0.62338
2024-08-21 21:58:19.732 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][2/77] Data 0.021 (0.021) Batch 0.064 (0.064) Remain 00:00:24 loss: 0.1533 data: 0.0093 Lr: 0.62175
2024-08-21 21:58:19.795 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][3/77] Data 0.026 (0.024) Batch 0.063 (0.064) Remain 00:00:24 loss: 0.1664 data: -0.0161 Lr: 0.62013
2024-08-21 21:58:19.859 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][4/77] Data 0.027 (0.025) Batch 0.063 (0.064) Remain 00:00:24 loss: 0.1156 data: 0.0085 Lr: 0.61851
2024-08-21 21:58:19.920 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][5/77] Data 0.026 (0.025) Batch 0.061 (0.063) Remain 00:00:24 loss: 0.2429 data: -0.0002 Lr: 0.61688
2024-08-21 21:58:19.984 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][6/77] Data 0.025 (0.025) Batch 0.064 (0.063) Remain 00:00:24 loss: 0.2039 data: -0.0010 Lr: 0.61526
2024-08-21 21:58:20.048 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][7/77] Data 0.026 (0.025) Batch 0.064 (0.063) Remain 00:00:24 loss: 0.1413 data: -0.0143 Lr: 0.61364
2024-08-21 21:58:20.113 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][8/77] Data 0.025 (0.025) Batch 0.064 (0.064) Remain 00:00:24 loss: 0.1110 data: 0.0009 Lr: 0.61201
2024-08-21 21:58:20.180 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][9/77] Data 0.026 (0.025) Batch 0.068 (0.064) Remain 00:00:24 loss: 0.0825 data: 0.0181 Lr: 0.61039
2024-08-21 21:58:20.245 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][10/77] Data 0.027 (0.025) Batch 0.065 (0.064) Remain 00:00:24 loss: 0.0958 data: -0.0050 Lr: 0.60877
2024-08-21 21:58:20.309 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][11/77] Data 0.027 (0.026) Batch 0.065 (0.064) Remain 00:00:24 loss: 0.0947 data: 0.0128 Lr: 0.60714
2024-08-21 21:58:20.374 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][12/77] Data 0.027 (0.026) Batch 0.065 (0.064) Remain 00:00:24 loss: 0.1205 data: 0.0025 Lr: 0.60552
2024-08-21 21:58:20.442 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][13/77] Data 0.027 (0.026) Batch 0.068 (0.064) Remain 00:00:24 loss: 0.1303 data: 0.0144 Lr: 0.60390
2024-08-21 21:58:20.506 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][14/77] Data 0.027 (0.026) Batch 0.064 (0.064) Remain 00:00:23 loss: 0.1313 data: -0.0103 Lr: 0.60227
2024-08-21 21:58:20.576 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][15/77] Data 0.027 (0.026) Batch 0.070 (0.065) Remain 00:00:24 loss: 0.1086 data: 0.0008 Lr: 0.60065
2024-08-21 21:58:20.642 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][16/77] Data 0.027 (0.026) Batch 0.066 (0.065) Remain 00:00:24 loss: 0.1408 data: -0.0114 Lr: 0.59903
2024-08-21 21:58:20.706 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][17/77] Data 0.027 (0.026) Batch 0.064 (0.065) Remain 00:00:23 loss: 0.1057 data: -0.0005 Lr: 0.59740
2024-08-21 21:58:20.771 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][18/77] Data 0.027 (0.026) Batch 0.065 (0.065) Remain 00:00:23 loss: 0.1254 data: 0.0003 Lr: 0.59578
2024-08-21 21:58:20.842 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][19/77] Data 0.027 (0.026) Batch 0.071 (0.065) Remain 00:00:23 loss: 0.1254 data: 0.0128 Lr: 0.59416
2024-08-21 21:58:20.842 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_250
2024-08-21 21:58:20.868 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -494.98760986328125
2024-08-21 21:58:20.868 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -241.2342529296875
2024-08-21 21:58:20.939 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][20/77] Data 0.054 (0.028) Batch 0.098 (0.067) Remain 00:00:24 loss: 0.1407 data: -0.0035 Lr: 0.59253
2024-08-21 21:58:21.019 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][21/77] Data 0.031 (0.028) Batch 0.079 (0.068) Remain 00:00:24 loss: 0.1860 data: 0.0170 Lr: 0.59091
2024-08-21 21:58:21.106 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][22/77] Data 0.037 (0.028) Batch 0.087 (0.068) Remain 00:00:24 loss: 0.2819 data: -0.0123 Lr: 0.58929
2024-08-21 21:58:21.187 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][23/77] Data 0.037 (0.029) Batch 0.081 (0.069) Remain 00:00:25 loss: 0.1165 data: 0.0074 Lr: 0.58766
2024-08-21 21:58:21.252 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][24/77] Data 0.027 (0.029) Batch 0.065 (0.069) Remain 00:00:24 loss: 0.1336 data: 0.0008 Lr: 0.58604
2024-08-21 21:58:21.322 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][25/77] Data 0.027 (0.029) Batch 0.069 (0.069) Remain 00:00:24 loss: 0.2333 data: 0.0049 Lr: 0.58442
2024-08-21 21:58:21.405 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][26/77] Data 0.036 (0.029) Batch 0.084 (0.070) Remain 00:00:25 loss: 0.1357 data: 0.0082 Lr: 0.58279
2024-08-21 21:58:21.479 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][27/77] Data 0.035 (0.029) Batch 0.073 (0.070) Remain 00:00:25 loss: 0.1543 data: -0.0030 Lr: 0.58117
2024-08-21 21:58:21.543 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][28/77] Data 0.027 (0.029) Batch 0.065 (0.069) Remain 00:00:24 loss: 0.1353 data: 0.0003 Lr: 0.57955
2024-08-21 21:58:21.608 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][29/77] Data 0.027 (0.029) Batch 0.065 (0.069) Remain 00:00:24 loss: 0.2058 data: 0.0033 Lr: 0.57792
2024-08-21 21:58:21.673 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][30/77] Data 0.027 (0.029) Batch 0.065 (0.069) Remain 00:00:24 loss: 0.1120 data: -0.0038 Lr: 0.57630
2024-08-21 21:58:21.737 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][31/77] Data 0.027 (0.029) Batch 0.064 (0.069) Remain 00:00:24 loss: 0.1285 data: -0.0020 Lr: 0.57468
2024-08-21 21:58:21.802 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][32/77] Data 0.027 (0.029) Batch 0.065 (0.069) Remain 00:00:24 loss: 0.1136 data: 0.0069 Lr: 0.57305
2024-08-21 21:58:21.866 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][33/77] Data 0.027 (0.029) Batch 0.064 (0.069) Remain 00:00:24 loss: 0.1860 data: 0.0004 Lr: 0.57143
2024-08-21 21:58:21.932 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][34/77] Data 0.027 (0.029) Batch 0.065 (0.069) Remain 00:00:24 loss: 0.1481 data: -0.0012 Lr: 0.56981
2024-08-21 21:58:21.996 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][35/77] Data 0.027 (0.029) Batch 0.064 (0.068) Remain 00:00:24 loss: 0.0829 data: -0.0101 Lr: 0.56818
2024-08-21 21:58:22.063 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][36/77] Data 0.027 (0.029) Batch 0.066 (0.068) Remain 00:00:23 loss: 0.1112 data: -0.0069 Lr: 0.56656
2024-08-21 21:58:22.128 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][37/77] Data 0.027 (0.029) Batch 0.066 (0.068) Remain 00:00:23 loss: 0.2661 data: -0.0185 Lr: 0.56494
2024-08-21 21:58:22.195 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][38/77] Data 0.027 (0.029) Batch 0.066 (0.068) Remain 00:00:23 loss: 0.1117 data: -0.0115 Lr: 0.56331
2024-08-21 21:58:22.260 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][39/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:23 loss: 0.1327 data: 0.0002 Lr: 0.56169
2024-08-21 21:58:22.326 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][40/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:23 loss: 0.1525 data: 0.0048 Lr: 0.56006
2024-08-21 21:58:22.391 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][41/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:23 loss: 0.1518 data: -0.0055 Lr: 0.55844
2024-08-21 21:58:22.456 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][42/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:23 loss: 0.1008 data: -0.0115 Lr: 0.55682
2024-08-21 21:58:22.520 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][43/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:23 loss: 0.1352 data: 0.0048 Lr: 0.55519
2024-08-21 21:58:22.586 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][44/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:23 loss: 0.1462 data: -0.0127 Lr: 0.55357
2024-08-21 21:58:22.653 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][45/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:23 loss: 0.1539 data: 0.0013 Lr: 0.55195
2024-08-21 21:58:22.719 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][46/77] Data 0.027 (0.028) Batch 0.067 (0.068) Remain 00:00:23 loss: 0.0927 data: -0.0181 Lr: 0.55032
2024-08-21 21:58:22.785 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][47/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:22 loss: 0.1474 data: -0.0068 Lr: 0.54870
2024-08-21 21:58:22.849 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][48/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:22 loss: 0.1190 data: -0.0043 Lr: 0.54708
2024-08-21 21:58:22.915 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][49/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:22 loss: 0.1037 data: -0.0033 Lr: 0.54545
2024-08-21 21:58:22.980 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][50/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:22 loss: 0.1525 data: 0.0039 Lr: 0.54383
2024-08-21 21:58:23.043 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][51/77] Data 0.027 (0.028) Batch 0.063 (0.067) Remain 00:00:22 loss: 0.1970 data: 0.0043 Lr: 0.54221
2024-08-21 21:58:23.108 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][52/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:22 loss: 0.1296 data: 0.0039 Lr: 0.54058
2024-08-21 21:58:23.172 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][53/77] Data 0.027 (0.028) Batch 0.064 (0.067) Remain 00:00:22 loss: 0.1332 data: 0.0042 Lr: 0.53896
2024-08-21 21:58:23.236 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][54/77] Data 0.027 (0.028) Batch 0.064 (0.067) Remain 00:00:22 loss: 0.1724 data: 0.0206 Lr: 0.53734
2024-08-21 21:58:23.301 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][55/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:22 loss: 0.1911 data: -0.0055 Lr: 0.53571
2024-08-21 21:58:23.366 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][56/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:22 loss: 0.1428 data: 0.0123 Lr: 0.53409
2024-08-21 21:58:23.431 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][57/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:22 loss: 0.1644 data: 0.0129 Lr: 0.53247
2024-08-21 21:58:23.497 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][58/77] Data 0.027 (0.028) Batch 0.066 (0.067) Remain 00:00:22 loss: 0.1334 data: 0.0061 Lr: 0.53084
2024-08-21 21:58:23.562 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][59/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:21 loss: 0.2173 data: 0.0014 Lr: 0.52922
2024-08-21 21:58:23.627 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][60/77] Data 0.028 (0.028) Batch 0.065 (0.067) Remain 00:00:21 loss: 0.1143 data: -0.0059 Lr: 0.52760
2024-08-21 21:58:23.693 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][61/77] Data 0.027 (0.028) Batch 0.065 (0.067) Remain 00:00:21 loss: 0.1309 data: -0.0065 Lr: 0.52597
2024-08-21 21:58:23.757 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][62/77] Data 0.027 (0.028) Batch 0.064 (0.067) Remain 00:00:21 loss: 0.1024 data: -0.0120 Lr: 0.52435
2024-08-21 21:58:23.821 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][63/77] Data 0.027 (0.028) Batch 0.064 (0.067) Remain 00:00:21 loss: 0.1592 data: 0.0034 Lr: 0.52273
2024-08-21 21:58:23.886 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][64/77] Data 0.027 (0.028) Batch 0.064 (0.067) Remain 00:00:21 loss: 0.1333 data: -0.0036 Lr: 0.52110
2024-08-21 21:58:23.953 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][65/77] Data 0.027 (0.028) Batch 0.067 (0.067) Remain 00:00:21 loss: 0.1394 data: 0.0011 Lr: 0.51948
2024-08-21 21:58:24.018 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][66/77] Data 0.027 (0.028) Batch 0.064 (0.067) Remain 00:00:21 loss: 0.1903 data: -0.0039 Lr: 0.51786
2024-08-21 21:58:24.088 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][67/77] Data 0.027 (0.028) Batch 0.070 (0.067) Remain 00:00:21 loss: 0.1215 data: 0.0163 Lr: 0.51623
2024-08-21 21:58:24.155 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][68/77] Data 0.027 (0.028) Batch 0.068 (0.067) Remain 00:00:21 loss: 0.1009 data: -0.0100 Lr: 0.51461
2024-08-21 21:58:24.237 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][69/77] Data 0.027 (0.028) Batch 0.082 (0.067) Remain 00:00:21 loss: 0.1302 data: -0.0002 Lr: 0.51299
2024-08-21 21:58:24.237 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_300
2024-08-21 21:58:24.263 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -552.4868774414062
2024-08-21 21:58:24.263 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -264.9616394042969
2024-08-21 21:58:24.336 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][70/77] Data 0.054 (0.028) Batch 0.099 (0.068) Remain 00:00:21 loss: 0.1410 data: -0.0036 Lr: 0.51136
2024-08-21 21:58:24.404 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][71/77] Data 0.027 (0.028) Batch 0.068 (0.068) Remain 00:00:21 loss: 0.1015 data: 0.0014 Lr: 0.50974
2024-08-21 21:58:24.479 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][72/77] Data 0.027 (0.028) Batch 0.075 (0.068) Remain 00:00:21 loss: 0.1680 data: 0.0001 Lr: 0.50812
2024-08-21 21:58:24.543 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][73/77] Data 0.027 (0.028) Batch 0.064 (0.068) Remain 00:00:21 loss: 0.2154 data: -0.0077 Lr: 0.50649
2024-08-21 21:58:24.616 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][74/77] Data 0.027 (0.028) Batch 0.073 (0.068) Remain 00:00:21 loss: 0.2349 data: 0.0020 Lr: 0.50487
2024-08-21 21:58:24.680 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][75/77] Data 0.027 (0.028) Batch 0.064 (0.068) Remain 00:00:21 loss: 0.1724 data: 0.0091 Lr: 0.50325
2024-08-21 21:58:24.744 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][76/77] Data 0.027 (0.028) Batch 0.064 (0.068) Remain 00:00:20 loss: 0.1855 data: 0.0090 Lr: 0.50162
2024-08-21 21:58:24.785 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [4/8][77/77] Data 0.030 (0.028) Batch 0.040 (0.067) Remain 00:00:20 loss: 0.1891 data: 0.0059 Lr: 0.50000
2024-08-21 21:58:24.785 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 21:58:28.317 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0441, Accuracy: 0.9856
2024-08-21 21:58:28.318 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 21:58:28.318 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 21:58:28.396 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][1/77] Data 0.043 (0.043) Batch 0.078 (0.078) Remain 00:00:23 loss: 0.1400 data: -0.0048 Lr: 0.49838
2024-08-21 21:58:28.451 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][2/77] Data 0.023 (0.023) Batch 0.055 (0.055) Remain 00:00:16 loss: 0.0978 data: 0.0003 Lr: 0.49675
2024-08-21 21:58:28.506 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][3/77] Data 0.022 (0.022) Batch 0.055 (0.055) Remain 00:00:16 loss: 0.1569 data: -0.0084 Lr: 0.49513
2024-08-21 21:58:28.561 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][4/77] Data 0.022 (0.022) Batch 0.055 (0.055) Remain 00:00:16 loss: 0.1120 data: 0.0040 Lr: 0.49351
2024-08-21 21:58:28.616 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][5/77] Data 0.023 (0.022) Batch 0.055 (0.055) Remain 00:00:16 loss: 0.0880 data: 0.0098 Lr: 0.49188
2024-08-21 21:58:28.674 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][6/77] Data 0.022 (0.022) Batch 0.057 (0.056) Remain 00:00:16 loss: 0.1852 data: -0.0010 Lr: 0.49026
2024-08-21 21:58:28.729 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][7/77] Data 0.022 (0.022) Batch 0.056 (0.056) Remain 00:00:16 loss: 0.1671 data: -0.0009 Lr: 0.48864
2024-08-21 21:58:28.785 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][8/77] Data 0.022 (0.022) Batch 0.056 (0.056) Remain 00:00:16 loss: 0.1187 data: 0.0052 Lr: 0.48701
2024-08-21 21:58:28.841 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][9/77] Data 0.022 (0.022) Batch 0.056 (0.056) Remain 00:00:16 loss: 0.1214 data: -0.0215 Lr: 0.48539
2024-08-21 21:58:28.895 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][10/77] Data 0.022 (0.022) Batch 0.054 (0.055) Remain 00:00:16 loss: 0.0786 data: -0.0005 Lr: 0.48377
2024-08-21 21:58:28.951 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][11/77] Data 0.022 (0.022) Batch 0.056 (0.056) Remain 00:00:16 loss: 0.1566 data: 0.0047 Lr: 0.48214
2024-08-21 21:58:29.006 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][12/77] Data 0.022 (0.022) Batch 0.054 (0.055) Remain 00:00:16 loss: 0.1797 data: 0.0077 Lr: 0.48052
2024-08-21 21:58:29.061 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][13/77] Data 0.023 (0.022) Batch 0.056 (0.055) Remain 00:00:16 loss: 0.1708 data: -0.0017 Lr: 0.47890
2024-08-21 21:58:29.117 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][14/77] Data 0.022 (0.022) Batch 0.056 (0.055) Remain 00:00:16 loss: 0.0955 data: -0.0142 Lr: 0.47727
2024-08-21 21:58:29.172 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][15/77] Data 0.023 (0.022) Batch 0.055 (0.055) Remain 00:00:16 loss: 0.1413 data: 0.0090 Lr: 0.47565
2024-08-21 21:58:29.228 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][16/77] Data 0.022 (0.022) Batch 0.056 (0.056) Remain 00:00:16 loss: 0.0813 data: -0.0122 Lr: 0.47403
2024-08-21 21:58:29.283 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][17/77] Data 0.024 (0.022) Batch 0.055 (0.055) Remain 00:00:16 loss: 0.1047 data: 0.0150 Lr: 0.47240
2024-08-21 21:58:29.346 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][18/77] Data 0.021 (0.022) Batch 0.063 (0.056) Remain 00:00:16 loss: 0.1077 data: -0.0023 Lr: 0.47078
2024-08-21 21:58:29.410 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][19/77] Data 0.027 (0.023) Batch 0.064 (0.056) Remain 00:00:16 loss: 0.1664 data: -0.0049 Lr: 0.46916
2024-08-21 21:58:29.474 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][20/77] Data 0.027 (0.023) Batch 0.064 (0.057) Remain 00:00:16 loss: 0.0678 data: 0.0180 Lr: 0.46753
2024-08-21 21:58:29.541 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][21/77] Data 0.027 (0.023) Batch 0.067 (0.057) Remain 00:00:16 loss: 0.0849 data: 0.0004 Lr: 0.46591
2024-08-21 21:58:29.620 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][22/77] Data 0.038 (0.024) Batch 0.079 (0.058) Remain 00:00:16 loss: 0.1474 data: 0.0093 Lr: 0.46429
2024-08-21 21:58:29.688 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][23/77] Data 0.028 (0.024) Batch 0.068 (0.059) Remain 00:00:16 loss: 0.2380 data: -0.0026 Lr: 0.46266
2024-08-21 21:58:29.757 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][24/77] Data 0.027 (0.024) Batch 0.069 (0.059) Remain 00:00:16 loss: 0.1284 data: -0.0075 Lr: 0.46104
2024-08-21 21:58:29.832 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][25/77] Data 0.037 (0.025) Batch 0.075 (0.060) Remain 00:00:16 loss: 0.1101 data: -0.0091 Lr: 0.45942
2024-08-21 21:58:29.915 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][26/77] Data 0.037 (0.025) Batch 0.083 (0.061) Remain 00:00:17 loss: 0.1283 data: 0.0128 Lr: 0.45779
2024-08-21 21:58:29.981 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][27/77] Data 0.027 (0.025) Batch 0.066 (0.061) Remain 00:00:17 loss: 0.1117 data: 0.0105 Lr: 0.45617
2024-08-21 21:58:30.062 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][28/77] Data 0.035 (0.026) Batch 0.080 (0.062) Remain 00:00:17 loss: 0.1148 data: 0.0011 Lr: 0.45455
2024-08-21 21:58:30.145 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][29/77] Data 0.036 (0.026) Batch 0.084 (0.062) Remain 00:00:17 loss: 0.0620 data: 0.0052 Lr: 0.45292
2024-08-21 21:58:30.221 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][30/77] Data 0.030 (0.026) Batch 0.076 (0.063) Remain 00:00:17 loss: 0.0778 data: -0.0017 Lr: 0.45130
2024-08-21 21:58:30.298 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][31/77] Data 0.038 (0.026) Batch 0.077 (0.063) Remain 00:00:17 loss: 0.1026 data: 0.0064 Lr: 0.44968
2024-08-21 21:58:30.363 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][32/77] Data 0.022 (0.026) Batch 0.066 (0.063) Remain 00:00:17 loss: 0.0817 data: 0.0188 Lr: 0.44805
2024-08-21 21:58:30.427 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][33/77] Data 0.021 (0.026) Batch 0.064 (0.063) Remain 00:00:17 loss: 0.1285 data: 0.0007 Lr: 0.44643
2024-08-21 21:58:30.492 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][34/77] Data 0.021 (0.026) Batch 0.064 (0.064) Remain 00:00:17 loss: 0.1199 data: -0.0118 Lr: 0.44481
2024-08-21 21:58:30.557 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][35/77] Data 0.021 (0.026) Batch 0.065 (0.064) Remain 00:00:17 loss: 0.0672 data: 0.0114 Lr: 0.44318
2024-08-21 21:58:30.622 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][36/77] Data 0.021 (0.026) Batch 0.065 (0.064) Remain 00:00:17 loss: 0.0788 data: -0.0139 Lr: 0.44156
2024-08-21 21:58:30.686 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][37/77] Data 0.021 (0.026) Batch 0.065 (0.064) Remain 00:00:17 loss: 0.0428 data: 0.0010 Lr: 0.43994
2024-08-21 21:58:30.750 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][38/77] Data 0.021 (0.025) Batch 0.063 (0.064) Remain 00:00:17 loss: 0.0915 data: -0.0011 Lr: 0.43831
2024-08-21 21:58:30.813 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][39/77] Data 0.021 (0.025) Batch 0.063 (0.064) Remain 00:00:17 loss: 0.1537 data: -0.0121 Lr: 0.43669
2024-08-21 21:58:30.876 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][40/77] Data 0.021 (0.025) Batch 0.063 (0.064) Remain 00:00:17 loss: 0.1617 data: -0.0064 Lr: 0.43506
2024-08-21 21:58:30.939 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][41/77] Data 0.020 (0.025) Batch 0.063 (0.064) Remain 00:00:17 loss: 0.1698 data: 0.0134 Lr: 0.43344
2024-08-21 21:58:31.001 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][42/77] Data 0.021 (0.025) Batch 0.062 (0.064) Remain 00:00:16 loss: 0.0722 data: 0.0074 Lr: 0.43182
2024-08-21 21:58:31.002 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_350
2024-08-21 21:58:31.026 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -563.8388671875
2024-08-21 21:58:31.027 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -265.894775390625
2024-08-21 21:58:31.090 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][43/77] Data 0.046 (0.025) Batch 0.089 (0.064) Remain 00:00:17 loss: 0.1109 data: 0.0085 Lr: 0.43019
2024-08-21 21:58:31.153 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][44/77] Data 0.022 (0.025) Batch 0.063 (0.064) Remain 00:00:16 loss: 0.1602 data: 0.0050 Lr: 0.42857
2024-08-21 21:58:31.215 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][45/77] Data 0.021 (0.025) Batch 0.063 (0.064) Remain 00:00:16 loss: 0.0640 data: 0.0038 Lr: 0.42695
2024-08-21 21:58:31.278 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][46/77] Data 0.021 (0.025) Batch 0.063 (0.064) Remain 00:00:16 loss: 0.1053 data: -0.0016 Lr: 0.42532
2024-08-21 21:58:31.341 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][47/77] Data 0.021 (0.025) Batch 0.063 (0.064) Remain 00:00:16 loss: 0.1870 data: -0.0064 Lr: 0.42370
2024-08-21 21:58:31.403 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][48/77] Data 0.021 (0.025) Batch 0.063 (0.064) Remain 00:00:16 loss: 0.0696 data: -0.0077 Lr: 0.42208
2024-08-21 21:58:31.466 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][49/77] Data 0.021 (0.025) Batch 0.062 (0.064) Remain 00:00:16 loss: 0.0952 data: 0.0010 Lr: 0.42045
2024-08-21 21:58:31.530 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][50/77] Data 0.022 (0.025) Batch 0.064 (0.064) Remain 00:00:16 loss: 0.1227 data: 0.0031 Lr: 0.41883
2024-08-21 21:58:31.593 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][51/77] Data 0.021 (0.025) Batch 0.063 (0.064) Remain 00:00:16 loss: 0.2182 data: 0.0146 Lr: 0.41721
2024-08-21 21:58:31.656 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][52/77] Data 0.021 (0.025) Batch 0.063 (0.064) Remain 00:00:16 loss: 0.0911 data: -0.0058 Lr: 0.41558
2024-08-21 21:58:31.719 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][53/77] Data 0.022 (0.025) Batch 0.062 (0.064) Remain 00:00:16 loss: 0.1289 data: -0.0125 Lr: 0.41396
2024-08-21 21:58:31.783 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][54/77] Data 0.021 (0.025) Batch 0.064 (0.064) Remain 00:00:16 loss: 0.0768 data: -0.0028 Lr: 0.41234
2024-08-21 21:58:31.846 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][55/77] Data 0.022 (0.025) Batch 0.064 (0.064) Remain 00:00:16 loss: 0.0955 data: -0.0214 Lr: 0.41071
2024-08-21 21:58:31.910 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][56/77] Data 0.021 (0.025) Batch 0.064 (0.064) Remain 00:00:16 loss: 0.0689 data: -0.0066 Lr: 0.40909
2024-08-21 21:58:31.974 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][57/77] Data 0.022 (0.025) Batch 0.064 (0.064) Remain 00:00:16 loss: 0.1428 data: -0.0083 Lr: 0.40747
2024-08-21 21:58:32.038 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][58/77] Data 0.022 (0.024) Batch 0.064 (0.064) Remain 00:00:16 loss: 0.0544 data: -0.0136 Lr: 0.40584
2024-08-21 21:58:32.102 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][59/77] Data 0.022 (0.024) Batch 0.064 (0.064) Remain 00:00:15 loss: 0.0981 data: -0.0025 Lr: 0.40422
2024-08-21 21:58:32.166 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][60/77] Data 0.022 (0.024) Batch 0.064 (0.064) Remain 00:00:15 loss: 0.1388 data: -0.0015 Lr: 0.40260
2024-08-21 21:58:32.230 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][61/77] Data 0.021 (0.024) Batch 0.064 (0.064) Remain 00:00:15 loss: 0.1021 data: 0.0039 Lr: 0.40097
2024-08-21 21:58:32.297 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][62/77] Data 0.022 (0.024) Batch 0.067 (0.064) Remain 00:00:15 loss: 0.1163 data: -0.0047 Lr: 0.39935
2024-08-21 21:58:32.362 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][63/77] Data 0.022 (0.024) Batch 0.065 (0.064) Remain 00:00:15 loss: 0.1037 data: 0.0097 Lr: 0.39773
2024-08-21 21:58:32.427 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][64/77] Data 0.021 (0.024) Batch 0.065 (0.064) Remain 00:00:15 loss: 0.1277 data: 0.0087 Lr: 0.39610
2024-08-21 21:58:32.491 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][65/77] Data 0.021 (0.024) Batch 0.064 (0.064) Remain 00:00:15 loss: 0.1589 data: 0.0106 Lr: 0.39448
2024-08-21 21:58:32.555 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][66/77] Data 0.022 (0.024) Batch 0.064 (0.064) Remain 00:00:15 loss: 0.0973 data: -0.0167 Lr: 0.39286
2024-08-21 21:58:32.620 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][67/77] Data 0.021 (0.024) Batch 0.064 (0.064) Remain 00:00:15 loss: 0.1007 data: -0.0091 Lr: 0.39123
2024-08-21 21:58:32.684 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][68/77] Data 0.022 (0.024) Batch 0.064 (0.064) Remain 00:00:15 loss: 0.1212 data: 0.0082 Lr: 0.38961
2024-08-21 21:58:32.748 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][69/77] Data 0.020 (0.024) Batch 0.064 (0.064) Remain 00:00:15 loss: 0.1010 data: -0.0219 Lr: 0.38799
2024-08-21 21:58:32.812 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][70/77] Data 0.022 (0.024) Batch 0.064 (0.064) Remain 00:00:15 loss: 0.0982 data: -0.0131 Lr: 0.38636
2024-08-21 21:58:32.876 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][71/77] Data 0.022 (0.024) Batch 0.064 (0.064) Remain 00:00:15 loss: 0.1516 data: -0.0031 Lr: 0.38474
2024-08-21 21:58:32.940 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][72/77] Data 0.022 (0.024) Batch 0.064 (0.064) Remain 00:00:15 loss: 0.1054 data: 0.0094 Lr: 0.38312
2024-08-21 21:58:33.006 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][73/77] Data 0.021 (0.024) Batch 0.066 (0.064) Remain 00:00:15 loss: 0.0855 data: 0.0064 Lr: 0.38149
2024-08-21 21:58:33.074 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][74/77] Data 0.027 (0.024) Batch 0.068 (0.064) Remain 00:00:15 loss: 0.0731 data: 0.0072 Lr: 0.37987
2024-08-21 21:58:33.141 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][75/77] Data 0.027 (0.024) Batch 0.067 (0.064) Remain 00:00:15 loss: 0.0771 data: -0.0094 Lr: 0.37825
2024-08-21 21:58:33.211 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][76/77] Data 0.028 (0.024) Batch 0.069 (0.064) Remain 00:00:14 loss: 0.1124 data: 0.0090 Lr: 0.37662
2024-08-21 21:58:33.257 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [5/8][77/77] Data 0.031 (0.024) Batch 0.047 (0.064) Remain 00:00:14 loss: 0.1450 data: 0.0192 Lr: 0.37500
2024-08-21 21:58:33.258 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 21:58:37.838 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0333, Accuracy: 0.9884
2024-08-21 21:58:37.838 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 21:58:37.839 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 21:58:37.932 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][1/77] Data 0.055 (0.055) Batch 0.093 (0.093) Remain 00:00:21 loss: 0.0722 data: -0.0011 Lr: 0.37338
2024-08-21 21:58:37.997 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][2/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:14 loss: 0.1174 data: 0.0130 Lr: 0.37175
2024-08-21 21:58:38.061 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][3/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:14 loss: 0.0653 data: -0.0149 Lr: 0.37013
2024-08-21 21:58:38.126 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][4/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:14 loss: 0.1142 data: 0.0154 Lr: 0.36851
2024-08-21 21:58:38.190 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][5/77] Data 0.027 (0.027) Batch 0.064 (0.065) Remain 00:00:14 loss: 0.0680 data: 0.0002 Lr: 0.36688
2024-08-21 21:58:38.256 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][6/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:14 loss: 0.0491 data: -0.0313 Lr: 0.36526
2024-08-21 21:58:38.320 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][7/77] Data 0.027 (0.027) Batch 0.064 (0.065) Remain 00:00:14 loss: 0.2070 data: -0.0014 Lr: 0.36364
2024-08-21 21:58:38.384 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][8/77] Data 0.027 (0.027) Batch 0.064 (0.065) Remain 00:00:14 loss: 0.0932 data: 0.0011 Lr: 0.36201
2024-08-21 21:58:38.449 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][9/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:14 loss: 0.1344 data: 0.0141 Lr: 0.36039
2024-08-21 21:58:38.514 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][10/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:14 loss: 0.1796 data: 0.0063 Lr: 0.35877
2024-08-21 21:58:38.580 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][11/77] Data 0.027 (0.027) Batch 0.066 (0.065) Remain 00:00:14 loss: 0.0516 data: 0.0060 Lr: 0.35714
2024-08-21 21:58:38.678 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][12/77] Data 0.027 (0.027) Batch 0.098 (0.068) Remain 00:00:14 loss: 0.0921 data: -0.0053 Lr: 0.35552
2024-08-21 21:58:38.745 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][13/77] Data 0.027 (0.027) Batch 0.067 (0.068) Remain 00:00:14 loss: 0.0816 data: -0.0085 Lr: 0.35390
2024-08-21 21:58:38.813 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][14/77] Data 0.027 (0.027) Batch 0.068 (0.068) Remain 00:00:14 loss: 0.1435 data: 0.0011 Lr: 0.35227
2024-08-21 21:58:38.882 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][15/77] Data 0.028 (0.027) Batch 0.069 (0.068) Remain 00:00:14 loss: 0.0900 data: -0.0009 Lr: 0.35065
2024-08-21 21:58:38.882 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_400
2024-08-21 21:58:38.905 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -550.2217407226562
2024-08-21 21:58:38.905 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -261.5187072753906
2024-08-21 21:58:38.969 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][16/77] Data 0.051 (0.029) Batch 0.087 (0.069) Remain 00:00:14 loss: 0.1310 data: 0.0100 Lr: 0.34903
2024-08-21 21:58:39.035 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][17/77] Data 0.027 (0.029) Batch 0.066 (0.069) Remain 00:00:14 loss: 0.0832 data: -0.0082 Lr: 0.34740
2024-08-21 21:58:39.101 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][18/77] Data 0.027 (0.028) Batch 0.066 (0.069) Remain 00:00:14 loss: 0.1926 data: 0.0112 Lr: 0.34578
2024-08-21 21:58:39.167 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][19/77] Data 0.027 (0.028) Batch 0.066 (0.069) Remain 00:00:14 loss: 0.0882 data: 0.0047 Lr: 0.34416
2024-08-21 21:58:39.233 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][20/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:14 loss: 0.1464 data: 0.0030 Lr: 0.34253
2024-08-21 21:58:39.299 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][21/77] Data 0.028 (0.028) Batch 0.066 (0.068) Remain 00:00:14 loss: 0.1807 data: -0.0080 Lr: 0.34091
2024-08-21 21:58:39.365 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][22/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:14 loss: 0.1237 data: 0.0072 Lr: 0.33929
2024-08-21 21:58:39.430 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][23/77] Data 0.027 (0.028) Batch 0.065 (0.068) Remain 00:00:14 loss: 0.1167 data: -0.0051 Lr: 0.33766
2024-08-21 21:58:39.496 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][24/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:14 loss: 0.0767 data: 0.0033 Lr: 0.33604
2024-08-21 21:58:39.564 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][25/77] Data 0.028 (0.028) Batch 0.068 (0.068) Remain 00:00:14 loss: 0.0679 data: 0.0152 Lr: 0.33442
2024-08-21 21:58:39.631 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][26/77] Data 0.028 (0.028) Batch 0.067 (0.068) Remain 00:00:13 loss: 0.0960 data: -0.0023 Lr: 0.33279
2024-08-21 21:58:39.699 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][27/77] Data 0.028 (0.028) Batch 0.068 (0.068) Remain 00:00:13 loss: 0.1212 data: 0.0118 Lr: 0.33117
2024-08-21 21:58:39.765 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][28/77] Data 0.027 (0.028) Batch 0.066 (0.068) Remain 00:00:13 loss: 0.0872 data: -0.0045 Lr: 0.32955
2024-08-21 21:58:39.836 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][29/77] Data 0.027 (0.028) Batch 0.071 (0.068) Remain 00:00:13 loss: 0.1450 data: -0.0048 Lr: 0.32792
2024-08-21 21:58:39.918 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][30/77] Data 0.036 (0.028) Batch 0.082 (0.068) Remain 00:00:13 loss: 0.1173 data: 0.0166 Lr: 0.32630
2024-08-21 21:58:40.002 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][31/77] Data 0.034 (0.028) Batch 0.084 (0.069) Remain 00:00:13 loss: 0.1359 data: 0.0001 Lr: 0.32468
2024-08-21 21:58:40.093 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][32/77] Data 0.036 (0.029) Batch 0.091 (0.070) Remain 00:00:13 loss: 0.0476 data: 0.0063 Lr: 0.32305
2024-08-21 21:58:40.186 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][33/77] Data 0.038 (0.029) Batch 0.092 (0.070) Remain 00:00:14 loss: 0.0963 data: -0.0038 Lr: 0.32143
2024-08-21 21:58:40.267 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][34/77] Data 0.033 (0.029) Batch 0.081 (0.071) Remain 00:00:14 loss: 0.0791 data: -0.0071 Lr: 0.31981
2024-08-21 21:58:40.351 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][35/77] Data 0.029 (0.029) Batch 0.083 (0.071) Remain 00:00:14 loss: 0.1137 data: -0.0320 Lr: 0.31818
2024-08-21 21:58:40.438 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][36/77] Data 0.029 (0.029) Batch 0.088 (0.072) Remain 00:00:14 loss: 0.1248 data: 0.0079 Lr: 0.31656
2024-08-21 21:58:40.522 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][37/77] Data 0.029 (0.029) Batch 0.084 (0.072) Remain 00:00:14 loss: 0.1025 data: 0.0035 Lr: 0.31494
2024-08-21 21:58:40.599 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][38/77] Data 0.029 (0.029) Batch 0.076 (0.072) Remain 00:00:13 loss: 0.0811 data: 0.0007 Lr: 0.31331
2024-08-21 21:58:40.680 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][39/77] Data 0.037 (0.029) Batch 0.081 (0.072) Remain 00:00:13 loss: 0.0891 data: 0.0020 Lr: 0.31169
2024-08-21 21:58:40.763 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][40/77] Data 0.029 (0.029) Batch 0.083 (0.073) Remain 00:00:13 loss: 0.1577 data: -0.0045 Lr: 0.31006
2024-08-21 21:58:40.839 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][41/77] Data 0.032 (0.029) Batch 0.076 (0.073) Remain 00:00:13 loss: 0.0728 data: 0.0053 Lr: 0.30844
2024-08-21 21:58:40.910 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][42/77] Data 0.029 (0.029) Batch 0.071 (0.073) Remain 00:00:13 loss: 0.1118 data: -0.0071 Lr: 0.30682
2024-08-21 21:58:40.977 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][43/77] Data 0.028 (0.029) Batch 0.068 (0.072) Remain 00:00:13 loss: 0.0802 data: -0.0066 Lr: 0.30519
2024-08-21 21:58:41.044 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][44/77] Data 0.027 (0.029) Batch 0.066 (0.072) Remain 00:00:13 loss: 0.0673 data: 0.0052 Lr: 0.30357
2024-08-21 21:58:41.119 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][45/77] Data 0.028 (0.029) Batch 0.075 (0.072) Remain 00:00:13 loss: 0.1386 data: 0.0016 Lr: 0.30195
2024-08-21 21:58:41.199 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][46/77] Data 0.029 (0.029) Batch 0.080 (0.073) Remain 00:00:13 loss: 0.0445 data: -0.0031 Lr: 0.30032
2024-08-21 21:58:41.271 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][47/77] Data 0.028 (0.029) Batch 0.072 (0.073) Remain 00:00:13 loss: 0.0724 data: 0.0088 Lr: 0.29870
2024-08-21 21:58:41.343 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][48/77] Data 0.028 (0.029) Batch 0.072 (0.073) Remain 00:00:13 loss: 0.0614 data: -0.0063 Lr: 0.29708
2024-08-21 21:58:41.426 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][49/77] Data 0.028 (0.029) Batch 0.082 (0.073) Remain 00:00:13 loss: 0.1036 data: 0.0184 Lr: 0.29545
2024-08-21 21:58:41.510 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][50/77] Data 0.028 (0.029) Batch 0.084 (0.073) Remain 00:00:13 loss: 0.0643 data: 0.0187 Lr: 0.29383
2024-08-21 21:58:41.590 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][51/77] Data 0.028 (0.029) Batch 0.080 (0.073) Remain 00:00:13 loss: 0.0874 data: -0.0055 Lr: 0.29221
2024-08-21 21:58:41.657 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][52/77] Data 0.027 (0.029) Batch 0.067 (0.073) Remain 00:00:13 loss: 0.1468 data: -0.0154 Lr: 0.29058
2024-08-21 21:58:41.723 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][53/77] Data 0.027 (0.029) Batch 0.066 (0.073) Remain 00:00:13 loss: 0.0849 data: 0.0010 Lr: 0.28896
2024-08-21 21:58:41.790 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][54/77] Data 0.027 (0.029) Batch 0.067 (0.073) Remain 00:00:12 loss: 0.1236 data: -0.0035 Lr: 0.28734
2024-08-21 21:58:41.856 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][55/77] Data 0.027 (0.029) Batch 0.066 (0.073) Remain 00:00:12 loss: 0.0972 data: 0.0044 Lr: 0.28571
2024-08-21 21:58:41.923 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][56/77] Data 0.027 (0.029) Batch 0.067 (0.073) Remain 00:00:12 loss: 0.1307 data: -0.0034 Lr: 0.28409
2024-08-21 21:58:41.993 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][57/77] Data 0.027 (0.029) Batch 0.069 (0.073) Remain 00:00:12 loss: 0.0635 data: 0.0192 Lr: 0.28247
2024-08-21 21:58:42.071 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][58/77] Data 0.028 (0.029) Batch 0.078 (0.073) Remain 00:00:12 loss: 0.1956 data: -0.0038 Lr: 0.28084
2024-08-21 21:58:42.146 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][59/77] Data 0.028 (0.029) Batch 0.075 (0.073) Remain 00:00:12 loss: 0.0884 data: -0.0079 Lr: 0.27922
2024-08-21 21:58:42.217 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][60/77] Data 0.029 (0.029) Batch 0.071 (0.073) Remain 00:00:12 loss: 0.0757 data: -0.0010 Lr: 0.27760
2024-08-21 21:58:42.291 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][61/77] Data 0.029 (0.029) Batch 0.074 (0.073) Remain 00:00:12 loss: 0.1196 data: -0.0009 Lr: 0.27597
2024-08-21 21:58:42.368 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][62/77] Data 0.028 (0.029) Batch 0.078 (0.073) Remain 00:00:12 loss: 0.1195 data: -0.0007 Lr: 0.27435
2024-08-21 21:58:42.449 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][63/77] Data 0.029 (0.029) Batch 0.081 (0.073) Remain 00:00:12 loss: 0.1004 data: -0.0075 Lr: 0.27273
2024-08-21 21:58:42.534 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][64/77] Data 0.029 (0.029) Batch 0.085 (0.073) Remain 00:00:12 loss: 0.2092 data: 0.0079 Lr: 0.27110
2024-08-21 21:58:42.606 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][65/77] Data 0.029 (0.029) Batch 0.072 (0.073) Remain 00:00:12 loss: 0.0912 data: -0.0062 Lr: 0.26948
2024-08-21 21:58:42.606 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_450
2024-08-21 21:58:42.636 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -564.2738037109375
2024-08-21 21:58:42.637 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -275.3643798828125
2024-08-21 21:58:42.710 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][66/77] Data 0.060 (0.029) Batch 0.104 (0.073) Remain 00:00:12 loss: 0.0797 data: 0.0081 Lr: 0.26786
2024-08-21 21:58:42.782 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][67/77] Data 0.029 (0.029) Batch 0.073 (0.073) Remain 00:00:12 loss: 0.0942 data: -0.0052 Lr: 0.26623
2024-08-21 21:58:42.854 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][68/77] Data 0.029 (0.029) Batch 0.072 (0.073) Remain 00:00:12 loss: 0.0940 data: -0.0219 Lr: 0.26461
2024-08-21 21:58:42.925 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][69/77] Data 0.028 (0.029) Batch 0.071 (0.073) Remain 00:00:11 loss: 0.0862 data: -0.0128 Lr: 0.26299
2024-08-21 21:58:42.996 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][70/77] Data 0.028 (0.029) Batch 0.071 (0.073) Remain 00:00:11 loss: 0.1497 data: -0.0059 Lr: 0.26136
2024-08-21 21:58:43.066 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][71/77] Data 0.028 (0.029) Batch 0.070 (0.073) Remain 00:00:11 loss: 0.0885 data: -0.0177 Lr: 0.25974
2024-08-21 21:58:43.132 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][72/77] Data 0.027 (0.029) Batch 0.066 (0.073) Remain 00:00:11 loss: 0.0638 data: 0.0072 Lr: 0.25812
2024-08-21 21:58:43.197 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][73/77] Data 0.027 (0.029) Batch 0.065 (0.073) Remain 00:00:11 loss: 0.1208 data: -0.0020 Lr: 0.25649
2024-08-21 21:58:43.263 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][74/77] Data 0.027 (0.029) Batch 0.066 (0.073) Remain 00:00:11 loss: 0.1422 data: 0.0071 Lr: 0.25487
2024-08-21 21:58:43.329 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][75/77] Data 0.027 (0.029) Batch 0.066 (0.073) Remain 00:00:11 loss: 0.1569 data: 0.0052 Lr: 0.25325
2024-08-21 21:58:43.395 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][76/77] Data 0.027 (0.029) Batch 0.066 (0.073) Remain 00:00:11 loss: 0.0862 data: -0.0066 Lr: 0.25162
2024-08-21 21:58:43.438 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [6/8][77/77] Data 0.030 (0.029) Batch 0.043 (0.072) Remain 00:00:11 loss: 0.0541 data: 0.0074 Lr: 0.25000
2024-08-21 21:58:43.439 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 21:58:47.754 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0346, Accuracy: 0.9884
2024-08-21 21:58:47.754 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 21:58:47.833 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][1/77] Data 0.044 (0.044) Batch 0.079 (0.079) Remain 00:00:12 loss: 0.0798 data: -0.0064 Lr: 0.24838
2024-08-21 21:58:47.891 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][2/77] Data 0.022 (0.022) Batch 0.058 (0.058) Remain 00:00:08 loss: 0.1267 data: -0.0154 Lr: 0.24675
2024-08-21 21:58:47.948 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][3/77] Data 0.022 (0.022) Batch 0.057 (0.057) Remain 00:00:08 loss: 0.1178 data: 0.0051 Lr: 0.24513
2024-08-21 21:58:48.004 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][4/77] Data 0.022 (0.022) Batch 0.057 (0.057) Remain 00:00:08 loss: 0.1232 data: -0.0016 Lr: 0.24351
2024-08-21 21:58:48.062 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][5/77] Data 0.025 (0.023) Batch 0.058 (0.057) Remain 00:00:08 loss: 0.0794 data: -0.0099 Lr: 0.24188
2024-08-21 21:58:48.118 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][6/77] Data 0.022 (0.023) Batch 0.056 (0.057) Remain 00:00:08 loss: 0.1100 data: 0.0066 Lr: 0.24026
2024-08-21 21:58:48.174 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][7/77] Data 0.022 (0.023) Batch 0.056 (0.057) Remain 00:00:08 loss: 0.0776 data: 0.0012 Lr: 0.23864
2024-08-21 21:58:48.230 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][8/77] Data 0.023 (0.023) Batch 0.055 (0.057) Remain 00:00:08 loss: 0.0645 data: -0.0092 Lr: 0.23701
2024-08-21 21:58:48.285 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][9/77] Data 0.022 (0.023) Batch 0.055 (0.056) Remain 00:00:08 loss: 0.0879 data: 0.0104 Lr: 0.23539
2024-08-21 21:58:48.341 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][10/77] Data 0.022 (0.022) Batch 0.057 (0.056) Remain 00:00:08 loss: 0.0833 data: -0.0118 Lr: 0.23377
2024-08-21 21:58:48.397 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][11/77] Data 0.024 (0.023) Batch 0.055 (0.056) Remain 00:00:08 loss: 0.1064 data: 0.0034 Lr: 0.23214
2024-08-21 21:58:48.452 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][12/77] Data 0.022 (0.023) Batch 0.056 (0.056) Remain 00:00:08 loss: 0.1045 data: -0.0093 Lr: 0.23052
2024-08-21 21:58:48.507 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][13/77] Data 0.022 (0.022) Batch 0.055 (0.056) Remain 00:00:07 loss: 0.1088 data: 0.0014 Lr: 0.22890
2024-08-21 21:58:48.563 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][14/77] Data 0.022 (0.022) Batch 0.056 (0.056) Remain 00:00:07 loss: 0.0866 data: -0.0302 Lr: 0.22727
2024-08-21 21:58:48.620 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][15/77] Data 0.022 (0.022) Batch 0.057 (0.056) Remain 00:00:07 loss: 0.0586 data: -0.0052 Lr: 0.22565
2024-08-21 21:58:48.706 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][16/77] Data 0.038 (0.023) Batch 0.086 (0.058) Remain 00:00:08 loss: 0.0631 data: 0.0036 Lr: 0.22403
2024-08-21 21:58:48.786 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][17/77] Data 0.037 (0.024) Batch 0.081 (0.060) Remain 00:00:08 loss: 0.1263 data: 0.0048 Lr: 0.22240
2024-08-21 21:58:48.851 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][18/77] Data 0.027 (0.024) Batch 0.065 (0.060) Remain 00:00:08 loss: 0.1110 data: 0.0172 Lr: 0.22078
2024-08-21 21:58:48.917 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][19/77] Data 0.027 (0.025) Batch 0.066 (0.060) Remain 00:00:08 loss: 0.0942 data: 0.0141 Lr: 0.21916
2024-08-21 21:58:48.983 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][20/77] Data 0.027 (0.025) Batch 0.065 (0.060) Remain 00:00:08 loss: 0.0861 data: 0.0081 Lr: 0.21753
2024-08-21 21:58:49.050 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][21/77] Data 0.027 (0.025) Batch 0.067 (0.061) Remain 00:00:08 loss: 0.1220 data: 0.0044 Lr: 0.21591
2024-08-21 21:58:49.114 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][22/77] Data 0.027 (0.025) Batch 0.064 (0.061) Remain 00:00:08 loss: 0.0632 data: -0.0057 Lr: 0.21429
2024-08-21 21:58:49.178 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][23/77] Data 0.027 (0.025) Batch 0.065 (0.061) Remain 00:00:08 loss: 0.1248 data: -0.0005 Lr: 0.21266
2024-08-21 21:58:49.243 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][24/77] Data 0.027 (0.025) Batch 0.065 (0.061) Remain 00:00:08 loss: 0.1256 data: -0.0035 Lr: 0.21104
2024-08-21 21:58:49.309 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][25/77] Data 0.027 (0.025) Batch 0.066 (0.061) Remain 00:00:07 loss: 0.0970 data: -0.0067 Lr: 0.20942
2024-08-21 21:58:49.375 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][26/77] Data 0.027 (0.025) Batch 0.066 (0.062) Remain 00:00:07 loss: 0.0915 data: 0.0067 Lr: 0.20779
2024-08-21 21:58:49.441 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][27/77] Data 0.028 (0.025) Batch 0.066 (0.062) Remain 00:00:07 loss: 0.0819 data: 0.0105 Lr: 0.20617
2024-08-21 21:58:49.506 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][28/77] Data 0.027 (0.025) Batch 0.066 (0.062) Remain 00:00:07 loss: 0.0349 data: -0.0124 Lr: 0.20455
2024-08-21 21:58:49.572 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][29/77] Data 0.027 (0.026) Batch 0.065 (0.062) Remain 00:00:07 loss: 0.0900 data: 0.0209 Lr: 0.20292
2024-08-21 21:58:49.636 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][30/77] Data 0.025 (0.026) Batch 0.064 (0.062) Remain 00:00:07 loss: 0.0373 data: 0.0119 Lr: 0.20130
2024-08-21 21:58:49.691 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][31/77] Data 0.022 (0.025) Batch 0.055 (0.062) Remain 00:00:07 loss: 0.1559 data: -0.0029 Lr: 0.19968
2024-08-21 21:58:49.747 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][32/77] Data 0.024 (0.025) Batch 0.056 (0.062) Remain 00:00:07 loss: 0.1462 data: -0.0170 Lr: 0.19805
2024-08-21 21:58:49.803 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][33/77] Data 0.022 (0.025) Batch 0.056 (0.062) Remain 00:00:07 loss: 0.0786 data: -0.0237 Lr: 0.19643
2024-08-21 21:58:49.857 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][34/77] Data 0.022 (0.025) Batch 0.054 (0.061) Remain 00:00:07 loss: 0.0719 data: -0.0005 Lr: 0.19481
2024-08-21 21:58:49.913 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][35/77] Data 0.024 (0.025) Batch 0.056 (0.061) Remain 00:00:07 loss: 0.1943 data: -0.0102 Lr: 0.19318
2024-08-21 21:58:49.973 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][36/77] Data 0.024 (0.025) Batch 0.059 (0.061) Remain 00:00:07 loss: 0.0905 data: -0.0073 Lr: 0.19156
2024-08-21 21:58:50.035 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][37/77] Data 0.023 (0.025) Batch 0.063 (0.061) Remain 00:00:07 loss: 0.0760 data: 0.0178 Lr: 0.18994
2024-08-21 21:58:50.099 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][38/77] Data 0.021 (0.025) Batch 0.064 (0.061) Remain 00:00:07 loss: 0.0644 data: 0.0097 Lr: 0.18831
2024-08-21 21:58:50.099 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_500
2024-08-21 21:58:50.125 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -578.7744140625
2024-08-21 21:58:50.125 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -275.3176574707031
2024-08-21 21:58:50.190 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][39/77] Data 0.047 (0.025) Batch 0.091 (0.062) Remain 00:00:07 loss: 0.0432 data: 0.0174 Lr: 0.18669
2024-08-21 21:58:50.253 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][40/77] Data 0.022 (0.025) Batch 0.063 (0.062) Remain 00:00:07 loss: 0.0826 data: -0.0229 Lr: 0.18506
2024-08-21 21:58:50.318 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][41/77] Data 0.022 (0.025) Batch 0.064 (0.062) Remain 00:00:07 loss: 0.0483 data: 0.0221 Lr: 0.18344
2024-08-21 21:58:50.383 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][42/77] Data 0.022 (0.025) Batch 0.065 (0.062) Remain 00:00:07 loss: 0.1649 data: 0.0027 Lr: 0.18182
2024-08-21 21:58:50.446 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][43/77] Data 0.021 (0.025) Batch 0.064 (0.062) Remain 00:00:06 loss: 0.1106 data: -0.0048 Lr: 0.18019
2024-08-21 21:58:50.508 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][44/77] Data 0.021 (0.025) Batch 0.062 (0.062) Remain 00:00:06 loss: 0.1095 data: 0.0079 Lr: 0.17857
2024-08-21 21:58:50.571 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][45/77] Data 0.022 (0.025) Batch 0.063 (0.062) Remain 00:00:06 loss: 0.1621 data: 0.0060 Lr: 0.17695
2024-08-21 21:58:50.634 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][46/77] Data 0.022 (0.025) Batch 0.063 (0.062) Remain 00:00:06 loss: 0.1142 data: 0.0093 Lr: 0.17532
2024-08-21 21:58:50.696 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][47/77] Data 0.022 (0.025) Batch 0.062 (0.062) Remain 00:00:06 loss: 0.0745 data: 0.0069 Lr: 0.17370
2024-08-21 21:58:50.759 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][48/77] Data 0.022 (0.025) Batch 0.063 (0.062) Remain 00:00:06 loss: 0.1024 data: -0.0116 Lr: 0.17208
2024-08-21 21:58:50.821 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][49/77] Data 0.022 (0.025) Batch 0.062 (0.062) Remain 00:00:06 loss: 0.1062 data: 0.0085 Lr: 0.17045
2024-08-21 21:58:50.884 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][50/77] Data 0.021 (0.025) Batch 0.063 (0.062) Remain 00:00:06 loss: 0.1000 data: -0.0132 Lr: 0.16883
2024-08-21 21:58:50.947 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][51/77] Data 0.022 (0.025) Batch 0.062 (0.062) Remain 00:00:06 loss: 0.0951 data: -0.0028 Lr: 0.16721
2024-08-21 21:58:51.009 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][52/77] Data 0.023 (0.025) Batch 0.063 (0.062) Remain 00:00:06 loss: 0.0727 data: 0.0056 Lr: 0.16558
2024-08-21 21:58:51.072 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][53/77] Data 0.023 (0.025) Batch 0.063 (0.062) Remain 00:00:06 loss: 0.1158 data: 0.0171 Lr: 0.16396
2024-08-21 21:58:51.134 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][54/77] Data 0.023 (0.025) Batch 0.062 (0.062) Remain 00:00:06 loss: 0.0604 data: -0.0228 Lr: 0.16234
2024-08-21 21:58:51.199 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][55/77] Data 0.023 (0.024) Batch 0.064 (0.062) Remain 00:00:06 loss: 0.0743 data: -0.0035 Lr: 0.16071
2024-08-21 21:58:51.260 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][56/77] Data 0.022 (0.024) Batch 0.062 (0.062) Remain 00:00:06 loss: 0.0655 data: 0.0114 Lr: 0.15909
2024-08-21 21:58:51.334 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][57/77] Data 0.021 (0.024) Batch 0.074 (0.063) Remain 00:00:06 loss: 0.0808 data: -0.0014 Lr: 0.15747
2024-08-21 21:58:51.412 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][58/77] Data 0.020 (0.024) Batch 0.078 (0.063) Remain 00:00:06 loss: 0.1015 data: -0.0027 Lr: 0.15584
2024-08-21 21:58:51.479 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][59/77] Data 0.021 (0.024) Batch 0.068 (0.063) Remain 00:00:06 loss: 0.0880 data: 0.0132 Lr: 0.15422
2024-08-21 21:58:51.545 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][60/77] Data 0.021 (0.024) Batch 0.066 (0.063) Remain 00:00:05 loss: 0.0822 data: -0.0033 Lr: 0.15260
2024-08-21 21:58:51.599 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][61/77] Data 0.021 (0.024) Batch 0.054 (0.063) Remain 00:00:05 loss: 0.0928 data: -0.0037 Lr: 0.15097
2024-08-21 21:58:51.654 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][62/77] Data 0.023 (0.024) Batch 0.055 (0.063) Remain 00:00:05 loss: 0.1067 data: 0.0063 Lr: 0.14935
2024-08-21 21:58:51.709 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][63/77] Data 0.023 (0.024) Batch 0.055 (0.063) Remain 00:00:05 loss: 0.0777 data: 0.0070 Lr: 0.14773
2024-08-21 21:58:51.763 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][64/77] Data 0.022 (0.024) Batch 0.054 (0.062) Remain 00:00:05 loss: 0.0661 data: 0.0008 Lr: 0.14610
2024-08-21 21:58:51.818 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][65/77] Data 0.023 (0.024) Batch 0.055 (0.062) Remain 00:00:05 loss: 0.0878 data: 0.0134 Lr: 0.14448
2024-08-21 21:58:51.874 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][66/77] Data 0.023 (0.024) Batch 0.056 (0.062) Remain 00:00:05 loss: 0.1405 data: -0.0271 Lr: 0.14286
2024-08-21 21:58:51.929 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][67/77] Data 0.023 (0.024) Batch 0.055 (0.062) Remain 00:00:05 loss: 0.0468 data: 0.0089 Lr: 0.14123
2024-08-21 21:58:51.984 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][68/77] Data 0.022 (0.024) Batch 0.055 (0.062) Remain 00:00:05 loss: 0.1499 data: 0.0155 Lr: 0.13961
2024-08-21 21:58:52.040 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][69/77] Data 0.022 (0.024) Batch 0.056 (0.062) Remain 00:00:05 loss: 0.0372 data: -0.0251 Lr: 0.13799
2024-08-21 21:58:52.094 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][70/77] Data 0.022 (0.024) Batch 0.054 (0.062) Remain 00:00:05 loss: 0.0964 data: -0.0143 Lr: 0.13636
2024-08-21 21:58:52.151 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][71/77] Data 0.023 (0.024) Batch 0.057 (0.062) Remain 00:00:05 loss: 0.0549 data: -0.0263 Lr: 0.13474
2024-08-21 21:58:52.218 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][72/77] Data 0.031 (0.024) Batch 0.067 (0.062) Remain 00:00:05 loss: 0.0630 data: 0.0012 Lr: 0.13312
2024-08-21 21:58:52.283 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][73/77] Data 0.027 (0.024) Batch 0.065 (0.062) Remain 00:00:05 loss: 0.0975 data: -0.0014 Lr: 0.13149
2024-08-21 21:58:52.347 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][74/77] Data 0.027 (0.024) Batch 0.064 (0.062) Remain 00:00:05 loss: 0.1266 data: 0.0047 Lr: 0.12987
2024-08-21 21:58:52.411 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][75/77] Data 0.027 (0.024) Batch 0.064 (0.062) Remain 00:00:04 loss: 0.1062 data: -0.0082 Lr: 0.12825
2024-08-21 21:58:52.475 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][76/77] Data 0.027 (0.024) Batch 0.064 (0.062) Remain 00:00:04 loss: 0.0498 data: 0.0136 Lr: 0.12662
2024-08-21 21:58:52.514 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [7/8][77/77] Data 0.029 (0.024) Batch 0.039 (0.062) Remain 00:00:04 loss: 0.1221 data: -0.0050 Lr: 0.12500
2024-08-21 21:58:52.515 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 21:58:55.875 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0303, Accuracy: 0.9893
2024-08-21 21:58:55.875 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 21:58:55.875 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 21:58:56.006 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][1/77] Data 0.097 (0.097) Batch 0.130 (0.130) Remain 00:00:10 loss: 0.0522 data: 0.0042 Lr: 0.12338
2024-08-21 21:58:56.061 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][2/77] Data 0.021 (0.021) Batch 0.056 (0.056) Remain 00:00:04 loss: 0.0653 data: -0.0089 Lr: 0.12175
2024-08-21 21:58:56.117 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][3/77] Data 0.021 (0.021) Batch 0.056 (0.056) Remain 00:00:04 loss: 0.0641 data: -0.0115 Lr: 0.12013
2024-08-21 21:58:56.173 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][4/77] Data 0.022 (0.022) Batch 0.056 (0.056) Remain 00:00:04 loss: 0.0693 data: -0.0063 Lr: 0.11851
2024-08-21 21:58:56.230 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][5/77] Data 0.022 (0.022) Batch 0.057 (0.056) Remain 00:00:04 loss: 0.0560 data: -0.0005 Lr: 0.11688
2024-08-21 21:58:56.285 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][6/77] Data 0.022 (0.022) Batch 0.055 (0.056) Remain 00:00:04 loss: 0.0592 data: -0.0105 Lr: 0.11526
2024-08-21 21:58:56.338 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][7/77] Data 0.022 (0.022) Batch 0.053 (0.055) Remain 00:00:03 loss: 0.1155 data: 0.0051 Lr: 0.11364
2024-08-21 21:58:56.394 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][8/77] Data 0.022 (0.022) Batch 0.056 (0.055) Remain 00:00:03 loss: 0.1452 data: 0.0020 Lr: 0.11201
2024-08-21 21:58:56.458 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][9/77] Data 0.028 (0.023) Batch 0.064 (0.057) Remain 00:00:03 loss: 0.1189 data: 0.0036 Lr: 0.11039
2024-08-21 21:58:56.522 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][10/77] Data 0.027 (0.023) Batch 0.064 (0.057) Remain 00:00:03 loss: 0.0811 data: -0.0051 Lr: 0.10877
2024-08-21 21:58:56.587 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][11/77] Data 0.027 (0.023) Batch 0.065 (0.058) Remain 00:00:03 loss: 0.1472 data: 0.0083 Lr: 0.10714
2024-08-21 21:58:56.587 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_550
2024-08-21 21:58:56.609 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -580.2357788085938
2024-08-21 21:58:56.609 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -276.48919677734375
2024-08-21 21:58:56.676 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][12/77] Data 0.049 (0.026) Batch 0.088 (0.061) Remain 00:00:04 loss: 0.1101 data: 0.0290 Lr: 0.10552
2024-08-21 21:58:56.741 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][13/77] Data 0.027 (0.026) Batch 0.065 (0.061) Remain 00:00:03 loss: 0.0933 data: -0.0056 Lr: 0.10390
2024-08-21 21:58:56.806 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][14/77] Data 0.027 (0.026) Batch 0.065 (0.062) Remain 00:00:03 loss: 0.0665 data: -0.0007 Lr: 0.10227
2024-08-21 21:58:56.871 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][15/77] Data 0.027 (0.026) Batch 0.066 (0.062) Remain 00:00:03 loss: 0.0995 data: -0.0010 Lr: 0.10065
2024-08-21 21:58:56.938 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][16/77] Data 0.027 (0.026) Batch 0.066 (0.062) Remain 00:00:03 loss: 0.0997 data: 0.0066 Lr: 0.09903
2024-08-21 21:58:57.004 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][17/77] Data 0.027 (0.026) Batch 0.066 (0.062) Remain 00:00:03 loss: 0.1029 data: 0.0026 Lr: 0.09740
2024-08-21 21:58:57.070 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][18/77] Data 0.028 (0.026) Batch 0.066 (0.063) Remain 00:00:03 loss: 0.0876 data: 0.0020 Lr: 0.09578
2024-08-21 21:58:57.134 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][19/77] Data 0.027 (0.026) Batch 0.064 (0.063) Remain 00:00:03 loss: 0.0501 data: -0.0174 Lr: 0.09416
2024-08-21 21:58:57.199 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][20/77] Data 0.027 (0.026) Batch 0.066 (0.063) Remain 00:00:03 loss: 0.0653 data: 0.0143 Lr: 0.09253
2024-08-21 21:58:57.264 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][21/77] Data 0.027 (0.026) Batch 0.065 (0.063) Remain 00:00:03 loss: 0.1085 data: -0.0120 Lr: 0.09091
2024-08-21 21:58:57.329 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][22/77] Data 0.027 (0.026) Batch 0.065 (0.063) Remain 00:00:03 loss: 0.0626 data: -0.0081 Lr: 0.08929
2024-08-21 21:58:57.395 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][23/77] Data 0.027 (0.026) Batch 0.066 (0.063) Remain 00:00:03 loss: 0.0608 data: -0.0021 Lr: 0.08766
2024-08-21 21:58:57.461 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][24/77] Data 0.027 (0.026) Batch 0.066 (0.063) Remain 00:00:03 loss: 0.0711 data: -0.0006 Lr: 0.08604
2024-08-21 21:58:57.527 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][25/77] Data 0.027 (0.027) Batch 0.066 (0.063) Remain 00:00:03 loss: 0.0529 data: 0.0032 Lr: 0.08442
2024-08-21 21:58:57.593 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][26/77] Data 0.027 (0.027) Batch 0.066 (0.063) Remain 00:00:03 loss: 0.1054 data: -0.0088 Lr: 0.08279
2024-08-21 21:58:57.661 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][27/77] Data 0.027 (0.027) Batch 0.068 (0.064) Remain 00:00:03 loss: 0.0671 data: 0.0039 Lr: 0.08117
2024-08-21 21:58:57.726 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][28/77] Data 0.027 (0.027) Batch 0.065 (0.064) Remain 00:00:03 loss: 0.0548 data: 0.0135 Lr: 0.07955
2024-08-21 21:58:57.790 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][29/77] Data 0.027 (0.027) Batch 0.064 (0.064) Remain 00:00:03 loss: 0.0902 data: 0.0106 Lr: 0.07792
2024-08-21 21:58:57.862 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][30/77] Data 0.031 (0.027) Batch 0.071 (0.064) Remain 00:00:03 loss: 0.1078 data: -0.0106 Lr: 0.07630
2024-08-21 21:58:57.932 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][31/77] Data 0.032 (0.027) Batch 0.070 (0.064) Remain 00:00:03 loss: 0.0874 data: -0.0173 Lr: 0.07468
2024-08-21 21:58:57.999 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][32/77] Data 0.027 (0.027) Batch 0.067 (0.064) Remain 00:00:02 loss: 0.0403 data: -0.0115 Lr: 0.07305
2024-08-21 21:58:58.073 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][33/77] Data 0.027 (0.027) Batch 0.074 (0.065) Remain 00:00:02 loss: 0.0608 data: -0.0029 Lr: 0.07143
2024-08-21 21:58:58.144 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][34/77] Data 0.027 (0.027) Batch 0.071 (0.065) Remain 00:00:02 loss: 0.1161 data: 0.0003 Lr: 0.06981
2024-08-21 21:58:58.209 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][35/77] Data 0.027 (0.027) Batch 0.066 (0.065) Remain 00:00:02 loss: 0.0612 data: 0.0125 Lr: 0.06818
2024-08-21 21:58:58.287 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][36/77] Data 0.033 (0.027) Batch 0.078 (0.065) Remain 00:00:02 loss: 0.0764 data: 0.0010 Lr: 0.06656
2024-08-21 21:58:58.359 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][37/77] Data 0.027 (0.027) Batch 0.072 (0.065) Remain 00:00:02 loss: 0.0581 data: -0.0047 Lr: 0.06494
2024-08-21 21:58:58.429 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][38/77] Data 0.027 (0.027) Batch 0.070 (0.065) Remain 00:00:02 loss: 0.1198 data: -0.0152 Lr: 0.06331
2024-08-21 21:58:58.496 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][39/77] Data 0.027 (0.027) Batch 0.068 (0.066) Remain 00:00:02 loss: 0.0801 data: -0.0018 Lr: 0.06169
2024-08-21 21:58:58.564 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][40/77] Data 0.028 (0.027) Batch 0.067 (0.066) Remain 00:00:02 loss: 0.0561 data: -0.0032 Lr: 0.06006
2024-08-21 21:58:58.628 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][41/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:02 loss: 0.0919 data: -0.0107 Lr: 0.05844
2024-08-21 21:58:58.693 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][42/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:02 loss: 0.1135 data: -0.0013 Lr: 0.05682
2024-08-21 21:58:58.758 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][43/77] Data 0.027 (0.027) Batch 0.065 (0.066) Remain 00:00:02 loss: 0.0693 data: 0.0111 Lr: 0.05519
2024-08-21 21:58:58.822 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][44/77] Data 0.027 (0.027) Batch 0.064 (0.065) Remain 00:00:02 loss: 0.0579 data: 0.0088 Lr: 0.05357
2024-08-21 21:58:58.887 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][45/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:02 loss: 0.0760 data: -0.0045 Lr: 0.05195
2024-08-21 21:58:58.952 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][46/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:02 loss: 0.1015 data: 0.0117 Lr: 0.05032
2024-08-21 21:58:59.016 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][47/77] Data 0.027 (0.027) Batch 0.064 (0.065) Remain 00:00:02 loss: 0.0995 data: -0.0022 Lr: 0.04870
2024-08-21 21:58:59.081 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][48/77] Data 0.027 (0.027) Batch 0.064 (0.065) Remain 00:00:01 loss: 0.1515 data: -0.0002 Lr: 0.04708
2024-08-21 21:58:59.146 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][49/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:01 loss: 0.1398 data: -0.0015 Lr: 0.04545
2024-08-21 21:58:59.212 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][50/77] Data 0.027 (0.027) Batch 0.066 (0.065) Remain 00:00:01 loss: 0.0285 data: 0.0231 Lr: 0.04383
2024-08-21 21:58:59.279 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][51/77] Data 0.027 (0.027) Batch 0.066 (0.065) Remain 00:00:01 loss: 0.1759 data: -0.0132 Lr: 0.04221
2024-08-21 21:58:59.346 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][52/77] Data 0.027 (0.027) Batch 0.067 (0.065) Remain 00:00:01 loss: 0.1512 data: -0.0014 Lr: 0.04058
2024-08-21 21:58:59.411 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][53/77] Data 0.027 (0.027) Batch 0.065 (0.065) Remain 00:00:01 loss: 0.0949 data: -0.0004 Lr: 0.03896
2024-08-21 21:58:59.477 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][54/77] Data 0.028 (0.027) Batch 0.066 (0.065) Remain 00:00:01 loss: 0.0580 data: -0.0088 Lr: 0.03734
2024-08-21 21:58:59.543 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][55/77] Data 0.027 (0.027) Batch 0.066 (0.066) Remain 00:00:01 loss: 0.0980 data: 0.0110 Lr: 0.03571
2024-08-21 21:58:59.609 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][56/77] Data 0.028 (0.027) Batch 0.066 (0.066) Remain 00:00:01 loss: 0.1013 data: -0.0090 Lr: 0.03409
2024-08-21 21:58:59.675 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][57/77] Data 0.027 (0.027) Batch 0.066 (0.066) Remain 00:00:01 loss: 0.0699 data: -0.0103 Lr: 0.03247
2024-08-21 21:58:59.742 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][58/77] Data 0.028 (0.027) Batch 0.068 (0.066) Remain 00:00:01 loss: 0.1009 data: -0.0074 Lr: 0.03084
2024-08-21 21:58:59.807 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][59/77] Data 0.027 (0.027) Batch 0.064 (0.066) Remain 00:00:01 loss: 0.0582 data: 0.0111 Lr: 0.02922
2024-08-21 21:58:59.874 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][60/77] Data 0.028 (0.027) Batch 0.067 (0.066) Remain 00:00:01 loss: 0.0731 data: -0.0182 Lr: 0.02760
2024-08-21 21:58:59.940 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][61/77] Data 0.027 (0.027) Batch 0.066 (0.066) Remain 00:00:01 loss: 0.0888 data: -0.0017 Lr: 0.02597
2024-08-21 21:58:59.941 | INFO     | trim.callbacks.misc:save_checkpoint:149 - => Saving checkpoint to: step_600
2024-08-21 21:58:59.969 | INFO     | trim.callbacks.misc:save_checkpoint:155 - ### Model weight: -579.1841430664062
2024-08-21 21:58:59.970 | INFO     | trim.callbacks.misc:save_checkpoint:157 - ### Optimizer weight: -276.1045227050781
2024-08-21 21:59:00.063 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][62/77] Data 0.068 (0.028) Batch 0.123 (0.067) Remain 00:00:01 loss: 0.0489 data: -0.0054 Lr: 0.02435
2024-08-21 21:59:00.137 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][63/77] Data 0.028 (0.028) Batch 0.074 (0.067) Remain 00:00:00 loss: 0.0543 data: 0.0069 Lr: 0.02273
2024-08-21 21:59:00.214 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][64/77] Data 0.036 (0.028) Batch 0.078 (0.067) Remain 00:00:00 loss: 0.1009 data: 0.0149 Lr: 0.02110
2024-08-21 21:59:00.284 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][65/77] Data 0.028 (0.028) Batch 0.069 (0.067) Remain 00:00:00 loss: 0.0647 data: 0.0015 Lr: 0.01948
2024-08-21 21:59:00.362 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][66/77] Data 0.036 (0.028) Batch 0.078 (0.067) Remain 00:00:00 loss: 0.1054 data: 0.0058 Lr: 0.01786
2024-08-21 21:59:00.439 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][67/77] Data 0.028 (0.028) Batch 0.077 (0.067) Remain 00:00:00 loss: 0.0326 data: -0.0020 Lr: 0.01623
2024-08-21 21:59:00.517 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][68/77] Data 0.030 (0.028) Batch 0.078 (0.067) Remain 00:00:00 loss: 0.1392 data: 0.0182 Lr: 0.01461
2024-08-21 21:59:00.594 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][69/77] Data 0.029 (0.028) Batch 0.076 (0.067) Remain 00:00:00 loss: 0.1139 data: 0.0176 Lr: 0.01299
2024-08-21 21:59:00.674 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][70/77] Data 0.029 (0.028) Batch 0.080 (0.068) Remain 00:00:00 loss: 0.0534 data: 0.0084 Lr: 0.01136
2024-08-21 21:59:00.747 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][71/77] Data 0.033 (0.028) Batch 0.073 (0.068) Remain 00:00:00 loss: 0.0653 data: 0.0041 Lr: 0.00974
2024-08-21 21:59:00.815 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][72/77] Data 0.027 (0.028) Batch 0.068 (0.068) Remain 00:00:00 loss: 0.1117 data: -0.0221 Lr: 0.00812
2024-08-21 21:59:00.884 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][73/77] Data 0.027 (0.028) Batch 0.069 (0.068) Remain 00:00:00 loss: 0.0357 data: 0.0078 Lr: 0.00649
2024-08-21 21:59:00.953 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][74/77] Data 0.028 (0.028) Batch 0.069 (0.068) Remain 00:00:00 loss: 0.0974 data: 0.0147 Lr: 0.00487
2024-08-21 21:59:01.024 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][75/77] Data 0.028 (0.028) Batch 0.071 (0.068) Remain 00:00:00 loss: 0.1167 data: -0.0198 Lr: 0.00325
2024-08-21 21:59:01.099 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][76/77] Data 0.027 (0.028) Batch 0.074 (0.068) Remain 00:00:00 loss: 0.1158 data: -0.0090 Lr: 0.00162
2024-08-21 21:59:01.143 | INFO     | trim.callbacks.misc:on_training_step_end:101 - Train: [8/8][77/77] Data 0.029 (0.028) Batch 0.044 (0.068) Remain 00:00:00 loss: 0.0749 data: -0.0091 Lr: 0.00000
2024-08-21 21:59:01.143 | INFO     | trim.callbacks.evaluator:eval:19 - >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
2024-08-21 21:59:05.088 | INFO     | trim.callbacks.evaluator:eval:35 - Test set: Average loss: 0.0290, Accuracy: 0.9904
2024-08-21 21:59:05.089 | INFO     | trim.callbacks.evaluator:eval:39 - <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
2024-08-21 21:59:05.089 | INFO     | trim.callbacks.evaluator:eval:46 - New best metric!
2024-08-21 21:59:05.089 | INFO     | trim.callbacks.evaluator:on_training_phase_end:49 - Best mIoU: 0.9904, epoch at  7
